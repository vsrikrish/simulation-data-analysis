{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Homework 2 Solutions\n",
        "\n",
        "BEE 4850/5850, Fall 2024\n",
        "\n",
        "**Name**:\n",
        "\n",
        "**ID**:\n",
        "\n",
        "> **Due Date**\n",
        ">\n",
        "> Friday, 2/23/24, 9:00pm\n",
        "\n",
        "## Overview"
      ],
      "id": "6e4b4f2b-8fdd-4445-bc5f-17d0dd88e429"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Instructions\n",
        "\n",
        "The goal of this homework assignment is to practice developing and\n",
        "working with probability models for data.\n",
        "\n",
        "-   Problem 1 asks you to fit a sea-level rise model using normal\n",
        "    residuals and to assess the validity of that assumption.\n",
        "-   Problem 2 asks you to model the time series of hourly\n",
        "    weather-related variability at a tide gauge.\n",
        "-   Problem 3 asks you to model the occurrences of Cayuga Lake freezing,\n",
        "    and is only slightly adapted from Example 4.1 in [Statistical\n",
        "    Methods in the Atmospheric\n",
        "    Sciences](https://www.sciencedirect.com/book/9780128158234/statistical-methods-in-the-atmospheric-sciences)\n",
        "    by Daniel Wilks.\n",
        "-   Problem 4 (**graded only for graduate students**) asks you to\n",
        "    revisit the sea-level model in Problem 1 by including a model-data\n",
        "    discrepancy term in the model calibration."
      ],
      "id": "8ce0073e-9cd9-42f5-877a-1639b391d84e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Learning Outcomes\n",
        "\n",
        "After completing this assignments, students will be able to:\n",
        "\n",
        "-   develop probability models for data and model residuals under a\n",
        "    variety of statistical assumptions;\n",
        "-   evaluate the appropriateness of those assumptions through the use of\n",
        "    qualitative and quantitative evaluations of goodness-of-fit;\n",
        "-   fit a basic Bayesian model to data."
      ],
      "id": "2b3f220b-2376-4f04-8904-87b2618a91fd"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load Environment\n",
        "\n",
        "The following code loads the environment and makes sure all needed\n",
        "packages are installed. This should be at the start of most Julia\n",
        "scripts."
      ],
      "id": "ec65057b-69ab-4205-aaba-07a981fc794d"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import Pkg\n",
        "Pkg.activate(@__DIR__)\n",
        "Pkg.instantiate()"
      ],
      "id": "15428a24"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following packages are included in the environment (to help you find\n",
        "other similar packages in other languages). The code below loads these\n",
        "packages for use in the subsequent notebook (the desired functionality\n",
        "for each package is commented next to the package)."
      ],
      "id": "27a35ba6-c924-465c-b528-83d2aa4936d4"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "using Random # random number generation and seed-setting\n",
        "using DataFrames # tabular data structure\n",
        "using CSVFiles # reads/writes .csv files\n",
        "using Distributions # interface to work with probability distributions\n",
        "using Plots # plotting library\n",
        "using StatsBase # statistical quantities like mean, median, etc\n",
        "using StatsPlots # some additional statistical plotting tools\n",
        "using Optim # optimization tools\n",
        "\n",
        "Random.seed!(1)"
      ],
      "id": "15ad2f20"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problems (Total: 30 Points for 4850; 40 for 5850)\n",
        "\n",
        "### Problem 1"
      ],
      "id": "54481251-bb10-49d2-a809-8c2ee71acba3"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Consider the following sea-level rise model from [Rahmstorf\n",
        "(2007)](https://doi.org/10.1073/pnas.0907765106):\n",
        "\n",
        "$$\\frac{dH(t)}{dt} = \\alpha (T(t) - T_0),$$ where $T_0$ is the\n",
        "temperature (in $^\\circ C$) where sea-level is in equilibrium\n",
        "($dH/dt = 0$), and $\\alpha$ is the sea-level rise sensitivity to\n",
        "temperature. Discretizing this equation using the Euler method and using\n",
        "an annual timestep ($\\delta t = 1$), we get\n",
        "$$H(t+1) = H(t) + \\alpha (T(t) - T_0).$$\n",
        "\n",
        "**In this problem**:\n",
        "\n",
        "-   Load the data from the `data/` folder\n",
        "    -   Global mean temperature data from the HadCRUT 5.0.2.0 dataset\n",
        "        (<https://hadobs.metoffice.gov.uk/hadcrut5/data/HadCRUT.5.0.2.0/download.html>)\n",
        "        can be found in\n",
        "        `data/HadCRUT.5.0.2.0.analysis.summary_series.global.annual.csv`.\n",
        "        This data is averaged over the Northern and Southern Hemispheres\n",
        "        and over the whole year.\n",
        "    -   Global mean sea level anomalies (relative to the 1990 mean\n",
        "        global sea level) are in `data/CSIRO_Recons_gmsl_yr_2015.csv`,\n",
        "        courtesy of CSIRO\n",
        "        (<https://www.cmar.csiro.au/sealevel/sl_data_cmar.html>).\n",
        "-   Fit the model under the assumption of normal i.i.d. residuals by\n",
        "    maximizing the likelihood and report the parameter estimates. Note\n",
        "    that you will need another parameter $H_0$ for the initial sea\n",
        "    level. What can you conclude about the relationship between global\n",
        "    mean temperature increases and global mean sea level rise?\n",
        "-   How appropriate was the normal i.i.d. probability model for the\n",
        "    residuals? Use any needed quantitative or qualitative assessments of\n",
        "    goodness of fit to justify your answer. If this was not an\n",
        "    appropriate probability model, what would you change?"
      ],
      "id": "e5d9e076-5d44-4ea4-a514-69bb6c5e6c22"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***Solution***:\n",
        "\n",
        "First, let’s load the data and implement the model with a function."
      ],
      "id": "eb005379-c0a3-4754-8de9-abf92ce6ff2d"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/html": [
              "</div>"
            ]
          }
        }
      ],
      "source": [
        "# load data files\n",
        "slr_data = DataFrame(load(\"data/CSIRO_Recons_gmsl_yr_2015.csv\"))\n",
        "gmt_data = DataFrame(load(\"data/HadCRUT.5.0.2.0.analysis.summary_series.global.annual.csv\"))\n",
        "slr_data[:, :Time] = slr_data[:, :Time] .- 0.5; # remove 0.5 from Times\n",
        "dat = leftjoin(slr_data, gmt_data, on=\"Time\") # join data frames on time\n",
        "select!(dat, [1, 2, 3, 4])  # drop columns we don't need\n",
        "first(dat, 6)"
      ],
      "id": "0dfb1025"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "slr_model (generic function with 1 method)"
            ]
          }
        }
      ],
      "source": [
        "# slr_model: function to simulate sea-level rise from global mean temperature based on the Rahmstorf (2007) model\n",
        "\n",
        "function slr_model(α, T₀, H₀, temp_data)\n",
        "    temp_effect = α .* (temp_data .- T₀)\n",
        "    slr_predict = cumsum(temp_effect) .+ H₀\n",
        "    return slr_predict\n",
        "end"
      ],
      "id": "1a58f95a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let’s fit the model under the normal residual assumption."
      ],
      "id": "50039dfd-d85a-4d77-8852-eaa3f7415cc7"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "4-element Vector{Float64}:\n",
              "    1.863932768247625\n",
              "   -0.9709731494908135\n",
              " -157.3346053534705\n",
              "    5.911511282104595"
            ]
          }
        }
      ],
      "source": [
        "# split data structure into individual pieces\n",
        "years = dat[:, 1]\n",
        "sealevels = dat[:, 2]\n",
        "temp = dat[:, 4]\n",
        "\n",
        "# write function to calculate likelihood of residuals for given parameters\n",
        "# parameters are a vector [α, T₀, H₀, σ]\n",
        "function llik_normal(params, temp_data, slr_data)\n",
        "    slr_out = slr_model(params[1], params[2], params[3], temp_data)\n",
        "    resids = slr_out - slr_data\n",
        "    return sum(logpdf.(Normal(0, params[4]), resids))\n",
        "end\n",
        "\n",
        "# set up lower and upper bounds for the parameters for the optimization\n",
        "lbds = [0.0, -50.0, -200.0, 0.0]\n",
        "ubds = [10.0, 1.0, 0.0, 20.0]\n",
        "p0 = [5.0, -1.0, -100.0, 5.0]\n",
        "p_mle = Optim.optimize(p -> -llik_normal(p, temp, sealevels), lbds, ubds, p0).minimizer"
      ],
      "id": "4ffc3438"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As `p_mle[1]` has a positive value of 1.86, this suggests a positive\n",
        "correlation between global mean temperature and global mean sea levels.\n",
        "Of course, we can’t conclude anything about causality, as it could be\n",
        "that both are independently increasing simultaneously, or that there is\n",
        "a mutual cause of both increases. However, mechanistically it makes\n",
        "sense that temperature increases would increase sea levels, due to\n",
        "melting ice and thermal expansion of ocean waters.\n",
        "\n",
        "To look at the appropriateness of the normal residual assumption, let’s\n",
        "compute the residuals and look at a Q-Q plot and their autocorrelation."
      ],
      "id": "3937b3f4-929a-43ad-a45b-553d5430600e"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# compute the residuals\n",
        "resids = slr_model(p_mle[1], p_mle[2], p_mle[3], temp) - sealevels\n",
        "histogram(resids, xlabel=\"Residual\", ylabel=\"Count\", legend=false)"
      ],
      "id": "cell-fig-p1-histogram"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The residual histogram in\n",
        "<a href=\"#fig-p1-histogram\" class=\"quarto-xref\">Figure 1</a> looks a bit\n",
        "skewed. Let’s look at a Q-Q plot."
      ],
      "id": "eecb2efb-9e0c-48d5-ad3a-af5788dc2000"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "qqplot(Normal, resids, xlabel=\"Normal Theoretical Quantile\", ylabel=\"Sample Residual Quantile\", legend=false)"
      ],
      "id": "cell-fig-p1-qq"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Q-Q plot (<a href=\"#fig-p1-qq\" class=\"quarto-xref\">Figure 2</a>)\n",
        "looks good; there are some discrepancies near the tail, but that is not\n",
        "surprising."
      ],
      "id": "f3fa6549-aa99-4e64-bab8-d8dec1212a14"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "resid_acf = pacf(resids, 1:5)\n",
        "plot(resid_acf , marker=:circle, line=:stem, linewidth=3, markersize=5, legend=false, ylabel=\"Partial Autocorrelation\", xlabel=\"Time Lag\")"
      ],
      "id": "cell-fig-p1-acf"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The bigger problem is that the lag-1 autocorrelation coefficient is\n",
        "0.54, which is not consistent with the normally-distributed assumption.\n",
        "The next thing to try would be an AR(1) model for the residuals.\n",
        "\n",
        "### Problem 2"
      ],
      "id": "ac576a32-001b-44fc-871e-4425dab27926"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tide gauge data is complicated to analyze because it is influenced by\n",
        "different harmonic processes (such as the linear cycle). In this\n",
        "problem, we will develop a model for this data using [NOAA data from the\n",
        "Sewell’s Point tide\n",
        "gauge](https://tidesandcurrents.noaa.gov/waterlevels.html?id=8638610)\n",
        "outside of Norfolk, VA from `data/norfolk-hourly-surge-2015.csv`. This\n",
        "is hourly data (in m) from 2015 and includes both the observed data\n",
        "(`Verified (m)`) and the tide level predicted by NOAA’s sinusoidal model\n",
        "for periodic variability, such as tides and other seasonal cycles\n",
        "(`Predicted (m)`).\n",
        "\n",
        "**In this problem**:\n",
        "\n",
        "-   Load the data file. Take the difference between the observations and\n",
        "    the sinusoidal predictions to obtain the tide level which could be\n",
        "    attributed to weather-related variability (since for one year\n",
        "    sea-level rise and other factors are unlikely to matter). Plot this\n",
        "    data.\n",
        "-   Develop an autoregressive model for the weather-related variability\n",
        "    in the Norfolk tide gauge. Make sure to include your logic or\n",
        "    exploratory analysis used in determining the model specification.\n",
        "-   Use your model to simulate 1,000 realizations of hourly tide gauge\n",
        "    observations. What is the distribution of the maximum tide level?\n",
        "    How does this compare to the observed value?"
      ],
      "id": "999ab4dc-cd1e-4c71-9dd1-52a7db1bbc54"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***Solution***:\n",
        "\n",
        "First, load the data, find the weather-related residuals, and plot. To\n",
        "load and process the data, we will use the `DataFramesMeta.jl` package,\n",
        "which lets us string together commands in a convenient way. This is\n",
        "completely optional, however."
      ],
      "id": "40556b17-6370-4293-bf77-bf579ea788af"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "tide_dat = DataFrame(load(\"data/norfolk-hourly-surge-2015.csv\"))\n",
        "surge_resids = tide_dat[:, 5] - tide_dat[:, 3]\n",
        "plot(surge_resids; ylabel=\"Gauge Measurement (m)\", label=\"Weather-Related Residual\", legend=:topleft, xlabel=\"Hour Number\")"
      ],
      "id": "cell-fig-p2-data"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let’s look at the autocorrelation function of the residuals to determine\n",
        "what the autoregressive lag should be."
      ],
      "id": "7a4a9235-638b-4e17-9fa3-06c735e7a068"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "resid_acf = pacf(surge_resids, 1:5)\n",
        "plot(resid_acf , marker=:circle, line=:stem, linewidth=3, markersize=5, legend=false, ylabel=\"Partial Autocorrelation\", xlabel=\"Time Lag\")"
      ],
      "id": "cell-fig-p2-acf"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that there is a *very* strong autocorrelation at lag 1, but\n",
        "not much additional autocorrelation introduced at lag 2, so we will use\n",
        "an AR(1) model (though you might be justified in using an AR(2) model,\n",
        "as the lag-2 partial autocorrelation is not completely negligible).\n",
        "\n",
        "This AR(1) model is\n",
        "$$y_t = \\rho y_{t+1} + \\varepsilon, \\quad \\varepsilon \\sim N(0, \\sigma).$$\n",
        "Let’s implement this model and fit it. We’ll specify the likelihood\n",
        "based on whitening the residuals and computing the individual normal\n",
        "likelihoods of the resulting terms."
      ],
      "id": "2374461a-4584-4800-b340-8559788854d2"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "2-element Vector{Float64}:\n",
              " 0.9956392098280269\n",
              " 0.019663589611082367"
            ]
          }
        }
      ],
      "source": [
        "function llik_ar(ρ, σ, y)\n",
        "    ll = 0 # initialize log-likelihood counter\n",
        "    ll += logpdf(Normal(0, σ/sqrt(1-ρ^2)), y[1])\n",
        "    for i = 1:length(y)-1\n",
        "        residuals_whitened = y[i+1] - ρ * y[i]\n",
        "        ll += logpdf(Normal(0, σ), residuals_whitened)\n",
        "    end\n",
        "    return ll\n",
        "end\n",
        "\n",
        "# set up lower and upper bounds for the parameters for the optimization\n",
        "lbds = [0.0, 0.0]\n",
        "ubds = [1.0, 5.0]\n",
        "p0 = [0.5, 2.0]\n",
        "p_tide_mle = Optim.optimize(p -> -llik_ar(p[1], p[2], surge_resids), lbds, ubds, p0).minimizer"
      ],
      "id": "2fed8643"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can simulate these residuals and add them back to the NOAA model\n",
        "predictions using the `mapslices()` function. The resulting histogram of\n",
        "the maximum values is shown in\n",
        "<a href=\"#fig-p2-histogram\" class=\"quarto-xref\">Figure 6</a>."
      ],
      "id": "80ef500d-5993-4cf1-8ea2-4f8f3ff1af88"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# T is the length of the simulated series, N the number of simulations\n",
        "function simulate_ar1(ρ, σ, T, N)\n",
        "    y = zeros(T, N) # initialize storage\n",
        "    y[1, :] = rand(Normal(0, σ/sqrt(1-ρ^2)), N) # sample initial value\n",
        "    # simulate the remaining residuals\n",
        "    for t = 2:T\n",
        "        y[t, :] = ρ * y[t-1, :] + rand(Normal(0, σ), N)\n",
        "    end\n",
        "    return y\n",
        "end\n",
        "\n",
        "# simulate new observations\n",
        "simulated_resids = simulate_ar1(p_tide_mle[1], p_tide_mle[2], length(surge_resids), 1000)\n",
        "simulated_obs = mapslices(col -> col + tide_dat[:, 3], simulated_resids, dims=1)\n",
        "# plot histogram\n",
        "max_obs = [maximum(simulated_obs[:, n]) for n in 1:size(simulated_obs, 2)]\n",
        "histogram(max_obs, xlabel=\"Maximum Simulated Gauge Level (m)\", ylabel=\"Count\", legend=false)"
      ],
      "id": "cell-fig-p2-histogram"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This distribution fails to capture the observed maximum 1.98, which\n",
        "could be due to a model structural problem (we might want to allow for\n",
        "more extreme AR(1) innovations than the normal distribution of\n",
        "$\\varepsilon_t$) or just the unlikeliness of that one observation.\n",
        "\n",
        "### Problem 3"
      ],
      "id": "841da938-1846-44c1-bf57-343cb471be03"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As of 2010, Cayuga Lake has frozen in the following years: 1796, 1816,\n",
        "1856, 1875, 1884, 1904, 1912, 1934, 1961, and 1979. Based on this data,\n",
        "we would like to project whether Cayuga Lake is likely to freeze again\n",
        "in the next 25 years.\n",
        "\n",
        "**In this problem**:\n",
        "\n",
        "-   Assuming that observations began in 1780, write down a Bayesian\n",
        "    model for whether Cayuga Lake will freeze in a given year, using a\n",
        "    Bernoulli distribution. How did you select what prior to use?\n",
        "-   Find the maximum *a posteriori* estimate using your model.\n",
        "-   Generate 1,000 realizations of Cayuga Lake freezing occurrences from\n",
        "    1780 to 2010 and check the simulations against the occurrance data.\n",
        "-   Using your model, calculate the probability of Cayuga Lake freezing\n",
        "    at least once in the next 10 years.\n",
        "-   What do you think about the validity of your model, both in terms of\n",
        "    its ability to reproduce historical data and its use to make future\n",
        "    projections? Why might you believe or discount it? What changes\n",
        "    might you make (include thoughts about the prior)?"
      ],
      "id": "3bff5a5a-0faf-4018-8230-1b334564708c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***Solution***:\n",
        "\n",
        "First, let’s enter the data."
      ],
      "id": "fe704d36-9d0a-483c-a781-d6e636d3d195"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "10-element view(::Vector{Bool}, [17, 37, 77, 96, 105, 125, 133, 155, 182, 200]) with eltype Bool:\n",
              " 1\n",
              " 1\n",
              " 1\n",
              " 1\n",
              " 1\n",
              " 1\n",
              " 1\n",
              " 1\n",
              " 1\n",
              " 1"
            ]
          }
        }
      ],
      "source": [
        "fz_yrs = [1796, 1816, 1856, 1875, 1884, 1904, 1912, 1934, 1961, 1979]\n",
        "y = DataFrame(year=1780:2010, freeze=zeros(Bool, length(1780:2010)))\n",
        "# indexin(a, b) finds indices of a which are in b and returns nothing if not found\n",
        "# so we use !isnothing to only get the indices which occur in fz_yrs, and (.!) is needed\n",
        "# to broadcast\n",
        "y[.!(isnothing.(indexin(y.year, fz_yrs))), :freeze] .= true"
      ],
      "id": "970dea81"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using a Bernoulli distribution means that we need\n",
        "$y_t \\sim Bernoulli(p)$, where $p$ is the probability of freezing in\n",
        "year $t$. The maximum-likelihood estimate of $p$ is easy: it’s just the\n",
        "empirical frequency of `true`. But we want to develop a Bayesian model,\n",
        "as this empirically-observed frequency might not actually be the most\n",
        "likely outcome. To do this, we need to specify a prior for $p$, which is\n",
        "usually given as $p \\sim Beta(\\alpha, \\beta)$. We will use prior\n",
        "predictive simulations to determine appropriate Beta parameters."
      ],
      "id": "8ac03c60-33fe-43c3-9000-547550d653e9"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "sim_freeze (generic function with 1 method)"
            ]
          }
        }
      ],
      "source": [
        "function sim_freeze(α, β, nyrs, nsamples)\n",
        "    p = rand(Beta(α, β), nsamples)\n",
        "    fz = sum.(rand.(Bernoulli.(p), nyrs))\n",
        "    return fz\n",
        "end"
      ],
      "id": "c28ffe71"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let’s see what the impact of a $Beta(1, 10)$ prior and a $Beta(1, 20)$\n",
        "prior look like."
      ],
      "id": "f8b8c660-b6f9-4087-a946-1afdee487a8e"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "layout-nrow": 1
      },
      "outputs": [],
      "source": [
        "fz_10 = sim_freeze(1, 10, nrow(y), 1000)\n",
        "p1 = histogram(fz_10, xlabel=\"Number of Freezes\", ylabel=\"Count\", legend=false)\n",
        "plot!(p1, size=(300, 250))\n",
        "\n",
        "fz_20 = sim_freeze(1, 20, nrow(y), 1000)\n",
        "p2 = histogram(fz_20, xlabel=\"Number of Freezes\", ylabel=\"Count\", legend=false)\n",
        "plot!(p2, size=(300, 250))\n",
        "\n",
        "display(p1)\n",
        "display(p2)"
      ],
      "id": "93824301-7a54-4f81-9b54-adccd1284684"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"#fig-freeze-prior-2\" class=\"quarto-xref\">Figure 7 (b)</a> looks\n",
        "reasonable, since we might suspect that freezing shouldn’t occur too\n",
        "frequently. As a result, we’ll work with the $p \\sim Beta(1, 20)$ prior\n",
        "for this model. So the full model specification is:\n",
        "\n",
        "Now we can compute the MAP estimate."
      ],
      "id": "8ff38bc3-62c3-4ef4-9d61-2f49fe6a2f0e"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "0.039999999097374374"
            ]
          }
        }
      ],
      "source": [
        "# compute the log-posterior\n",
        "function lpost(p, dat)\n",
        "    lpri = logpdf(Beta(1, 20), p)\n",
        "    llik = sum(logpdf.(Bernoulli(p), dat))\n",
        "    return lpri + llik\n",
        "end\n",
        "\n",
        "map = Optim.optimize(p -> -lpost(p, y[:, :freeze]), 0, 1).minimizer"
      ],
      "id": "e281d128"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "That gives us a MAP estimate of 0.039999999097374374. Note that your\n",
        "answer might differ depending on the prior you selected. Now we can do\n",
        "simulations and compare the results to the observations. We will do this\n",
        "using histograms to show the number of occurrences, as the model does\n",
        "not account for any temporal changes in frequency."
      ],
      "id": "7560e69a-d61e-414b-9893-557303472420"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "function sim_freeze_map(p, nyrs, nsamples)\n",
        "    fz = mapslices(sum, rand(Bernoulli(p), (nyrs, nsamples)), dims=1)'\n",
        "    return fz\n",
        "end\n",
        "\n",
        "p = histogram(sim_freeze_map(map, nrow(y), 1000), xlabel=\"Number of Freezing Occurrences\", ylabel=\"Count\", label=false)\n",
        "vline!(p, [sum(y[:, :freeze])], label=\"Data\")"
      ],
      "id": "cell-fig-freeze-hist"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The probability of Cayuga Lake freezing in the next 10 years is\n",
        "calculated with:"
      ],
      "id": "2aa1467e-d8a6-4aa2-b4db-df60b92e0c73"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "4.22"
            ]
          }
        }
      ],
      "source": [
        "100 * sum(sim_freeze_map(map, 10, 1000)) / (10*1000)"
      ],
      "id": "1f7fd9c1"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Based on <a href=\"#fig-freeze-hist\" class=\"quarto-xref\">Figure 8</a>,\n",
        "the model seems to replicate the data well (with a slight bias towards\n",
        "fewer freezing events due to the prior). However, the model might be\n",
        "improved with time or temperature dependence, as the original data\n",
        "appears to have more freezing events earlier in the record and fewer\n",
        "after 1950.\n",
        "\n",
        "### Problem 4\n",
        "\n",
        "<span style=\"color:red;\">GRADED FOR 5850 STUDENTS ONLY</span>\n",
        "\n",
        "For the sea-level model in Problem 1, model the model-data discrepancy\n",
        "using an AR(1) process, with observation error modeled as normally\n",
        "distributed with standard deviation given by the uncertainty column in\n",
        "the data file.\n",
        "\n",
        "**In this problem**:\n",
        "\n",
        "-   Find the maximum likelihood estimate of the parameters with this\n",
        "    discrepancy structure. How does the parameter inference change from\n",
        "    the normal i.i.d. estimate in Problem 1?\n",
        "-   Generate 1,000 traces, plot a comparison of the hindcasts to those\n",
        "    from Problem 1, and compare the surprise indices.\n",
        "-   Determine whether you have accounted for autocorrelation in the\n",
        "    residuals appropriately (hint: generate realizations of just the\n",
        "    discrepancy series, compute the resulting residuals from the model\n",
        "    fit + discrepancy, and look at the distribution of autocorrelation\n",
        "    values).\n",
        "-   Which model specification would you prefer and why?\n",
        "\n",
        ":::\n",
        "\n",
        "From the lecture, we can implement the discrepancy structure likelihood\n",
        "as:"
      ],
      "id": "3e465c09-60a1-461e-842c-017daa5b7e55"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "5-element Vector{Float64}:\n",
              "    1.998198372838903\n",
              "   -0.9076726257161041\n",
              " -154.93253804174117\n",
              "    1.3977277644676105\n",
              "    0.8557998637644137"
            ]
          }
        }
      ],
      "source": [
        "function ar_covariance_mat(σ, ρ, y_err)\n",
        "    H = abs.((1:length(y_err)) .- (1:(length(y_err)))') # compute the outer product to get the correlation lags\n",
        "    ζ_var = σ^2 / (1-ρ^2)\n",
        "    Σ = ρ.^H * ζ_var\n",
        "    for i in 1:length(y_err)\n",
        "        Σ[i, i] += y_err[i]^2\n",
        "    end\n",
        "    return Σ\n",
        "end\n",
        "\n",
        "function ar_discrep_log_likelihood(p, σ, ρ, y, m, y_err)\n",
        "    y_pred = m(p)\n",
        "    residuals = y_pred .- y\n",
        "    Σ = ar_covariance_mat(σ, ρ, y_err)\n",
        "    ll = logpdf(MvNormal(zeros(length(y)), Σ), residuals)\n",
        "    return ll\n",
        "end\n",
        "\n",
        "# params vector is [α, T₀, H₀]\n",
        "slr_wrap(params) = slr_model(params[1], params[2], params[3], temp)\n",
        "\n",
        "# maximize log-likelihood within some range\n",
        "# important to make everything a Float instead of an Int \n",
        "lower = [0.0, -5.0, -200.0, 0.0, -1.0]\n",
        "upper = [5.0, 1.0, 0.0, 20.0, 1.0]\n",
        "p0 = [2.0, 0.0, -100.0, 5.0, 0.5]\n",
        "result = Optim.optimize(params -> -ar_discrep_log_likelihood(params[1:end-2], params[end-1], params[end], sealevels, slr_wrap, dat[:, 3]), lower, upper, p0)\n",
        "p_ar_mle = result.minimizer"
      ],
      "id": "aeb0162d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The AR discrepancy model has a higher sea-level sensitivity to\n",
        "temperature and a slightly higher equilibrium temperature and initial\n",
        "condition. Simulating a hindcast:"
      ],
      "id": "16050aaa-a2ae-424b-960f-39e6bd75431c"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "nsamples = 1000\n",
        "Σ = ar_covariance_mat(p_ar_mle[4], p_ar_mle[5], dat[:, 3])\n",
        "residuals_discrep = rand(MvNormal(zeros(nrow(dat)), Σ), nsamples)\n",
        "model_discrep = slr_wrap(p_ar_mle[1:end-2])\n",
        "model_sim_discrep = (residuals_discrep .+ model_discrep)'\n",
        "median_discrep = mapslices(col -> quantile(col, 0.5), model_sim_discrep; dims=1)' # compute median\n",
        "q90_discrep = mapslices(col -> quantile(col, [0.05, 0.95]), model_sim_discrep; dims=1) # compute 90% prediction interval\n",
        "plot(years, median_discrep, linewidth=2, label=\"Discrepancy Model\", ribbon=(median_discrep .- q90_discrep[1, :], q90_discrep[2, :] .- median_discrep), fillalpha=0.2, xlabel=\"Year\", ylabel=\"Sea Level Anomaly (mm)\")\n",
        "scatter!(years, sealevels, label=\"Observations\")"
      ],
      "id": "cell-fig-sealevel-ar-hind"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calculating the surprise index\n",
        "($1 - \\frac{1}{n} \\sum_{i=1}^n \\mathbb{I}_{\\mathcal{I}_\\alpha}(y_i)$) of\n",
        "both 90% intervals:"
      ],
      "id": "891eeef2-45da-4c6f-9d82-265cca23ea3c"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "2×134 Matrix{Float64}:\n",
              " -165.159  -164.663  -161.948  …  46.7122  48.602   52.3031  55.1233\n",
              " -146.702  -145.918  -144.36      65.5852  69.0316  70.9158  74.4833"
            ]
          }
        }
      ],
      "source": [
        "# need to compute the hindcast projection intervals for the iid model\n",
        "slr_iid_out = slr_model(p_mle[1], p_mle[2], p_mle[3], temp)\n",
        "iid_residuals = rand(Normal(0, p_mle[4]), (length(years), nsamples))\n",
        "model_sim_iid = (iid_residuals .+ slr_iid_out)'\n",
        "q90_iid = mapslices(col -> quantile(col, [0.05, 0.95]), model_sim_iid; dims=1) "
      ],
      "id": "a12efd93"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "2-element Vector{Float64}:\n",
              " 8.955223880597014\n",
              " 1.4925373134328357"
            ]
          }
        }
      ],
      "source": [
        "surprise_discrep = 0 # initialize surprise counter\n",
        "surprise_iid = 0\n",
        "# go through the data and check which points are outside of the 90% interval\n",
        "for i = 1:length(sealevels)\n",
        "    ## The || operator is an OR, so returns true if either of the terms are true\n",
        "    if (sealevels[i] < q90_discrep[1, i]) || (q90_discrep[2, i] < sealevels[i])\n",
        "        surprise_discrep += 1\n",
        "    end\n",
        "    if (sealevels[i] < q90_iid[1, i]) || (q90_iid[2, i] < sealevels[i])\n",
        "        surprise_iid += 1\n",
        "    end\n",
        "\n",
        "end\n",
        "si_iid = surprise_iid / length(sealevels) * 100\n",
        "si_discrep = surprise_discrep / length(sealevels) * 100\n",
        "[si_iid; si_discrep]"
      ],
      "id": "580f267f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The AR(1) discrepancy model here actually looks underconfident, which\n",
        "might suggest that the variance for the discrepancy process is too large\n",
        "due to the large historical SLR observation errors (which could be\n",
        "accounted for with a tighter prior in a Bayesian setup). But we should\n",
        "also check if we did account for the residual autocorrelation that we\n",
        "saw in Problem 1, which we can do by generating simulations without\n",
        "observation error."
      ],
      "id": "af07e096-9d9b-476f-8aec-a11736f1b96c"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "nsamples = 1000\n",
        "Σ = ar_covariance_mat(p_ar_mle[4], p_ar_mle[5], zeros(nrow(dat)))\n",
        "residuals_discrep = rand(MvNormal(zeros(nrow(dat)), Σ), nsamples)\n",
        "model_discrep = slr_wrap(p_ar_mle[1:end-2])\n",
        "model_sim_discrep = (residuals_discrep .+ model_discrep)'\n",
        "obs_error = model_sim_discrep .- sealevels'\n",
        "obs_error_acf = mapslices(col -> pacf(col, 1:5), obs_error, dims=2)\n",
        "boxplot(obs_error_acf, legend=false, xlabel=\"Autocorrelation Lag\", ylabel=\"Partial Autocorrelation\")"
      ],
      "id": "cell-fig-sealevel-ar-acf"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"#fig-sealevel-ar-acf\" class=\"quarto-xref\">Figure 10</a>\n",
        "suggests that the lag-1 autocorrelation has not actually been accounted\n",
        "for properly, which might suggest that there is a problem with the model\n",
        "structure or that a more sophisticated error model is needed (for\n",
        "example, linking the residuals to the global mean temperature or\n",
        "accounting for clustering in the residual variance). Between the two\n",
        "models, since neither model properly accounted for the lag-1 residual\n",
        "autocorrelation, we might prefer the simpler i.i.d. normal model which\n",
        "had a more appropriate surprise index."
      ],
      "id": "8981e333-6b06-4a20-9308-ee2bea6683a9"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "julia-1.9",
      "display_name": "Julia 1.9.4",
      "language": "julia"
    },
    "language_info": {
      "name": "julia",
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "version": "1.9.4"
    }
  }
}