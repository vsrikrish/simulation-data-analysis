[
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Class Schedule",
    "section": "",
    "text": "This page contains a schedule of the topics, content, and assignments for the semester. Note that this schedule will be updated as necessary the semester progresses, with all changes documented here.\nInstructions to save the slides as PDFs can be found here. The key is to hit E when the slides are open in your browser (ideally Chrome, but it may work in others) to toggle into PDF print mode.\nReadings can be accessed using the link (when needed, through the Cornell library with a Cornell login) or on Canvas.\n\n\n\n\n\n\n\n\n\n\n\n\nWeek\nDate\nTopic\nSlides\nReading\nExercises\nHomework\n\n\n\n\n\n\nHypothesis Testing and Modeling\n\n\n\n\n\n\n1\nMon, Jan 22\nOverview, GitHub Intro\n\nRockmore (2024)\n\n\n\n\n\nWed, Jan 24\nUncertainty and Probability Review\n\n\n\n\n\n\n2\nMon, Jan 29\nProbability Models and Model Residuals\n\nStein (2020)\n\n\n\n\n\nWed, Jan 31\nProbability Models II\n\n\n\n\n\n\n3\nMon, Feb 5\nExploratory Data Analysis\n\n\n\n\n\n\n\nWed, Feb 7\nMore on Discrepancy and Bayesian Statistics\n\n\n\n\n\n\n4\nMon, Feb 12\nMore Bayes and Extreme Values\n\nDoss-Gollin et al (2021)\n\n\n\n\n\nWed, Feb 14\nNo Class: Project Work\n\n\n\n\n\n\n5\nMon, Feb 19\nExtreme Value Theory and Models\n\nArns et al (2013)\n\n\n\n\n\nWed, Feb 21\nData Visualization\n\n\n\n\n\n\n6\nMon, Feb 26\nNo Class: February Break\n\n\n\n\n\n\n\nWed, Feb 28\nIn-Class Figure Discussion\n\n\n\n\n\n\n\n\nSimulation Methods\n\n\n\n\n\n\n7\nMon, Mar 4\nMonte Carlo Simulation\n\nKale (2021)\n\n\n\n\n\nWed, Mar 6\nSampling Distributions and The Bootstrap\n\n\n\n\n\n\n8\nMon, Mar 11\nThe Parametric Bootstrap\n\nRahmstorf & Coumou (2011)\n\n\n\n\n\nWed, Mar 13\nBayesian Computation\n\n\n\n\n\n\n9\nMon, Mar 18\nMarkov Chain Monte Carlo\n\nRuckert et al (2017)\n\n\n\n\n\nWed, Mar 20\nMCMC Convergence and Example\n\n\n\n\n\n\n10\nMon, Mar 25\nMCMC Lab\n\n\n\n\n\n\n\nWed, Mar 27\nLiterature Review Presentations\n\n\n\n\n\n\n\nMon, Apr 1\nNo Class: Spring Break\n\n\n\n\n\n\n\nWed, Apr 3\nNo Class: Spring Break\n\n\n\n\n\n\n\n\nModel Assessment and Selection\n\n\n\n\n\n\n11\nMon, Apr 8\nNo Class: Eclipse\n\n\n\n\n\n\n\nWed, Apr 10\nHypothesis Testing\n\n\n\n\n\n\n12\nMon, Apr 15\nModel Assessment\n\n\n\n\n\n\n\nWed, Apr 17\nCross-Validation and Predictive Assessment\n\n\n\n\n\n\n13\nMon, Apr 22\nInformation Criteria\n\n\n\n\n\n\n\nWed, Apr 24\nOther Information Criteria\n\n\n\n\n\n\n\n\nEmulation\n\n\n\n\n\n\n14\nMon, Apr 29\nModel Complexity and Emulation\n\n\n\n\n\n\n\nWed, May 1\nEmulation Methods\n\n\n\n\n\n\n15\nMon, May 6\nProject Presentations",
    "crumbs": [
      "Course Information",
      "Class Schedule"
    ]
  },
  {
    "objectID": "lit_review.html",
    "href": "lit_review.html",
    "title": "Literature Review Instructions",
    "section": "",
    "text": "For the literature review, you should pick a paper of your choice involving some aspect of data analysis and critically evaluate its statistical and scientific choices. The deliverables are:\n\nA short (10 minute) presentation, to be given in-class on 3/27 (submit this on Canvas on 3/28, no later than 9pm). Your presentation should concisely describe the paper’s key question, its scientific hypothesis, a brief overview of how those hypotheses were represented by the data and/or model, and a discussion of whether these choices supported the conclusions that the paper drew about the question. Note that a 10 minute presentation means that you should aim for no more than 5-6 slides (not including titles, transitions, or wrap-ups/references).\nA short written evaluation (2-3 pages, not including figures or references) expanding on the topics in the presentation, to be submitted on 3/29.\nFor students in 5850: Write a peer review for the paper, to be submitted on 5/6. This will require a more in-depth summary and critique of the scientific choices as well as the interpretability and utility of the figures. Try to identify places where the paper fails to make its central argument (or makes it well!) or where insufficient detail is provided for the claims that the paper is making and provide constructive ideas for what might improve the paper’s claims or exposition.",
    "crumbs": [
      "Literature Review Instructions"
    ]
  },
  {
    "objectID": "tutorials/julia-plots.html",
    "href": "tutorials/julia-plots.html",
    "title": "Tutorial: Making Plots with Julia",
    "section": "",
    "text": "This tutorial will give some examples of plotting and plotting features in Julia, as well as providing references to some relevant resources. The main plotting library is Plots.jl, but there are some others that provide useful features.",
    "crumbs": [
      "Julia Tutorials",
      "Making Plots"
    ]
  },
  {
    "objectID": "tutorials/julia-plots.html#overview",
    "href": "tutorials/julia-plots.html#overview",
    "title": "Tutorial: Making Plots with Julia",
    "section": "",
    "text": "This tutorial will give some examples of plotting and plotting features in Julia, as well as providing references to some relevant resources. The main plotting library is Plots.jl, but there are some others that provide useful features.",
    "crumbs": [
      "Julia Tutorials",
      "Making Plots"
    ]
  },
  {
    "objectID": "tutorials/julia-plots.html#some-resources",
    "href": "tutorials/julia-plots.html#some-resources",
    "title": "Tutorial: Making Plots with Julia",
    "section": "Some Resources",
    "text": "Some Resources\n\nPlots.jl useful tips\nPlots.jl examples\nPlot attributes\nAxis attributes\nColor names",
    "crumbs": [
      "Julia Tutorials",
      "Making Plots"
    ]
  },
  {
    "objectID": "tutorials/julia-plots.html#demos",
    "href": "tutorials/julia-plots.html#demos",
    "title": "Tutorial: Making Plots with Julia",
    "section": "Demos",
    "text": "Demos\n\nusing Plots\nusing Random\nRandom.seed!(1);\n\n\nLine Plots\nTo generate a basic line plot, use plot.\n\ny = rand(5)\nplot(y, label=\"original data\", legend=:topright)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere’s a lot of customization here that can occur, a lot of which is discussed in the docs or can be found with some Googling.\n\n\nAdding Plot Elements\nNow we can add some other lines and point markers.\n\ny2 = rand(5)\ny3 = rand(5)\nplot!(y2, label=\"new data\")\nscatter!(y3, label=\"even more data\")\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRemember that an exclamation mark (!) at the end of a function name means that function modifies an object in-place, so plot! and scatter! modify the current plotting object, they don’t create a new plot.\n\n\nRemoving Plot Elements\nSometimes we want to remove legends, axes, grid lines, and ticks.\n\nplot!(legend=false, axis=false, grid=false, ticks=false)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nAspect Ratio\nIf we want to have a square aspect ratio, use ratio = 1.\n\nv = rand(5)\nplot(v, ratio=1, legend=false)\nscatter!(v)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHeatmaps\nA heatmap is effectively a plotted matrix with colors chosen according to the values. Use clim to specify a fixed range for the color limits.\n\nA = rand(10, 10)\nheatmap(A, clim=(0, 1), ratio=1, legend=false, axis=false, ticks=false)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\nM = [ 0 1 0; 0 0 0; 1 0 0]\nwhiteblack = [RGBA(1,1,1,0), RGB(0,0,0)]\nheatmap(c=whiteblack, M, aspect_ratio = 1, ticks=.5:3.5, lims=(.5,3.5), gridalpha=1, legend=false, axis=false, ylabel=\"i\", xlabel=\"j\")\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCustom Colors\n\nusing Colors\n\nmycolors = [colorant\"lightslateblue\",colorant\"limegreen\",colorant\"red\"]\nA = [i for i=50:300, j=1:100]\nheatmap(A, c=mycolors, clim=(1,300))\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\nPlotting Areas Under Curves\n\ny = rand(10)\nplot(y, fillrange= y.*0 .+ .5, label= \"above/below 1/2\", legend =:top)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nx = LinRange(0,2,100)\ny1 = exp.(x)\ny2 = exp.(1.3 .* x)\nplot(x, y1, fillrange = y2, fillalpha = 0.35, c = 1, label = \"Confidence band\", legend = :topleft)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nx = -3:.01:3\nareaplot(x, exp.(-x.^2/2)/√(2π),alpha=.25,legend=false)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nM = [1 2 3; 7 8 9; 4 5 6; 0 .5 1.5]\nareaplot(1:3, M, seriescolor = [:red :green :blue ], fillalpha = [0.2 0.3 0.4])\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nusing SpecialFunctions\nf = x-&gt;exp(-x^2/2)/√(2π)\nδ = .01\nplot()\nx = √2 .* erfinv.(2 .*(δ/2 : δ : 1) .- 1)\nareaplot(x, f.(x), seriescolor=[ :red,:blue], legend=false)\nplot!(x, f.(x),c=:black)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlotting Shapes\n\nrectangle(w, h, x, y) = Shape(x .+ [0,w,w,0], y .+ [0,0,h,h])\ncircle(r,x,y) = (θ = LinRange(0,2π,500); (x.+r.*cos.(θ), y.+r.*sin.(θ)))\nplot(circle(5,0,0), ratio=1, c=:red, fill=true)\nplot!(rectangle(5*√2,5*√2,-2.5*√2,-2.5*√2),c=:white,fill=true,legend=false)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlotting Distributions\nThe StatsPlots.jl package is very useful for making various plots of probability distributions.\n\nusing Distributions, StatsPlots\nplot(Normal(2, 5))\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nscatter(LogNormal(0.8, 1.5))\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can also use this functionality to plot distributions of data in tabular data structures like DataFrames.\n\nusing DataFrames\ndat = DataFrame(a = 1:10, b = 10 .+ rand(10), c = 10 .* rand(10))\n@df dat density([:b :c], color=[:black :red])\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEditing Plots Manually\n\npl = plot(1:4,[1, 4, 9, 16])\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npl.attr\n\nRecipesPipeline.DefaultsDict with 30 entries:\n  :dpi                      =&gt; 96\n  :background_color_outside =&gt; :match\n  :plot_titlefontvalign     =&gt; :vcenter\n  :warn_on_unsupported      =&gt; true\n  :background_color         =&gt; RGBA{Float64}(1.0,1.0,1.0,1.0)\n  :inset_subplots           =&gt; nothing\n  :size                     =&gt; (672, 480)\n  :display_type             =&gt; :auto\n  :overwrite_figure         =&gt; true\n  :html_output_format       =&gt; :svg\n  :plot_titlefontfamily     =&gt; :match\n  :plot_titleindex          =&gt; 0\n  :foreground_color         =&gt; RGB{N0f8}(0.0,0.0,0.0)\n  :window_title             =&gt; \"Plots.jl\"\n  :plot_titlefontrotation   =&gt; 0.0\n  :extra_plot_kwargs        =&gt; Dict{Any, Any}()\n  :pos                      =&gt; (0, 0)\n  :plot_titlefonthalign     =&gt; :hcenter\n  :tex_output_standalone    =&gt; false\n  :extra_kwargs             =&gt; :series\n  :thickness_scaling        =&gt; 1\n  :layout                   =&gt; 1\n  :plot_titlelocation       =&gt; :center\n  :plot_titlefontsize       =&gt; 16\n  :plot_title               =&gt; \"\"\n  ⋮                         =&gt; ⋮\n\n\n\npl.series_list[1]\n\nPlots.Series(RecipesPipeline.DefaultsDict(:plot_object =&gt; Plot{Plots.GRBackend() n=1}, :subplot =&gt; Subplot{1}, :label =&gt; \"y1\", :fillalpha =&gt; nothing, :linealpha =&gt; nothing, :linecolor =&gt; RGBA{Float64}(0.0,0.6056031611752245,0.9786801175696073,1.0), :x_extrema =&gt; (NaN, NaN), :series_index =&gt; 1, :markerstrokealpha =&gt; nothing, :markeralpha =&gt; nothing…))\n\n\n\npl[:size]=(300,200)\n\n(300, 200)\n\n\n\npl\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLog-Scaled Axes\n\nxx = .1:.1:10\nplot(xx.^2, xaxis=:log, yaxis=:log)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplot(exp.(x), yaxis=:log)",
    "crumbs": [
      "Julia Tutorials",
      "Making Plots"
    ]
  },
  {
    "objectID": "tutorials/julia-basics.html",
    "href": "tutorials/julia-basics.html",
    "title": "Tutorial: Julia Basics",
    "section": "",
    "text": "This tutorial will give some examples of basic Julia commands and syntax.",
    "crumbs": [
      "Julia Tutorials",
      "Julia Basics"
    ]
  },
  {
    "objectID": "tutorials/julia-basics.html#overview",
    "href": "tutorials/julia-basics.html#overview",
    "title": "Tutorial: Julia Basics",
    "section": "",
    "text": "This tutorial will give some examples of basic Julia commands and syntax.",
    "crumbs": [
      "Julia Tutorials",
      "Julia Basics"
    ]
  },
  {
    "objectID": "tutorials/julia-basics.html#getting-help",
    "href": "tutorials/julia-basics.html#getting-help",
    "title": "Tutorial: Julia Basics",
    "section": "Getting Help",
    "text": "Getting Help\n\nCheck out the official documentation for Julia: https://docs.julialang.org/en/v1/.\nStack Overflow is a commonly-used resource for programming assistance.\nAt a code prompt or in the REPL, you can always type ?functionname to get help.",
    "crumbs": [
      "Julia Tutorials",
      "Julia Basics"
    ]
  },
  {
    "objectID": "tutorials/julia-basics.html#comments",
    "href": "tutorials/julia-basics.html#comments",
    "title": "Tutorial: Julia Basics",
    "section": "Comments",
    "text": "Comments\nComments hide statements from the interpreter or compiler. It’s a good idea to liberally comment your code so readers (including yourself!) know why your code is structured and written the way it is. Single-line comments in Julia are preceded with a #. Multi-line comments are preceded with #= and ended with =#",
    "crumbs": [
      "Julia Tutorials",
      "Julia Basics"
    ]
  },
  {
    "objectID": "tutorials/julia-basics.html#suppressing-output",
    "href": "tutorials/julia-basics.html#suppressing-output",
    "title": "Tutorial: Julia Basics",
    "section": "Suppressing Output",
    "text": "Suppressing Output\nYou can suppress output using a semi-colon (;).\n\n4+8;\n\nThat didn’t show anything, as opposed to:\n\n4+8\n\n12",
    "crumbs": [
      "Julia Tutorials",
      "Julia Basics"
    ]
  },
  {
    "objectID": "tutorials/julia-basics.html#variables",
    "href": "tutorials/julia-basics.html#variables",
    "title": "Tutorial: Julia Basics",
    "section": "Variables",
    "text": "Variables\nVariables are names which correspond to some type of object. These names are bound to objects (and hence their values) using the = operator.\n\nx = 5\n\n5\n\n\nVariables can be manipulated with standard arithmetic operators.\n\n4 + x\n\n9\n\n\nAnother advantage of Julia is the ability to use Greek letters (or other Unicode characters) as variable names. For example, type a backslash followed by the name of the Greek letter (i.e. \\alpha) followed by TAB.\n\nα = 3\n\n3\n\n\nYou can also include subscripts or superscripts in variable names using \\_ and \\^, respectively, followed by TAB. If using a Greek letter followed by a sub- or super-script, make sure you TAB following the name of the letter before the sub- or super-script. Effectively, TAB after you finish typing the name of each \\character.\n\nβ₁ = 10 # The name of this variable was entered with \\beta + TAB + \\_1 + TAB\n\n10\n\n\nHowever, try not to overwrite predefined names! For example, you might not want to use π as a variable name…\n\nπ\n\nπ = 3.1415926535897...\n\n\nIn the grand scheme of things, overwriting π is not a huge deal unless you want to do some trigonometry. However, there are more important predefined functions and variables that you may want to be aware of. Always check that a variable or function name is not predefined!",
    "crumbs": [
      "Julia Tutorials",
      "Julia Basics"
    ]
  },
  {
    "objectID": "tutorials/julia-basics.html#data-types",
    "href": "tutorials/julia-basics.html#data-types",
    "title": "Tutorial: Julia Basics",
    "section": "Data Types",
    "text": "Data Types\nEach datum (importantly, not the variable which is bound to it) has a data type. Julia types are similar to C types, in that they require not only the type of data (Int, Float, String, etc), but also the precision (which is related to the amount of memory allocated to the variable). Issues with precision won’t be a big deal in this class, though they matter when you’re concerned about performance vs. decimal accuracy of code.\nYou can identify the type of a variable or expression with the typeof() function.\n\ntypeof(\"This is a string.\")\n\nString\n\n\n\ntypeof(x)\n\nInt64\n\n\n\nNumeric types\nA key distinction is between an integer type (or Int) and a floating-point number type (or float). Integers only hold whole numbers, while floating-point numbers correspond to numbers with fractional (or decimal) parts. For example, 9 is an integer, while 9.25 is a floating point number. The difference between the two has to do with the way the number is stored in memory. 9, an integer, is handled differently in memory than 9.0, which is a floating-point number, even though they’re mathematically the same value.\n\ntypeof(9)\n\nInt64\n\n\n\ntypeof(9.25)\n\nFloat64\n\n\nSometimes certain function specifications will require you to use a Float variable instead of an Int. One way to force an Int variable to be a Float is to add a decimal point at the end of the integer.\n\ntypeof(9.)\n\nFloat64\n\n\n\n\nStrings\nStrings hold characters, rather than numeric values. Even if a string contains what seems like a number, it is actually stored as the character representation of the digits. As a result, you cannot use arithmetic operators (for example) on this datum.\n\n\"5\" + 5\n\nLoadError: MethodError: no method matching +(::String, ::Int64)\n\n\u001b[0mClosest candidates are:\n\u001b[0m  +(::Any, ::Any, \u001b[91m::Any\u001b[39m, \u001b[91m::Any...\u001b[39m)\n\u001b[0m\u001b[90m   @\u001b[39m \u001b[90mBase\u001b[39m \u001b[90m\u001b[4moperators.jl:578\u001b[24m\u001b[39m\n\u001b[0m  +(\u001b[91m::T\u001b[39m, ::T) where T&lt;:Union{Int128, Int16, Int32, Int64, Int8, UInt128, UInt16, UInt32, UInt64, UInt8}\n\u001b[0m\u001b[90m   @\u001b[39m \u001b[90mBase\u001b[39m \u001b[90m\u001b[4mint.jl:87\u001b[24m\u001b[39m\n\u001b[0m  +(\u001b[91m::ColorTypes.AbstractGray{Bool}\u001b[39m, ::Number)\n\u001b[0m\u001b[90m   @\u001b[39m \u001b[35mColorVectorSpace\u001b[39m \u001b[90m~/.julia/packages/ColorVectorSpace/tLy1N/src/\u001b[39m\u001b[90m\u001b[4mColorVectorSpace.jl:321\u001b[24m\u001b[39m\n\u001b[0m  ...\n\n\nHowever, you can try to tell Julia to interpret a string encoding a numeric character as a numeric value using the parse() function. This can also be used to encode a numeric data as a string.\n\nparse(Int64, \"5\") + 5\n\n10\n\n\nTwo strings can be concatenated using *:\n\n\"Hello\" * \" \" * \"there\"\n\n\"Hello there\"\n\n\n\n\nBooleans\nBoolean variables (or Bools) are logical variables, that can have true or false as values.\n\nb = true\n\ntrue\n\n\nNumerical comparisons, such as ==, !=, or &lt;, return a Bool.\n\nc = 9 &gt; 11\n\nfalse\n\n\nBools are important for logical flows, such as if-then-else blocks or certain types of loops.",
    "crumbs": [
      "Julia Tutorials",
      "Julia Basics"
    ]
  },
  {
    "objectID": "tutorials/julia-basics.html#mathematical-operations",
    "href": "tutorials/julia-basics.html#mathematical-operations",
    "title": "Tutorial: Julia Basics",
    "section": "Mathematical operations",
    "text": "Mathematical operations\nAddition, subtraction, multiplication, and division work as you would expect. Just pay attention to types! The type of the output is influenced by the type of the inputs: adding or multiplying an Int by a Float will always result in a Float, even if the Float is mathematically an integer. Division is a little special: dividing an Int by another Int will still return a float, because Julia doesn’t know ahead of time if the denominator is a factor of the numerator.\n\n3 + 5\n\n8\n\n\n\n3 * 2\n\n6\n\n\n\n3 * 2.\n\n6.0\n\n\n\n6 - 2\n\n4\n\n\n\n9 / 3\n\n3.0\n\n\nRaising a base to an exponent uses ^, not **.\n\n3^2\n\n9\n\n\nJulia allows the use of updating operators to simplify updating a variable in place (in other words, using x += 5 instead of x = x + 5.\n\nBoolean algebra\nLogical operations can be used on variables of type Bool. Typical operators are && (and), || (or), and ! (not).\n\ntrue && true\n\ntrue\n\n\n\ntrue && false\n\nfalse\n\n\n\ntrue || false\n\ntrue\n\n\n\n!true\n\nfalse\n\n\nComparisons can be chained together.\n\n3 &lt; 4 || 8 == 12\n\ntrue\n\n\nWe didn’t do this above, since Julia doesn’t require it, but it’s easier to understand these types of compound expressions if you use parentheses to signal the order of operations. This helps with debugging!\n\n(3 &lt; 4) || (8 == 12)\n\ntrue",
    "crumbs": [
      "Julia Tutorials",
      "Julia Basics"
    ]
  },
  {
    "objectID": "tutorials/julia-basics.html#data-structures",
    "href": "tutorials/julia-basics.html#data-structures",
    "title": "Tutorial: Julia Basics",
    "section": "Data Structures",
    "text": "Data Structures\nData structures are containers which hold multiple values in a convenient fashion. Julia has several built-in data structures, and there are many extensions provided in additional packages.\n\nTuples\nTuples are collections of values. Julia will pay attention to the types of these values, but they can be mixed. Tuples are also immutable: their values cannot be changed once they are defined.\nTuples can be defined by just separating values with commas.\n\ntest_tuple = 4, 5, 6\n\n(4, 5, 6)\n\n\nTo access a value, use square brackets and the desired index. Note: Julia indexing starts at 1, not 0!\n\ntest_tuple[1]\n\n4\n\n\nAs mentioned above, tuples are immutable. What happens if we try to change the value of the first element of test_tuple?\n\ntest_tuple[1] = 5\n\nLoadError: MethodError: no method matching setindex!(::Tuple{Int64, Int64, Int64}, ::Int64, ::Int64)\n\n\nTuples also do not have to hold the same types of values.\n\ntest_tuple_2 = 4, 5., 'h'\ntypeof(test_tuple_2)\n\nTuple{Int64, Float64, Char}\n\n\nTuples can also be defined by enclosing the values in parentheses.\ntest_tuple_3 = (4, 5., 'h')\ntypeof(test_tuple_3)\n\n\nArrays\nArrays also hold multiple values, which can be accessed based on their index position. Arrays are commonly defined using square brackets.\n\ntest_array = [1, 4, 7, 8]\ntest_array[2]\n\n4\n\n\nUnlike tuples, arrays are mutable, and their contained values can be changed later.\n\ntest_array[1] = 6\ntest_array\n\n4-element Vector{Int64}:\n 6\n 4\n 7\n 8\n\n\nArrays also can hold multiple types. Unlike tuples, this causes the array to no longer care about types at all.\n\ntest_array_2 = [6, 5., 'h']\ntypeof(test_array_2)\n\n\nVector{Any} (alias for Array{Any, 1})\n\n\n\nCompare this with test_array:\n\ntypeof(test_array)\n\n\nVector{Int64} (alias for Array{Int64, 1})\n\n\n\n\n\nDictionaries\nInstead of using integer indices based on position, dictionaries are indexed by keys. They are specified by passing key-value pairs to the Dict() method.\n\ntest_dict = Dict(\"A\"=&gt;1, \"B\"=&gt;2)\ntest_dict[\"B\"]\n\n2\n\n\n\n\nComprehensions\nCreating a data structure with more than a handful of elements can be tedious to do by hand. If your desired array follows a certain pattern, you can create structures using a comprehension. Comprehensions iterate over some other data structure (such as an array) implicitly and populate the new data structure based on the specified instructions.\n\n[i^2 for i in 0:1:5]\n\n6-element Vector{Int64}:\n  0\n  1\n  4\n  9\n 16\n 25\n\n\nFor dictionaries, make sure that you also specify the keys.\n\nDict(string(i) =&gt; i^2 for i in 0:1:5)\n\nDict{String, Int64} with 6 entries:\n  \"4\" =&gt; 16\n  \"1\" =&gt; 1\n  \"5\" =&gt; 25\n  \"0\" =&gt; 0\n  \"2\" =&gt; 4\n  \"3\" =&gt; 9",
    "crumbs": [
      "Julia Tutorials",
      "Julia Basics"
    ]
  },
  {
    "objectID": "tutorials/julia-basics.html#functions",
    "href": "tutorials/julia-basics.html#functions",
    "title": "Tutorial: Julia Basics",
    "section": "Functions",
    "text": "Functions\nA function is an object which accepts a tuple of arguments and maps them to a return value. In Julia, functions are defined using the following syntax.\n\nfunction my_actual_function(x, y)\n    return x + y\nend\nmy_actual_function(3, 5)\n\n8\n\n\nFunctions in Julia do not require explicit use of a return statement. They will return the last expression evaluated in their definition. However, it’s good style to explicitly return function outputs. This improves readability and debugging, especially when functions can return multiple expressions based on logical control flows (if-then-else blocks).\nFunctions in Julia are objects, and can be treated like other objects. They can be assigned to new variables or passed as arguments to other functions.\n\ng = my_actual_function\ng(3, 5)\n\n8\n\n\n\nfunction function_of_functions(f, x, y)\n    return f(x, y)\nend\nfunction_of_functions(g, 3, 5)\n\n8\n\n\n\nShort and Anonymous Functions\nIn addition to the long form of the function definition shown above, simple functions can be specified in more compact forms when helpful.\nThis is the short form:\n\nh₁(x) = x^2 # make the subscript using \\_1 + &lt;TAB&gt;\nh₁(4)\n\n16\n\n\nThis is the anonymous form:\n\nx-&gt;sin(x)\n(x-&gt;sin(x))(π/4)\n\n0.7071067811865475\n\n\n\n\nMutating Functions\nThe convention in Julia is that functions should not modify (or mutate) their input data. The reason for this is to ensure that the data is preserved. Mutating functions are mainly appropriate for applications where performance needs to be optimized, and making a copy of the input data would be too memory-intensive.\nIf you do write a mutating function in Julia, the convention is to add a ! to its name, like my_mutating_function!(x).\n\n\nOptional arguments\nThere are two extremes with regard to function parameters which do not always need to be changed. The first is to hard-code them into the function body, which has a clear downside: when you do want to change them, the function needs to be edited directly. The other extreme is to treat them as regular arguments, passing them every time the function is called. This has the downside of potentially creating bloated function calls, particularly when there is a standard default value that makes sense for most function evaluations.\nMost modern languages, including Julia, allow an alternate solution, which is to make these arguments optional. This involves setting a default value, which is used unless the argument is explicitly defined in a function call.\n\nfunction setting_optional_arguments(x, y, c=0.5)\n    return c * (x + y)\nend\n\nsetting_optional_arguments (generic function with 2 methods)\n\n\nIf we want to stick with the fixed value \\(c=0.5\\), all we have to do is call setting_optional_arguments with the x and y arguments.\n\nsetting_optional_arguments(3, 5)\n\n4.0\n\n\nOtherwise, we can pass a new value for c.\n\nsetting_optional_arguments(3, 5, 2)\n\n16\n\n\n\n\nPassing data structures as arguments\nInstead of passing variables individually, it may make sense to pass a data structure, such as an array or a tuple, and then unpacking within the function definition. This is straightforward in long form: access the appropriate elements using their index.\nIn short or anonymous form, there is a trick which allows the use of readable variables within the function definition.\n\nh₂((x,y)) = x*y # enclose the input arguments in parentheses to tell Julia to expect and unpack a tuple\n\nh₂ (generic function with 1 method)\n\n\n\nh₂((2, 3)) # this works perfectly, as we passed in a tuple\n\n6\n\n\n\nh₂(2, 3) # this gives an error, as h₂ expects a single tuple, not two different numeric values\n\nLoadError: MethodError: no method matching h₂(::Int64, ::Int64)\n\n\u001b[0mClosest candidates are:\n\u001b[0m  h₂(::Any)\n\u001b[0m\u001b[90m   @\u001b[39m \u001b[32mMain\u001b[39m \u001b[90m\u001b[4mIn[50]:1\u001b[24m\u001b[39m\n\n\n\nh₂([3, 10]) # this also works with arrays instead of tuples\n\n30\n\n\n\n\nVectorized operations\nJulia uses dot syntax to vectorize an operation and apply it element-wise across an array.\nFor example, to calculate the square root of 3:\n\nsqrt(3)\n\n1.7320508075688772\n\n\nTo calculate the square roots of every integer between 1 and 5:\n\nsqrt.([1, 2, 3, 4, 5])\n\n5-element Vector{Float64}:\n 1.0\n 1.4142135623730951\n 1.7320508075688772\n 2.0\n 2.23606797749979\n\n\nThe same dot syntax is used for arithmetic operations over arrays, since these operations are really functions.\n\n[1, 2, 3, 4] .* 2\n\n4-element Vector{Int64}:\n 2\n 4\n 6\n 8\n\n\nVectorization can be faster and is more concise to write and read than applying the same function to multiple variables or objects explicitly, so take advantage!\n\n\nReturning multiple values\nYou can return multiple values by separating them with a comma. This implicitly causes the function to return a tuple of values.\n\nfunction return_multiple_values(x, y)\n    return x + y, x * y\nend\nreturn_multiple_values(3, 5)\n\n(8, 15)\n\n\nThese values can be unpacked into multiple variables.\n\nn, ν = return_multiple_values(3, 5)\nn\n\n8\n\n\n\nν\n\n15\n\n\n\n\nReturning nothing\nSometimes you don’t want a function to return any values at all. For example, you might want a function that only prints a string to the console.\n\nfunction print_some_string(x)\n    println(\"x: $x\")\n    return nothing\nend\nprint_some_string(42)\n\nx: 42",
    "crumbs": [
      "Julia Tutorials",
      "Julia Basics"
    ]
  },
  {
    "objectID": "tutorials/julia-basics.html#printing-text-output",
    "href": "tutorials/julia-basics.html#printing-text-output",
    "title": "Tutorial: Julia Basics",
    "section": "Printing Text Output",
    "text": "Printing Text Output\nThe Text() function returns its argument as a plain text string. Notice how this is different from evaluating a string!\n\nText(\"I'm printing a string.\")\n\nI'm printing a string.\n\n\nText() is used in this tutorial as it returns the string passed to it. To print directly to the console, use println().\n\nprintln(\"I'm writing a string to the console.\")\n\nI'm writing a string to the console.\n\n\n\nPrinting Variables In a String\nWhat if we want to include the value of a variable inside of a string? We do this using string interpolation, using $variablename inside of the string.\n\nbar = 42\nText(\"Now I'm printing a variable: $bar\")\n\nNow I'm printing a variable: 42",
    "crumbs": [
      "Julia Tutorials",
      "Julia Basics"
    ]
  },
  {
    "objectID": "tutorials/julia-basics.html#control-flows",
    "href": "tutorials/julia-basics.html#control-flows",
    "title": "Tutorial: Julia Basics",
    "section": "Control Flows",
    "text": "Control Flows\nOne of the tricky things about learning a new programming language can be getting used to the specifics of control flow syntax. These types of flows include conditional if-then-else statements or loops.\n\nConditional Blocks\nConditional blocks allow different pieces of code to be evaluated depending on the value of a boolean expression or variable. For example, if we wanted to compute the absolute value of a number, rather than using abs():\n\nfunction our_abs(x)\n    if x &gt;= 0\n        return x\n    else\n        return -x\n    end\nend\n\nour_abs (generic function with 1 method)\n\n\n\nour_abs(4)\n\n4\n\n\n\nour_abs(-4)\n\n4\n\n\nTo nest conditional statements, use elseif.\n\nfunction test_sign(x)\n    if x &gt; 0\n        return Text(\"x is positive.\")\n    elseif x &lt; 0\n        return Text(\"x is negative.\")\n    else\n        return Text(\"x is zero.\")\n    end\nend\n\ntest_sign (generic function with 1 method)\n\n\n\ntest_sign(-5)\n\nx is negative.\n\n\n\ntest_sign(0)\n\nx is zero.\n\n\n\n\nLoops\nLoops allow expressions to be evaluated repeatedly until they are terminated. The two main types of loops are while loops and for loops.\n\nWhile loops\nwhile loops continue to evaluate an expression so long as a specified boolean condition is true. This is useful when you don’t know how many iterations it will take for the desired goal to be reached.\n\nfunction compute_factorial(x)\n    factorial = 1\n    while (x &gt; 1)\n        factorial *= x\n        x -= 1\n    end\n    return factorial\nend\ncompute_factorial(5)\n\n120\n\n\n\nWhile loops can easily turn into infinite loops if the condition is never meaningfully updated. Be careful, and look there if your programs are getting stuck. Also, If the expression in a while loop is false when the loop is reached, the loop will never be evaluated.\n\n\n\nFor loops\nfor loops run for a finite number of iterations, based on some defined index variable.\n\nfunction add_some_numbers(x)\n    total_sum = 0 # initialize at zero since we're adding\n    for i=1:x # the counter i is updated every iteration\n        total_sum += i\n    end\n    return total_sum\nend\nadd_some_numbers(4)\n\n10\n\n\nfor loops can also iterate over explicitly passed containers, rather than iterating over an incrementally-updated index sequence. Use the in keyword when defining the loop.\n\nfunction add_passed_numbers(set)\n    total_sum = 0\n    for i in set # this is the syntax we use when we want i to correspond to different container values\n        total_sum += i\n    end\n    return total_sum\nend\nadd_passed_numbers([1, 3, 5])\n\n9",
    "crumbs": [
      "Julia Tutorials",
      "Julia Basics"
    ]
  },
  {
    "objectID": "tutorials/julia-basics.html#linear-algebra",
    "href": "tutorials/julia-basics.html#linear-algebra",
    "title": "Tutorial: Julia Basics",
    "section": "Linear algebra",
    "text": "Linear algebra\nMatrices are defined in Julia as 2d arrays. Unlike basic arrays, matrices need to contain the same data type so Julia knows what operations are allowed. When defining a matrix, use semicolons to separate rows. Row elements should not be separated by commas.\n\ntest_matrix = [1 2 3; 4 5 6]\n\n2×3 Matrix{Int64}:\n 1  2  3\n 4  5  6\n\n\nYou can also specify matrices using spaces and newlines.\n\ntest_matrix_2 = [1 2 3\n                 4 5 6]\n\n2×3 Matrix{Int64}:\n 1  2  3\n 4  5  6\n\n\nFinally, matrices can be created using comprehensions by separating the inputs by a comma.\n\n[i*j for i in 1:1:5, j in 1:1:5]\n\n5×5 Matrix{Int64}:\n 1   2   3   4   5\n 2   4   6   8  10\n 3   6   9  12  15\n 4   8  12  16  20\n 5  10  15  20  25\n\n\nVectors are treated as 1d matrices.\n\ntest_row_vector = [1 2 3]\n\n1×3 Matrix{Int64}:\n 1  2  3\n\n\n\ntest_col_vector = [1; 2; 3]\n\n3-element Vector{Int64}:\n 1\n 2\n 3\n\n\nMany linear algebra operations on vectors and matrices can be loaded using the LinearAlgebra package.",
    "crumbs": [
      "Julia Tutorials",
      "Julia Basics"
    ]
  },
  {
    "objectID": "tutorials/julia-basics.html#package-management",
    "href": "tutorials/julia-basics.html#package-management",
    "title": "Tutorial: Julia Basics",
    "section": "Package management",
    "text": "Package management\nSometimes you might need functionality that does not exist in base Julia. Julia handles packages using the Pkg package manager. After finding a package which has the functions that you need, you have two options: 1. Use the package management prompt in the Julia REPL (the standard Julia interface; what you get when you type julia in your terminal). Enter this by typing ] at the standard green Julia prompt julia&gt;. This will become a blue pkg&gt;. You can then download and install new packages using add packagename. 2. From the standard prompt, enter using Pkg; Pkg.add(packagename). The packagename package can then be used by adding using packagename to the start of the script.",
    "crumbs": [
      "Julia Tutorials",
      "Julia Basics"
    ]
  },
  {
    "objectID": "exercises/ex03/ex03.html",
    "href": "exercises/ex03/ex03.html",
    "title": "Exercise Set 03: Spurious Correlations",
    "section": "",
    "text": "Due Date\n\n\n\nFriday, 2/9/24, 9:00pm",
    "crumbs": [
      "Exercises",
      "Exercise 3"
    ]
  },
  {
    "objectID": "exercises/ex03/ex03.html#overview",
    "href": "exercises/ex03/ex03.html#overview",
    "title": "Exercise Set 03: Spurious Correlations",
    "section": "Overview",
    "text": "Overview\n\nInstructions\nThe goal of this exercise is for you to find datasets and reason about the relationships (or lack thereof!) between variables.\n\n\nLoad Environment\nThe following code loads the environment and makes sure all needed packages are installed. This should be at the start of most Julia scripts.\n\n\nimport Pkg\nPkg.activate(@__DIR__)\nPkg.instantiate()\n\n\nThe following packages are included in the environment (to help you find other similar packages in other languages). The code below loads these packages for use in the subsequent notebook (the desired functionality for each package is commented next to the package).\n\n\nusing DataFrames # tabular data structure\nusing CSVFiles # reads/writes .csv files\nusing Plots # plotting library\nusing StatsBase # statistical quantities like mean, median, etc\nusing StatsPlots # some additional statistical plotting tools",
    "crumbs": [
      "Exercises",
      "Exercise 3"
    ]
  },
  {
    "objectID": "exercises/ex03/ex03.html#problem",
    "href": "exercises/ex03/ex03.html#problem",
    "title": "Exercise Set 03: Spurious Correlations",
    "section": "Problem",
    "text": "Problem\nFind a single or multiple datasets (don’t just pull from Spurious Correlations!!) where two or more variables appear to be correlated, but this correlation is likely spurious. Plot the relevant variable(s) and show they are correlated through any needed quantiative and/or qualitative means. Explain why you think the correlation is spurious.",
    "crumbs": [
      "Exercises",
      "Exercise 3"
    ]
  },
  {
    "objectID": "exercises/ex03/ex03.html#references",
    "href": "exercises/ex03/ex03.html#references",
    "title": "Exercise Set 03: Spurious Correlations",
    "section": "References",
    "text": "References",
    "crumbs": [
      "Exercises",
      "Exercise 3"
    ]
  },
  {
    "objectID": "exercises/ex08/ex08.html",
    "href": "exercises/ex08/ex08.html",
    "title": "Exercise Set 08: The Bootstrap",
    "section": "",
    "text": "Due Date\n\n\n\nFriday, 3/15/24, 9:00pm",
    "crumbs": [
      "Exercises",
      "Exercise 8"
    ]
  },
  {
    "objectID": "exercises/ex08/ex08.html#overview",
    "href": "exercises/ex08/ex08.html#overview",
    "title": "Exercise Set 08: The Bootstrap",
    "section": "Overview",
    "text": "Overview\n\nInstructions\nThe goal of this exercise is for you to explore the differences in the confidence intervals produced by a non-parametric and parametric bootstrap.\n\n\nLoad Environment\nThe following code loads the environment and makes sure all needed packages are installed. This should be at the start of most Julia scripts.\n\n\nimport Pkg\nPkg.activate(@__DIR__)\nPkg.instantiate()\n\n\nThe following packages are included in the environment (to help you find other similar packages in other languages). The code below loads these packages for use in the subsequent notebook (the desired functionality for each package is commented next to the package).\n\n\nusing Distributions # API to work with statistical distributions\nusing Plots # plotting library\nusing StatsBase # statistical quantities like mean, median, etc\nusing StatsPlots # some additional statistical plotting tools\nusing DataFrames # tabular data storage\nusing DataFramesMeta # API for chaining DataFrames commands\nusing Dates # datetime API\nusing CSV # read/write CSV files\nusing Random",
    "crumbs": [
      "Exercises",
      "Exercise 8"
    ]
  },
  {
    "objectID": "exercises/ex08/ex08.html#problems",
    "href": "exercises/ex08/ex08.html#problems",
    "title": "Exercise Set 08: The Bootstrap",
    "section": "Problems",
    "text": "Problems\n\nProblem 1\nLet’s revisit the 2015 Sewell’s Point tide gauge data, which consists of hourly observations and predicted sea-level based on NOAA’s harmonic model, as shown in Figure 1.\n\nfunction load_data(fname)\n    date_format = \"yyyy-mm-dd HH:MM\"\n    # this uses the DataFramesMeta package -- it's pretty cool\n    return @chain fname begin\n        CSV.File(; dateformat=date_format)\n        DataFrame\n        rename(\n            \"Time (GMT)\" =&gt; \"time\", \"Predicted (m)\" =&gt; \"harmonic\", \"Verified (m)\" =&gt; \"gauge\"\n        )\n        @transform :datetime = (Date.(:Date, \"yyyy/mm/dd\") + Time.(:time))\n        select(:datetime, :gauge, :harmonic)\n        @transform :weather = :gauge - :harmonic\n        @transform :month = (month.(:datetime))\n    end\nend\n\ndat = load_data(\"data/norfolk-hourly-surge-2015.csv\")\n\nplot(dat.datetime, dat.gauge; ylabel=\"Gauge Measurement (m)\", label=\"Observed\", legend=:topleft, xlabel=\"Date/Time\", color=:blue)\nplot!(dat.datetime, dat.harmonic, label=\"Prediction\", color=:orange)\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: 2015 data from the Sewell’s Point tide gauge, including NOAA’s predictions for sea-level harmonics.\n\n\n\n\nWe detrend the data to isolate the weather-induced variability by subtracting the predictions from the observations; the results (following the Julia code) are in dat[:, :weather], visualized in Figure 2.\n\nplot(dat.datetime, dat.weather; ylabel=\"Gauge Weather Variability (m)\", label=\"Detrended Data\", linewidth=1, legend=:topleft, xlabel=\"Date/Time\")\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Detrended 2015 data from the Sewell’s Point tide gauge.\n\n\n\n\nWe would like to understand the uncertainty in an estimate of the median level of weather-induced variability.\nIn this problem:\n\nUse 1,000 non-parametric bootstrap samples to compute a 95% confidence interval for the median.\nAssuming the weather-induced variability is independently and identically distributed according to a normal distribution, compute a 95% confidence interval using 1,000 parametric bootstrap samples.\nCompare the two confidence intervals.\nWhat can you say about the bias of the median as an estimator?",
    "crumbs": [
      "Exercises",
      "Exercise 8"
    ]
  },
  {
    "objectID": "exercises/ex07/ex07.html",
    "href": "exercises/ex07/ex07.html",
    "title": "Exercise Set 07: Monte Carlo Simulations",
    "section": "",
    "text": "Due Date\n\n\n\nFriday, 3/08/24, 9:00pm\nA common engineering problem is to quantify flood risk, which is typically computed by propagating a flood hazard distribution through a depth-damage function relating flood depths to economic damages. A reasonable depth-damage function is a bounded logistic function, \\[d(h) = \\mathbb{1}_{x &gt; 0} \\frac{L}{1 + \\exp(-k (x - x_0))},\\]\nwhere \\(d\\) is the damage as a percent of total structure value, \\(h\\) is the water depth in m, \\(\\mathbb{1}_{x &gt; 0}\\) is the indicator function, \\(L\\) is the maximum loss in USD, \\(k\\) is the slope of the depth-damage relationship, and \\(x_0\\) is the inflection point. We’ll assume \\(L=\\$200,000\\), \\(k=0.75\\), and \\(x_0=4\\).\nFor this problem, suppose that we have two different probability distributions characterizing annual maxima flood depths:\nTo control for the variances in different programming languages and how they implement both the LogNormal and GEV distributions, match your distributions to Figure 1.\nplot(LogNormal(1.5, 0.3), xlims=(0, 10), xlabel=\"Annual Maximum Flood Depth\", ylabel=\"Probability Density\", label=\"LogNormal(1.5, 0.25)\")\nplot!(GeneralizedExtremeValue(4.5, 1.5, 0.3), label=\"GEV(4.5, 1.5, 0.3)\")\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Distributions used in this Monte Carlo exercise.\nWhat are the Monte Carlo estimates of the expected value and the 99% quantile for the annual maximum damage the structure would suffer for each of these flood hazard distributions? How did you ensure your sample size was sufficiently large? Why do you think the estimates differed or did not differ?",
    "crumbs": [
      "Exercises",
      "Exercise 7"
    ]
  },
  {
    "objectID": "exercises/ex07/ex07.html#overview",
    "href": "exercises/ex07/ex07.html#overview",
    "title": "Exercise Set 07: Monte Carlo Simulations",
    "section": "Overview",
    "text": "Overview\n\nInstructions\nThe goal of this exercise is for you to explore how sensitive Monte Carlo estimates can be to the underlying probability distribution(s).\n\n\nLoad Environment\nThe following code loads the environment and makes sure all needed packages are installed. This should be at the start of most Julia scripts.\n\n\nimport Pkg\nPkg.activate(@__DIR__)\nPkg.instantiate()\n\n\nThe following packages are included in the environment (to help you find other similar packages in other languages). The code below loads these packages for use in the subsequent notebook (the desired functionality for each package is commented next to the package).\n\n\nusing Distributions # API to work with statistical distributions\nusing Plots # plotting library\nusing StatsBase # statistical quantities like mean, median, etc\nusing StatsPlots # some additional statistical plotting tools",
    "crumbs": [
      "Exercises",
      "Exercise 7"
    ]
  },
  {
    "objectID": "exercises/ex07/ex07.html#problems",
    "href": "exercises/ex07/ex07.html#problems",
    "title": "Exercise Set 07: Monte Carlo Simulations",
    "section": "Problems",
    "text": "Problems",
    "crumbs": [
      "Exercises",
      "Exercise 7"
    ]
  },
  {
    "objectID": "exercises/ex07/ex07.html#references",
    "href": "exercises/ex07/ex07.html#references",
    "title": "Exercise Set 07: Monte Carlo Simulations",
    "section": "References",
    "text": "References",
    "crumbs": [
      "Exercises",
      "Exercise 7"
    ]
  },
  {
    "objectID": "rubrics/standard.html",
    "href": "rubrics/standard.html",
    "title": "Standard Rubrics",
    "section": "",
    "text": "These rubrics are intended as a description for students and a guide to the grader for how to assign partial credit.",
    "crumbs": [
      "Homework",
      "Standard Rubric"
    ]
  },
  {
    "objectID": "rubrics/standard.html#meta-rubric",
    "href": "rubrics/standard.html#meta-rubric",
    "title": "Standard Rubrics",
    "section": "Meta-Rubric",
    "text": "Meta-Rubric\n\nIndividual problems may vary from these standard rubrics based on the assessed learning outcomes for the problem, but they should provide an overview of what features students ought to be included in a given solution.\nEach bullet point should appear as a separate rubric item in Gradescope. We will use positive grading (points awarded for each component).\nEach standard rubric describes partial credit for a problem with 10 points. Partial credit for (sub)problems worth less than 10 points should be scaled appropriately. Full problems usually combine one of the modeling methods with some interpretation questions, so the rubric will be a combination of the individual components.\nGenerally, rubrics for 10 point “entire problems” can be summarized with the following:\n\n+4 points for an answer with a correct implementation (including model setup).\n+2 points for the correct solution (including details such as labels, units, etc.).\n+4 points for the interpretation. These points may be broken up across subproblems, but the general assignment of points should follow this summary.\n\nNote that the points may not scale with the distribution of work for a given problem. That’s life! It may take more work to derive and implement your model than it does to intrepret your results, but in this class we care a lot about your ability to critically evaluate and interpret modeling results.\nSometimes problems will be broken up into\nThe graders cannot read your mind and will not try to. Submissions that are unclear for any reason, including but not limited to unclear syntax (English or code), uncommented code, lack of reasoning or derivation, too much detail or writing, will not be given credit. The grader has complete discretion here. If something in your solution is ambiguous, the grader has been instructed to interpret it the “wrong” way. Clear responses are a sign of understanding, which is part of what we’re assessing.\nYou will not be doubly penalized for getting the “right” solution to the “wrong” setup (you would have been penalized above), but this requires the grader to be able to easily identify that your implementation is correct given the wrong model. If your implementation is unclear, you are likely to lose points here because we have no way of knowing that your answer is “right” without re-coding your problem.\nYou will not be given credit for your code (this isn’t a programming class); the code is a means to solving the problem, and we’re more interested in how you set up the problem and interpret the solution. This does mean that code that is sloppy but works is perfectly acceptable. However, your code may get you partial credit if the TA can easily find where you made a mistake, so make sure your submitted code is well commented. It’s important to write down the mathematics of what your code is trying to implement, because the TA will only take a cursory look at your code.\nSome rubrics include “deadly sins” which will cause an immediate zero to be given for the problem. Do not do these!\nRegardless of the problem type, the following penalties will be applied per incident:\n\n-1 for missing units.",
    "crumbs": [
      "Homework",
      "Standard Rubric"
    ]
  },
  {
    "objectID": "rubrics/standard.html#standard-rubrics",
    "href": "rubrics/standard.html#standard-rubrics",
    "title": "Standard Rubrics",
    "section": "Standard Rubrics",
    "text": "Standard Rubrics\n\nSimulation Rubric\n10 points =\n\n+4 for the model derivation.\n\nThis includes any relevant reasoning from mass-balance or other principles.\n\n+4 for discretizing the model correctly.\n\n-1 if no justification is provided for the chosen step-size(s).\n\n+2 points for obtaining the correct solution.\n\n-1 if the error is from a minor bug. I recommend providing an explicit sketch of your code’s procedure, so if that is correct any differences are likely to be the result of a minor bug, such as a misentered number (not ideal, but you understand what you’re doing!). Otherwise you are relying on the TA to interpret your code.\n\n\n\n\nMonte Carlo Rubric\n10 points =\n\n+4 for a clear English explanation of the sampling plan\n\nYou must justify each distribution used, including your choice of mean and standard deviation.\n\n+2 for justification of Monte Carlo sample size.\n\nDeadly Sin: No points will be given for a Monte Carlo solution with an arbitrary sample size.\n\n+2 for an estimate of the Monte Carlo standard error.\n+1 for setting a seed for reproducibility.\n+2 for correct estimate.\n\nDeviations from the posted solution are ok if they’re within the Monte Carlo standard error.\n\n\n\n\nFigure Rubrics\n10 points =\n\n+2 for appropriate choice of axes.\n\nDeadly Sin!: No points will be given for the figure if the axes are not labelled.\n\n+4 for the correct data series.\n+2 for a descriptive legend.\n+2 for a succinct description or caption.\n\n-1 for a description which is too wordy but contains the relevant information.\n\n\n\n\nInterpretation Rubrics\n10 points =\n\n+4 for specific reference to the modeling results;\n\n-2 if the results are not specifically referenced and the reader has to refer to the previous problems to understand the interpretation.\n\n+6 for thoughtfulness of the interpretation;\n\n-4 for not specifically referencing relevant model assumptions;\nDeadly Sin: Do not neglect fundamental engineering principles in any of your interpretations. You will be given a zero if you make a recommendation which e.g. violates an engineering code or standard.\nDeadly Sin: Completely generic interpretations will result in zero points.",
    "crumbs": [
      "Homework",
      "Standard Rubric"
    ]
  },
  {
    "objectID": "project/report.html",
    "href": "project/report.html",
    "title": "Project Report Instructions",
    "section": "",
    "text": "These are the instructions for your final project report. The report should be submitted on Gradescope during finals week, but no later than May 17, 2024."
  },
  {
    "objectID": "project/report.html#report-guidelines",
    "href": "project/report.html#report-guidelines",
    "title": "Project Report Instructions",
    "section": "Report Guidelines",
    "text": "Report Guidelines\nThe proposal should have the following components with 1 inch margins (on all sides) and at least 11 point font. Some of this may be repeated from your proposal and/or simulation study; this is fine.\n\nBackground: Provide background on your project topic and the relevant environmental system. Why is this topic interesting or valuable to understand? What are you trying to understand or what do you hyptohesize?\nData and Models: What data are you using? Provide exploratory plots. What model(s) are you using to explore the data and your hypotheses? Include details about the probability or statistical assumptions your model is making and how well-founded you think these are in light of the system and the data.\nResults: What have you learned from your analysis? Were your hypotheses well founded? What visual or quantative evidence are you using to support these conclusions?\nDiscussion: Reflect on the data and models that you used. How well suited were they for the analysis? In retrospect, what might have been better? How might they have biased or influenced your results?\nReferences: Make sure to include any external sources referenced during the course of completing your project.\n\nYou do not need to include any code in your reports, but if you do, please reserve this for an appendix at the end of the report, not mixed in with the above sections. If you generated a figure or fit a model using the code, include the figure along with the relevant text but do not include code. There is no page minimum or maximum on the report, but it should include, clearly, all of the above content."
  },
  {
    "objectID": "project/presentation.html",
    "href": "project/presentation.html",
    "title": "Project Presentation Instructions",
    "section": "",
    "text": "m These are the instructions for your final project presentation, which will be given in class on May 6, 2024."
  },
  {
    "objectID": "project/presentation.html#presentation-submission",
    "href": "project/presentation.html#presentation-submission",
    "title": "Project Presentation Instructions",
    "section": "Presentation Submission",
    "text": "Presentation Submission\nPlease submit your slides in PDF format by email to Prof. Srikrishnan no later than Friday, May 3, 2024."
  },
  {
    "objectID": "project/presentation.html#presentation-guidelines",
    "href": "project/presentation.html#presentation-guidelines",
    "title": "Project Presentation Instructions",
    "section": "Presentation Guidelines",
    "text": "Presentation Guidelines\nYour presentation should be short (10 minutes, which will be timed and strictly enforced in the interest of time). Your presentation should concisely describe your key question, your scientific hypothesis, and your results and conclusions. Note that a 10 minute presentation means that you should aim for no more than 5-6 slides (not including titles, transitions, or wrap-ups/references)."
  },
  {
    "objectID": "slides/lecture04-1.html#bayesian-probability",
    "href": "slides/lecture04-1.html#bayesian-probability",
    "title": "More Bayes and Extreme Value Modeling",
    "section": "Bayesian Probability",
    "text": "Bayesian Probability\nFrom the Bayesian perspective, probability is interpreted as the degree of belief in an outcome or proposition.\nThere are two different types of random quantities: - Observable quantities, or data (also random for frequentists); - Unobservable quantities, or parameters/structures."
  },
  {
    "objectID": "slides/lecture04-1.html#conditional-probability-notation",
    "href": "slides/lecture04-1.html#conditional-probability-notation",
    "title": "More Bayes and Extreme Value Modeling",
    "section": "Conditional Probability Notation",
    "text": "Conditional Probability Notation\nThen it makes sense to discuss the probability of - model parameters \\(\\mathbf{\\theta}\\) - unobserved data \\(\\tilde{\\mathbf{y}}\\) - model structures \\(\\mathcal{M}\\)\nconditional on the observations \\(\\mathbf{y}\\), which we can denote."
  },
  {
    "objectID": "slides/lecture04-1.html#bayesian-probability-as-conditional-probability",
    "href": "slides/lecture04-1.html#bayesian-probability-as-conditional-probability",
    "title": "More Bayes and Extreme Value Modeling",
    "section": "Bayesian Probability as Conditional Probability",
    "text": "Bayesian Probability as Conditional Probability\nThis fundamental conditioning on observations \\(p(\\mathbf{\\theta} | \\mathbf{y})\\) is a distinguishing feature of Bayesian inference.\nCompare: frequentist approaches are based on re-estimating \\(\\theta^\\mathbf{y}_{\\text{MLE}}\\) over the distribution of possible \\(\\mathbf{y}\\) conditional on the “true” \\(\\hat{\\theta}\\)."
  },
  {
    "objectID": "slides/lecture04-1.html#bayes-rule",
    "href": "slides/lecture04-1.html#bayes-rule",
    "title": "More Bayes and Extreme Value Modeling",
    "section": "Bayes’ Rule",
    "text": "Bayes’ Rule\n\\[\\underbrace{{p(\\theta | y)}}_{\\text{posterior}} = \\frac{\\overbrace{p(y | \\theta)}^{\\text{likelihood}}}{\\underbrace{p(y)}_\\text{normalization}} \\overbrace{p(\\theta)}^\\text{prior}\\]"
  },
  {
    "objectID": "slides/lecture04-1.html#sequential-bayesian-updating",
    "href": "slides/lecture04-1.html#sequential-bayesian-updating",
    "title": "More Bayes and Extreme Value Modeling",
    "section": "Sequential Bayesian Updating",
    "text": "Sequential Bayesian Updating\nCan update sequentially by treating the “old” posterior as the “new” prior:\n\\[p(\\theta | y_{\\text{old}}, y_{\\text{new}}) \\propto p(y_{new} | y_\\text{old}, \\theta) \\color{red}p(y_{\\text{old}} | \\theta) p(\\theta)\\]"
  },
  {
    "objectID": "slides/lecture04-1.html#credible-intervals",
    "href": "slides/lecture04-1.html#credible-intervals",
    "title": "More Bayes and Extreme Value Modeling",
    "section": "Credible Intervals",
    "text": "Credible Intervals\nBayesian credible intervals are straightforward to interpret: \\(\\theta\\) is in \\(I\\) with probability \\(\\alpha\\).\nIn other words, choose \\(I\\) such that \\[p(\\theta \\in I | \\mathbf{y}) = \\alpha.\\]\nThis is not usually a unique choice, but the “equal-tailed interval” between the \\((1-\\alpha)/2\\) and \\((1+\\alpha)/2\\) quantiles is a common choice."
  },
  {
    "objectID": "slides/lecture04-1.html#bayesian-model-components",
    "href": "slides/lecture04-1.html#bayesian-model-components",
    "title": "More Bayes and Extreme Value Modeling",
    "section": "Bayesian Model Components",
    "text": "Bayesian Model Components\nA fully specified Bayesian model includes:\n\nProbability model for the data given the parameters (the likelihood), \\(p(y | \\theta)\\)t\nPrior distributions over the parameters, \\(p(\\theta)\\)"
  },
  {
    "objectID": "slides/lecture04-1.html#prior-probabilities-and-non-identifiability",
    "href": "slides/lecture04-1.html#prior-probabilities-and-non-identifiability",
    "title": "More Bayes and Extreme Value Modeling",
    "section": "Prior Probabilities and Non-Identifiability",
    "text": "Prior Probabilities and Non-Identifiability\nCan deal with non-identifiability issues for complex models, such as hierarchical models by placing appropriate priors to distinguish between components or specify dependence between levels."
  },
  {
    "objectID": "slides/lecture04-1.html#hierarchical-models",
    "href": "slides/lecture04-1.html#hierarchical-models",
    "title": "More Bayes and Extreme Value Modeling",
    "section": "Hierarchical Models",
    "text": "Hierarchical Models\n\\[\\begin{align}\ny_j | \\theta_j, \\phi &\\sim P(y_j | \\theta_j, \\phi) \\nonumber \\\\\n\\theta_j | \\phi &\\sim P(\\theta_j | \\phi) \\nonumber \\\\\n\\phi &\\sim P(\\phi) \\nonumber\n\\end{align}\\]"
  },
  {
    "objectID": "slides/lecture04-1.html#mixture-models",
    "href": "slides/lecture04-1.html#mixture-models",
    "title": "More Bayes and Extreme Value Modeling",
    "section": "Mixture Models",
    "text": "Mixture Models\n\\[\\begin{align}\ny_j | \\theta_{z_j} &\\sim P(y_j | \\theta_{z_j}) \\\\\nz_j &\\sim \\text{Categorical}(\\phi) \\\\\n\\theta_{z_j} &\\sim P(\\theta_{z_j}) \\\\\n\\phi &\\sim P(\\phi)\n\\end{align}\\]"
  },
  {
    "objectID": "slides/lecture04-1.html#generative-modeling",
    "href": "slides/lecture04-1.html#generative-modeling",
    "title": "More Bayes and Extreme Value Modeling",
    "section": "Generative Modeling",
    "text": "Generative Modeling\nBayesian models lend themselves towards generative simulation by generating new data \\(\\tilde{y}\\) through the posterior predictive distribution:\n\\[p(\\tilde{y} | \\mathbf{y}) = \\int_{\\Theta} p(\\tilde{y} | \\theta) p(\\theta | \\mathbf{y}) d\\theta\\]\nThis allows us to test the model through simulation (e.g. hindcasting) and generate probabilistic predictions."
  },
  {
    "objectID": "slides/lecture04-1.html#what-makes-a-good-prior",
    "href": "slides/lecture04-1.html#what-makes-a-good-prior",
    "title": "More Bayes and Extreme Value Modeling",
    "section": "What Makes A Good Prior?",
    "text": "What Makes A Good Prior?\n\nReflects level of understanding (informative vs. weakly informative vs. non-informative).\nDoes not zero out probability of plausible values.\nRegularization (extreme values should be less probable)"
  },
  {
    "objectID": "slides/lecture04-1.html#what-makes-a-bad-prior",
    "href": "slides/lecture04-1.html#what-makes-a-bad-prior",
    "title": "More Bayes and Extreme Value Modeling",
    "section": "What Makes A Bad Prior?",
    "text": "What Makes A Bad Prior?\n\nAssigns probability zero to plausible values;\nWeights implausible values equally as more plausible ones;\nDouble counts information (e.g. fitting a prior to data which is also used in the likelihood)\nChosen based on vibes."
  },
  {
    "objectID": "slides/lecture04-1.html#extreme-values-and-environmental-data",
    "href": "slides/lecture04-1.html#extreme-values-and-environmental-data",
    "title": "More Bayes and Extreme Value Modeling",
    "section": "Extreme Values and Environmental Data",
    "text": "Extreme Values and Environmental Data\n\n\n\n\nSource: Doss-Gollin & Keller (2023)"
  },
  {
    "objectID": "slides/lecture04-1.html#extreme-values-and-environmental-data-1",
    "href": "slides/lecture04-1.html#extreme-values-and-environmental-data-1",
    "title": "More Bayes and Extreme Value Modeling",
    "section": "Extreme Values and Environmental Data",
    "text": "Extreme Values and Environmental Data\n\nStreamflow\nPrecipitation\nStorm tides\nHeat waves\nWind speeds"
  },
  {
    "objectID": "slides/lecture04-1.html#extreme-values-and-risk",
    "href": "slides/lecture04-1.html#extreme-values-and-risk",
    "title": "More Bayes and Extreme Value Modeling",
    "section": "Extreme Values and Risk",
    "text": "Extreme Values and Risk\n\n\n\n\nSource: XKCD 2107"
  },
  {
    "objectID": "slides/lecture04-1.html#return-periods-and-return-levels",
    "href": "slides/lecture04-1.html#return-periods-and-return-levels",
    "title": "More Bayes and Extreme Value Modeling",
    "section": "Return Periods and Return Levels",
    "text": "Return Periods and Return Levels\n\nExceedance probability (often framed as annual exceedance probability (AEP)) \\(p\\)\nReturn period or recurrence interval: $T=\nReturn level: Value that will be exceeded with probability \\(p=\\frac{1}{T}\\), aka the \\(1-p\\) quantile."
  },
  {
    "objectID": "slides/lecture04-1.html#why-is-modelingextrapolating-extremes-challenging",
    "href": "slides/lecture04-1.html#why-is-modelingextrapolating-extremes-challenging",
    "title": "More Bayes and Extreme Value Modeling",
    "section": "Why Is Modeling/Extrapolating Extremes Challenging?",
    "text": "Why Is Modeling/Extrapolating Extremes Challenging?\n\n\nNot much data\nSensitive to small changes in distributional assumptions/tail areas (Exercise 4!)"
  },
  {
    "objectID": "slides/lecture04-1.html#impact-of-distribution-change-on-extremes",
    "href": "slides/lecture04-1.html#impact-of-distribution-change-on-extremes",
    "title": "More Bayes and Extreme Value Modeling",
    "section": "Impact of Distribution Change on Extremes",
    "text": "Impact of Distribution Change on Extremes\n\nCode\nxhi = quantile(Normal(0, 2), 0.975)\nxlo = quantile(Normal(0, 2), 0.025)\n\np1 = plot(Normal(0, 2), label=\"Base\", linewidth=2, color=:black, xticks=:false, yticks=:false, guidefontsize=16, legendfontsize=14, tickfontsize=14)\nplot!(Normal(0.5, 2), label=\"Shifted Mean\", linewidth=2, color=:black, linestyle=:dash)\nplot!(xhi:0.01:8, Normal(0, 2), color=:red, linewidth=0, fill=(0, 0.5, :red), label=false)\nplot!(xhi:0.01:8, Normal(0.5, 2), color=:red, linewidth=0, fill=(0, 0.5, :red), label=false)\nxlabel!(\"Variable\")\nplot!(size=(500, 400))\n\nnsamples = 100000\nxhi = quantile(Normal(0, 2), 0.99)\nxbase = rand(Normal(0, 2), nsamples)\nxshift = rand(Normal(0.5, 2), nsamples)\np2 = plot(sort(xbase), 1(nsamples:-1:1) ./ nsamples, label=\"Base\", color=:black, linewidth=2, xticks=:false, guidefontsize=16, legendfontsize=14, tickfontsize=14)\nxlabel!(\"Variable\")\nxlims!((-7, 6))\nylims!((1/1000, 1))\nplot!(sort(xshift), 1(nsamples:-1:1) ./ nsamples, color=:black, linestyle=:dash, linewidth=2, label=\"Shifted Mean\", legend=:bottomleft)\nyticks = [1, 1/10, 1/50, 1/100, 1/1000]\nyaxis!(\"Exceedance Probability\", yticks, :log, formatter=y -&gt; string(round(y; digits=3)))\nhline!([1-cdf(Normal(0, 2), xhi)], color=:red, label=:false)\nhline!([1-cdf(Normal(0.5, 2), xhi)], color=:red, label=:false, linestyle=:dash, linewidth=2)\nvline!([xhi], color=:blue, linewidth=2, label=:false)\nyticks = [1, 1/10, 1/50, 1/100, 1/1000]\nyaxis!(\"Exceedance Probability\", yticks, :log, formatter=y -&gt; string(round(y; digits=3)))\nplot!(size=(500, 400))\n\ndisplay(p1)\ndisplay(p2)\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Impact of distributional assumptions on extreme frequencies.\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure 1"
  },
  {
    "objectID": "slides/lecture04-1.html#frequency-of-cccurrence-extreme-impacts",
    "href": "slides/lecture04-1.html#frequency-of-cccurrence-extreme-impacts",
    "title": "More Bayes and Extreme Value Modeling",
    "section": "Frequency of Cccurrence ≠ Extreme impacts",
    "text": "Frequency of Cccurrence ≠ Extreme impacts\n\n\n\n\nExtremity of Winter Storm Uri Heat Demands\n\n\n\n\nSource: Doss-Gollin et al. (2021)"
  },
  {
    "objectID": "slides/lecture04-1.html#two-ways-to-frame-extremes",
    "href": "slides/lecture04-1.html#two-ways-to-frame-extremes",
    "title": "More Bayes and Extreme Value Modeling",
    "section": "Two Ways to Frame “Extremes”",
    "text": "Two Ways to Frame “Extremes”\n\n\nWhat is the distribution of “block” extremes, e.g. annual maxima (block maxima)?\nWhat is the distribution of extremes which exceed a certain value (peaks over threshold)?"
  },
  {
    "objectID": "slides/lecture04-1.html#example-tide-gauge-data",
    "href": "slides/lecture04-1.html#example-tide-gauge-data",
    "title": "More Bayes and Extreme Value Modeling",
    "section": "Example: Tide Gauge Data",
    "text": "Example: Tide Gauge Data\n\n\nCode\nfunction load_data(fname)\n    date_format = \"yyyy-mm-dd HH:MM\"\n    # this uses the DataFramesMeta package -- it's pretty cool\n    return @chain fname begin\n        CSV.File(; dateformat=date_format)\n        DataFrame\n        rename(\n            \"Time (GMT)\" =&gt; \"time\", \"Predicted (m)\" =&gt; \"harmonic\", \"Verified (m)\" =&gt; \"gauge\"\n        )\n        @transform :datetime = (Date.(:Date, \"yyyy/mm/dd\") + Time.(:time))\n        select(:datetime, :gauge, :harmonic)\n        @transform :weather = :gauge - :harmonic\n        @transform :month = (month.(:datetime))\n    end\nend\n\ndat = load_data(\"data/surge/norfolk-hourly-surge-2015.csv\")\n\np1 = plot(dat.datetime, dat.gauge; ylabel=\"Gauge Measurement (m)\", label=\"Observed\", legend=:topleft, tickfontsize=14, guidefontsize=16, legendfontsize=14, xlabel=\"Date/Time\", bottom_margin=5mm, left_margin=5mm, right_margin=5mm)\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: 2015 tide gauge data from the Norfolk, VA tide gauge."
  },
  {
    "objectID": "slides/lecture04-1.html#example-tide-gauge-data-1",
    "href": "slides/lecture04-1.html#example-tide-gauge-data-1",
    "title": "More Bayes and Extreme Value Modeling",
    "section": "Example: Tide Gauge Data",
    "text": "Example: Tide Gauge Data\n\n\nCode\nplot!(p1, dat.datetime, dat.harmonic, label=\"Predicted\", alpha=0.7)\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: 2015 tide gauge data with predicted harmonics from the Norfolk, VA tide gauge."
  },
  {
    "objectID": "slides/lecture04-1.html#example-detrended-data",
    "href": "slides/lecture04-1.html#example-detrended-data",
    "title": "More Bayes and Extreme Value Modeling",
    "section": "Example: Detrended Data",
    "text": "Example: Detrended Data\n\n\nCode\nplot(dat.datetime, dat.weather; ylabel=\"Gauge Weather Variability (m)\", label=\"Detrended Data\", linewidth=3, legend=:topleft, tickfontsize=14, guidefontsize=16, legendfontsize=14, xlabel=\"Date/Time\", bottom_margin=5mm, left_margin=5mm, right_margin=5mm)\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: 2015 detrended tide gauge data from the Norfolk, VA tide gauge."
  },
  {
    "objectID": "slides/lecture04-1.html#example-block-maxima",
    "href": "slides/lecture04-1.html#example-block-maxima",
    "title": "More Bayes and Extreme Value Modeling",
    "section": "Example: Block Maxima",
    "text": "Example: Block Maxima\n\n\nCode\np1 = plot(dat.datetime, dat.weather; ylabel=\"Gauge Weather Variability (m)\", label=\"Detrended Data\", linewidth=2, legend=:topleft, tickfontsize=14, guidefontsize=16, legendfontsize=14, xlabel=\"Date/Time\", bottom_margin=5mm, left_margin=5mm, right_margin=5mm)\nmax_dat = combine(dat -&gt; dat[argmax(dat.weather), :], groupby(transform(dat, :datetime =&gt; x-&gt;yearmonth.(x)), :datetime_function))\nscatter!(max_dat.datetime, max_dat.weather, label=\"Monthly Maxima\", markersize=5)\nmonth_start = collect(Date(2015, 01, 01):Dates.Month(1):Date(2015, 12, 01))\nvline!(DateTime.(month_start), color=:black, label=:false, linestyle=:dash)\n\np2 = histogram(\n    max_dat.weather,\n    normalize=:pdf,\n    orientation=:horizontal,\n    label=:false,\n    xlabel=\"PDF\",\n    ylabel=\"\",\n    yticks=[],\n    tickfontsize=16,\n    guidefontsize=18\n)\n\nl = @layout [a{0.7w} b{0.3w}]\nplot(p1, p2; layout=l, link=:y, ylims=(-0.4, 1.4), bottom_margin=5mm, left_margin=5mm)\nplot!(size=(1000, 450))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: 2015 detrended tide gauge data from the Norfolk, VA tide gauge."
  },
  {
    "objectID": "slides/lecture04-1.html#example-peaks-over-threshold",
    "href": "slides/lecture04-1.html#example-peaks-over-threshold",
    "title": "More Bayes and Extreme Value Modeling",
    "section": "Example: Peaks Over Threshold",
    "text": "Example: Peaks Over Threshold\n\n\nCode\nthresh = 0.5\np1 = plot(dat.datetime, dat.weather; linewidth=2, ylabel=\"Gauge Weather Variability (m)\", label=\"Observations\", legend=:top, tickfontsize=14, guidefontsize=16, legendfontsize=14, xlabel=\"Date/Time\")\nhline!([thresh], color=:red, linestyle=:dash, label=\"Threshold\")\nscatter!(dat.datetime[dat.weather .&gt; thresh], dat.weather[dat.weather .&gt; thresh], markershape=:x, color=:black, markersize=3, label=\"Exceedances\")\n\np2 = histogram(\n    dat.weather[dat.weather .&gt; thresh],\n    normalize=:pdf,\n    orientation=:horizontal,\n    label=:false,\n    xlabel=\"PDF\",\n    ylabel=\"\",\n    yticks=[],\n    tickfontsize=16,\n    guidefontsize=18\n)\n\nl = @layout [a{0.7w} b{0.3w}]\nplot(p1, p2; layout=l, link=:y, ylims=(-0.4, 1.4), bottom_margin=5mm, left_margin=5mm)\nplot!(size=(1000, 450))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: 2015 detrended tide gauge data from the Norfolk, VA tide gauge."
  },
  {
    "objectID": "slides/lecture04-1.html#san-francisco-tide-gauge-data",
    "href": "slides/lecture04-1.html#san-francisco-tide-gauge-data",
    "title": "More Bayes and Extreme Value Modeling",
    "section": "San Francisco Tide Gauge Data",
    "text": "San Francisco Tide Gauge Data\n\n\nCode\n# load SF tide gauge data\n# read in data and get annual maxima\nfunction load_data(fname)\n    date_format = DateFormat(\"yyyy-mm-dd HH:MM:SS\")\n    # This uses the DataFramesMeta.jl package, which makes it easy to string together commands to load and process data\n    df = @chain fname begin\n        CSV.read(DataFrame; header=false)\n        rename(\"Column1\" =&gt; \"year\", \"Column2\" =&gt; \"month\", \"Column3\" =&gt; \"day\", \"Column4\" =&gt; \"hour\", \"Column5\" =&gt; \"gauge\")\n        # need to reformat the decimal date in the data file\n        @transform :datetime = DateTime.(:year, :month, :day, :hour)\n        # replace -99999 with missing\n        @transform :gauge = ifelse.(abs.(:gauge) .&gt;= 9999, missing, :gauge)\n        select(:datetime, :gauge)\n    end\n    return df\nend\n\ndat = load_data(\"data/surge/h551.csv\")\n\n# detrend the data to remove the effects of sea-level rise and seasonal dynamics\nma_length = 366\nma_offset = Int(floor(ma_length/2))\nmoving_average(series,n) = [mean(@view series[i-n:i+n]) for i in n+1:length(series)-n]\ndat_ma = DataFrame(datetime=dat.datetime[ma_offset+1:end-ma_offset], residual=dat.gauge[ma_offset+1:end-ma_offset] .- moving_average(dat.gauge, ma_offset))\n\n# group data by year and compute the annual maxima\ndat_ma = dropmissing(dat_ma) # drop missing data\ndat_annmax = combine(dat_ma -&gt; dat_ma[argmax(dat_ma.residual), :], groupby(transform(dat_ma, :datetime =&gt; x-&gt;year.(x)), :datetime_function))\ndelete!(dat_annmax, nrow(dat_annmax)) # delete 2023; haven't seen much of that year yet\nrename!(dat_annmax, :datetime_function =&gt; :Year)\nselect!(dat_annmax, [:Year, :residual])\ndat_annmax.residual = dat_annmax.residual / 1000 # convert to m\n\n# make plots\np1 = plot(\n    dat_annmax.Year,\n    dat_annmax.residual;\n    xlabel=\"Year\",\n    ylabel=\"Annual Max Tide Level (m)\",\n    label=false,\n    marker=:circle,\n    markersize=5,\n    tickfontsize=16,\n    guidefontsize=18\n)\np2 = histogram(\n    dat_annmax.residual,\n    normalize=:pdf,\n    orientation=:horizontal,\n    label=:false,\n    xlabel=\"PDF\",\n    ylabel=\"\",\n    yticks=[],\n    tickfontsize=16,\n    guidefontsize=18\n)\n\nl = @layout [a{0.7w} b{0.3w}]\nplot(p1, p2; layout=l, link=:y, ylims=(1, 1.7), bottom_margin=5mm, left_margin=5mm)\nplot!(size=(1000, 450))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: Annual maxima surge data from the San Francisco, CA tide gauge."
  },
  {
    "objectID": "slides/lecture04-1.html#proposed-probability-model",
    "href": "slides/lecture04-1.html#proposed-probability-model",
    "title": "More Bayes and Extreme Value Modeling",
    "section": "Proposed Probability Model",
    "text": "Proposed Probability Model\n\\[\n\\begin{align*}\n& y \\sim LogNormal(\\mu, \\sigma) \\tag{likelihood}\\\\\n& \\left. \\begin{aligned}\n& \\mu \\sim Normal(0, 1) \\\\\n& \\sigma \\sim HalfNormal(0, 5)\n\\end{aligned} \\right\\} \\tag{priors}\n\\end{align*}\n\\]\nWant to find:\n\\[p(\\mu, \\sigma | y) \\propto p(y | \\mu, \\sigma) p(\\mu)p(\\sigma)\\]"
  },
  {
    "objectID": "slides/lecture04-1.html#are-our-priors-reasonable",
    "href": "slides/lecture04-1.html#are-our-priors-reasonable",
    "title": "More Bayes and Extreme Value Modeling",
    "section": "Are Our Priors Reasonable?",
    "text": "Are Our Priors Reasonable?\nHard to tell! Let’s simulate data to see we get plausible outcomes.\nThis is called a prior predictive check."
  },
  {
    "objectID": "slides/lecture04-1.html#prior-predictive-check",
    "href": "slides/lecture04-1.html#prior-predictive-check",
    "title": "More Bayes and Extreme Value Modeling",
    "section": "Prior Predictive Check",
    "text": "Prior Predictive Check\n\n\nCode\n# sample from priors\nμ_sample = rand(Normal(0, 1), 1_000)\nσ_sample = rand(truncated(Normal(0, 5), 0, +Inf), 1_000)\n\n# define return periods and cmopute return levels for parameters\nreturn_periods = 2:100\nreturn_levels = zeros(1_000, length(return_periods))\nfor i in 1:1_000\n    return_levels[i, :] = quantile.(LogNormal(μ_sample[i], σ_sample[i]), 1 .- (1 ./ return_periods))\nend\n\nplt_prior_1 = plot(; yscale=:log10, yticks=10 .^ collect(0:2:16), ylabel=\"Return Level (m)\", xlabel=\"Return Period (yrs)\",\n    tickfontsize=16, legendfontsize=18, guidefontsize=18, bottom_margin=10mm, left_margin=10mm, legend=:topleft)\nfor idx in 1:1_000\n    label = idx == 1 ? \"Prior\" : false\n    plot!(plt_prior_1, return_periods, return_levels[idx, :]; color=:black, alpha=0.1, label=label)\nend\nplt_prior_1\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8: Prior predictive check of return periods with revised model"
  },
  {
    "objectID": "slides/lecture04-1.html#lets-revise-the-prior",
    "href": "slides/lecture04-1.html#lets-revise-the-prior",
    "title": "More Bayes and Extreme Value Modeling",
    "section": "Let’s Revise the Prior",
    "text": "Let’s Revise the Prior\n\\[\n\\begin{align*}\n& y \\sim LogNormal(\\mu, \\sigma) \\tag{likelihood}\\\\\n& \\left. \\begin{aligned}\n& \\mu \\sim Normal(0, 0.5) \\\\\n& \\sigma \\sim HalfNormal(0, 0.1)\n\\end{aligned} \\right\\} \\tag{priors}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/lecture04-1.html#prior-predictive-check-2",
    "href": "slides/lecture04-1.html#prior-predictive-check-2",
    "title": "More Bayes and Extreme Value Modeling",
    "section": "Prior Predictive Check 2",
    "text": "Prior Predictive Check 2\n\n\nCode\n# sample from priors\nμ_sample = rand(Normal(0, 0.5), 1_000)\nσ_sample = rand(truncated(Normal(0, 0.1), 0, +Inf), 1_000)\n\nreturn_periods = 2:100\nreturn_levels = zeros(1_000, length(return_periods))\nfor i in 1:1_000\n    return_levels[i, :] = quantile.(LogNormal(μ_sample[i], σ_sample[i]), 1 .- (1 ./ return_periods))\nend\n\nplt_prior_2 = plot(; ylabel=\"Return Level (m)\", xlabel=\"Return Period (yrs)\", tickfontsize=16, legendfontsize=18, guidefontsize=18, bottom_margin=10mm, left_margin=10mm)\nfor idx in 1:1_000\n    label = idx == 1 ? \"Prior\" : false\n    plot!(plt_prior_2, return_periods, return_levels[idx, :]; color=:black, alpha=0.1, label=label)\nend\nplt_prior_2\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 9: Prior predictive check of return periods with revised model"
  },
  {
    "objectID": "slides/lecture04-1.html#compute-posterior",
    "href": "slides/lecture04-1.html#compute-posterior",
    "title": "More Bayes and Extreme Value Modeling",
    "section": "Compute Posterior",
    "text": "Compute Posterior\n\n\n\n\nCode\nll(μ, σ) = sum(logpdf(LogNormal(μ, σ), dat_annmax.residual))\nlprior(μ, σ) = logpdf(Normal(0, 0.5), μ) + logpdf(truncated(Normal(0, 0.1), 0, Inf), σ)\nlposterior(μ, σ) = ll(μ, σ) + lprior(μ, σ)\n\np_map = optimize(p -&gt; -lposterior(p[1], p[2]), [0.0, 0.0], [1.0, 1.0], [0.5, 0.5]).minimizer\n\nμ = 0.15:0.005:0.35\nσ = 0.04:0.01:0.1\nposterior_vals = @. lposterior(μ', σ)\n\ncontour(μ, σ, posterior_vals, \n    levels=100, \n    clabels=false, \n    cbar=true, lw=1, \n    fill=(true,cgrad(:grays,[0,0.1,1.0])),\n    tickfontsize=16,\n    guidefontsize=18,\n    legendfontsize=18,\n    right_margin=20mm,\n    bottom_margin=5mm,\n    left_margin=5mm\n)\nscatter!([p_map[1]], [p_map[2]], label=\"MAP\", markersize=10, marker=:star)\nxlabel!(L\"$\\mu$\")\nylabel!(L\"$\\sigma$\")\nplot!(size=(900, 400))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 10: Posterior samples from surge model.\n\n\n\n\n\n\n\n2-element Vector{Float64}:\n 0.2546874075930782\n 0.055422136861588964"
  },
  {
    "objectID": "slides/lecture04-1.html#assess-map-fit",
    "href": "slides/lecture04-1.html#assess-map-fit",
    "title": "More Bayes and Extreme Value Modeling",
    "section": "Assess MAP Fit",
    "text": "Assess MAP Fit\n\nCode\np1 = histogram(\n    dat_annmax.residual,\n    normalize=:pdf,\n    legend=:false,\n    ylabel=\"PDF\",\n    xlabel=\"Annual Max Tide Level (m)\",\n    tickfontsize=16,\n    guidefontsize=18,\n    bottom_margin=5mm, left_margin=5mm\n)\nplot!(p1, LogNormal(p_map[1], p_map[2]),\n    linewidth=3,\n    color=:red)\nxlims!(p1, (1, 1.7))\nplot!(p1, size=(600, 450))\n\nreturn_periods = 2:500\nreturn_levels = quantile.(LogNormal(p_map[1], p_map[2]), 1 .- (1 ./ return_periods))\n\n# function to calculate exceedance probability and plot positions based on data quantile\nfunction exceedance_plot_pos(y)\n    N = length(y)\n    ys = sort(y; rev=false) # sorted values of y\n    nxp = xp = [r / (N + 1) for r in 1:N] # exceedance probability\n    xp = 1 .- nxp\n    return xp, ys\nend\nxp, ys = exceedance_plot_pos(dat_annmax.residual)\n\np2 = plot(return_periods, return_levels, linewidth=3, color=:blue, label=\"Model Fit\", tickfontsize=16, legendfontsize=18, guidefontsize=18, bottom_margin=5mm, left_margin=5mm, right_margin=10mm, legend=:bottomright)\nscatter!(p2, 1 ./ xp, ys, label=\"Observations\", color=:black, markersize=5)\nxlabel!(p2, \"Return Period (yrs)\")\nylabel!(p2, \"Return Level (m)\")\nxlims!(-1, 300)\nplot!(p2, size=(600, 450))\n\ndisplay(p1)\ndisplay(p2)\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Checks for model fit.\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure 11"
  },
  {
    "objectID": "slides/lecture04-1.html#key-points-bayesian-statistics",
    "href": "slides/lecture04-1.html#key-points-bayesian-statistics",
    "title": "More Bayes and Extreme Value Modeling",
    "section": "Key Points: Bayesian Statistics",
    "text": "Key Points: Bayesian Statistics\n\nProbability as degree of belief\nBayes’ Rule as the fundamental theorem of conditional probability\nBayesian updating as an information filter\nPrior selection important: lots to consider!\nCredible Intervals: Bayesian representations of uncertainty"
  },
  {
    "objectID": "slides/lecture04-1.html#key-points-extremes",
    "href": "slides/lecture04-1.html#key-points-extremes",
    "title": "More Bayes and Extreme Value Modeling",
    "section": "Key Points: Extremes",
    "text": "Key Points: Extremes\n\nEasy to under-estimate extreme events due to lack of data;\nTwo different approaches to understanding extremes\n\nBlock Maxima\nPeaks over Thresholds\n\nThese require different statistical approaches (more next week…)"
  },
  {
    "objectID": "slides/lecture04-1.html#next-classes",
    "href": "slides/lecture04-1.html#next-classes",
    "title": "More Bayes and Extreme Value Modeling",
    "section": "Next Classes",
    "text": "Next Classes\nWednesday: No class\nMonday: Extreme Value Theory and Models"
  },
  {
    "objectID": "slides/lecture04-1.html#assessments",
    "href": "slides/lecture04-1.html#assessments",
    "title": "More Bayes and Extreme Value Modeling",
    "section": "Assessments",
    "text": "Assessments\nExercise 4: Assigned, due Friday\nReading: Doss-Gollin et al. (2021)"
  },
  {
    "objectID": "slides/lecture04-1.html#references-1",
    "href": "slides/lecture04-1.html#references-1",
    "title": "More Bayes and Extreme Value Modeling",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n\n\nDoss-Gollin, J., & Keller, K. (2023). A subjective Bayesian framework for synthesizing deep uncertainties in climate risk management. Earths Future, 11, e2022EF003044. https://doi.org/10.1029/2022ef003044\n\n\nDoss-Gollin, J., Farnham, D. J., Lall, U., & Modi, V. (2021). How unprecedented was the February 2021 Texas cold snap? Environ. Res. Lett., 16, 064056. https://doi.org/gnswvt"
  },
  {
    "objectID": "slides/lecture02-2.html#probability-models",
    "href": "slides/lecture02-2.html#probability-models",
    "title": "Bayesian Statistics and Probability Models",
    "section": "Probability Models",
    "text": "Probability Models\nGoal: Write down a probability model for the data-generating process for \\(\\mathbf{y}\\).\n\nDirect statistical model, \\[\\mathbf{y} \\sim \\mathcal{D}(\\theta).\\]\nModel for the residuals of a numerical model, \\[\\mathbf{r} = \\mathbf{y} - F(\\mathbf{x}) \\sim \\mathcal{D}(\\theta).\\]"
  },
  {
    "objectID": "slides/lecture02-2.html#model-fitting-as-maximum-likelihood-estimation",
    "href": "slides/lecture02-2.html#model-fitting-as-maximum-likelihood-estimation",
    "title": "Bayesian Statistics and Probability Models",
    "section": "Model Fitting as Maximum Likelihood Estimation",
    "text": "Model Fitting as Maximum Likelihood Estimation\nWe can interpret fitting a model (reducing error according to some loss or error metric) as maximizing the probability of observing our data from this data generating process."
  },
  {
    "objectID": "slides/lecture02-2.html#the-energy-balance-model-revisited",
    "href": "slides/lecture02-2.html#the-energy-balance-model-revisited",
    "title": "Bayesian Statistics and Probability Models",
    "section": "The Energy Balance Model Revisited",
    "text": "The Energy Balance Model Revisited\nLast class, we introduced the Energy Balance Model (EBM):\n\\[T_{i+1} = T_i + \\frac{F_i - \\lambda T_i}{cd} \\Delta t\\]\nLet’s write this as \\(\\mathbf{T} = \\Lambda(\\mathbf{F})\\)."
  },
  {
    "objectID": "slides/lecture02-2.html#ebm-probability-model",
    "href": "slides/lecture02-2.html#ebm-probability-model",
    "title": "Bayesian Statistics and Probability Models",
    "section": "EBM Probability Model",
    "text": "EBM Probability Model\nAssume independent and identically distributed (i.i.d.) normal residuals:\n\\[\\mathbf{y_i} \\sim \\mathcal{N}(\\Lambda(\\mathbf{F_i}), \\sigma).\\]"
  },
  {
    "objectID": "slides/lecture02-2.html#i.i.d.-log-likelihood",
    "href": "slides/lecture02-2.html#i.i.d.-log-likelihood",
    "title": "Bayesian Statistics and Probability Models",
    "section": "i.i.d. Log-Likelihood",
    "text": "i.i.d. Log-Likelihood\n\\[\n\\log \\mathcal{L}(\\theta | \\mathbf{y}, F) = \\sum_{i=1}^n \\left[\\log \\frac{1}{\\sqrt{2\\pi}} - \\frac{1}{2\\sigma^2}(y_i - F(x_i))  ^2 \\right]\n\\]\nIf we ignore all of the constants, this is proportional to the negative mean-squared error."
  },
  {
    "objectID": "slides/lecture02-2.html#maximum-likelihood-estimate",
    "href": "slides/lecture02-2.html#maximum-likelihood-estimate",
    "title": "Bayesian Statistics and Probability Models",
    "section": "Maximum Likelihood Estimate",
    "text": "Maximum Likelihood Estimate\n\n# p are the model parameters, σ the standard deviation of the normal errors, y is the data, m the model function\nfunction log_likelihood(p, σ, y, m)\n    y_pred = m(p)\n    ll = sum(logpdf.(Normal.(y_pred, σ), y))\nend\n\nebm_wrap(params) = ebm(forcing_non_aerosol_85[idx], forcing_aerosol_85[idx], p = params)\n\n# maximize log-likelihood within some range\n# important to make everything a Float instead of an Int \nlower = [1.0, 50.0, 0.0, 0.0]\nupper = [4.0, 150.0, 2.0, 10.0]\np0 = [2.0, 100.0, 1.0, 1.0]\nresult = Optim.optimize(params -&gt; -log_likelihood(params[1:end-1], params[end], temp_obs, ebm_wrap), lower, upper, p0)\nθ = result.minimizer"
  },
  {
    "objectID": "slides/lecture02-2.html#maximum-likelihood-estimate-output",
    "href": "slides/lecture02-2.html#maximum-likelihood-estimate-output",
    "title": "Bayesian Statistics and Probability Models",
    "section": "Maximum Likelihood Estimate",
    "text": "Maximum Likelihood Estimate\n\n4-element Vector{Float64}:\n  1.9437468626999923\n 86.39077196261964\n  0.789333067296\n  0.12104289634093851"
  },
  {
    "objectID": "slides/lecture02-2.html#generating-simulations",
    "href": "slides/lecture02-2.html#generating-simulations",
    "title": "Bayesian Statistics and Probability Models",
    "section": "Generating Simulations",
    "text": "Generating Simulations\n\n# set number of sampled simulations\nn_samples = 1000\nresiduals = rand(Normal(0, θ[end]), (n_samples, length(temp_obs)))\nmodel_out = ebm_wrap(θ[1:end-1])\n# this uses broadcasting to \"sweep\" the model simulation across the sampled residual matrix\nmodel_sim = residuals .+ model_out' # need to transpose the model output vector due to how Julia treats vector dimensions"
  },
  {
    "objectID": "slides/lecture02-2.html#best-fit",
    "href": "slides/lecture02-2.html#best-fit",
    "title": "Bayesian Statistics and Probability Models",
    "section": "“Best Fit”",
    "text": "“Best Fit”\n\n\nCode\nplot(hind_years, model_out, color=:red, linewidth=3, label=\"Model Simulation\", guidefontsize=18, tickfontsize=16, legendfontsize=16, xlabel=\"Year\", ylabel=\"Temperature anomaly (°C)\", bottom_margin=5mm, left_margin=5mm)\nylims!(-0.5, 1.2)\nscatter!(hind_years, temp_obs, color=:black, label=\"Data\")\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Best fit for the EBM with normal residuals."
  },
  {
    "objectID": "slides/lecture02-2.html#adding-in-uncertain-residuals",
    "href": "slides/lecture02-2.html#adding-in-uncertain-residuals",
    "title": "Bayesian Statistics and Probability Models",
    "section": "Adding In Uncertain Residuals…",
    "text": "Adding In Uncertain Residuals…\n\n\nCode\nplot(hind_years, model_out, color=:red, linewidth=3, label=\"Model Simulation\", xlabel=\"Year\", ylabel=\"Temperature anomaly (°C)\", guidefontsize=18, tickfontsize=16, legendfontsize=16, bottom_margin=5mm, left_margin=5mm)\nplot!(hind_years, model_sim[1, :], color=:grey, linewidth=1, label=\"Model Simulation With Noise\", alpha=0.5)\nplot!(hind_years, model_sim[2:10, :]', color=:grey, linewidth=1, label=false, alpha=0.5)\nscatter!(hind_years, temp_obs, color=:black, label=\"Data\")\nylims!(-0.5, 1.2)\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Comparison of best fit with uncertain realization for the EBM with normal residuals."
  },
  {
    "objectID": "slides/lecture02-2.html#visualizing-the-uncertainty-spread",
    "href": "slides/lecture02-2.html#visualizing-the-uncertainty-spread",
    "title": "Bayesian Statistics and Probability Models",
    "section": "Visualizing the Uncertainty Spread",
    "text": "Visualizing the Uncertainty Spread\n\n\nCode\nq_90 = mapslices(col -&gt; quantile(col, [0.05, 0.95]), model_sim,; dims=1) # compute 90% prediction interval\n\nplot(hind_years, model_out, color=:red, linewidth=3, label=\"Model Simulation\", ribbon=(model_out .- q_90[1, :], q_90[2, :] .- model_out), fillalpha=0.3, xlabel=\"Year\", ylabel=\"Temperature anomaly (°C)\", guidefontsize=18, tickfontsize=16, legendfontsize=16, bottom_margin=5mm, left_margin=5mm)\nscatter!(hind_years, temp_obs, color=:black, label=\"Data\")\nylims!(-0.5, 1.2)\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Comparison of best fit with uncertain realization for the EBM with normal residuals."
  },
  {
    "objectID": "slides/lecture02-2.html#checking-for-calibration",
    "href": "slides/lecture02-2.html#checking-for-calibration",
    "title": "Bayesian Statistics and Probability Models",
    "section": "Checking for Calibration",
    "text": "Checking for Calibration\nFor an \\(\\alpha\\)% projection interval \\(\\mathcal{I}_\\alpha\\), we would expect ~\\(\\alpha\\)% of the data to be contained in this interval.\nThis rate is quantified by the surprise index: \\(1 - \\frac{1}{n} \\sum_{i=1}^n \\mathbb{I}_{\\mathcal{I}_\\alpha}(y_i).\\)"
  },
  {
    "objectID": "slides/lecture02-2.html#surprise-index",
    "href": "slides/lecture02-2.html#surprise-index",
    "title": "Bayesian Statistics and Probability Models",
    "section": "Surprise Index",
    "text": "Surprise Index\n\n\nCode\nsurprises = 0 # initialize surprise counter\n# go through the data and check which points are outside of the 90% interval\nfor i = 1:length(temp_obs)\n    ## The || operator is an OR, so returns true if either of the terms are true\n    if (temp_obs[i] &lt; q_90[1, i]) || (q_90[2, i] &lt; temp_obs[i])\n        surprises += 1\n    end\nend\nsurprises / length(temp_obs)\n\n\n0.11695906432748537\n\n\nWe used a 90% projection interval:\n\nHow does this compare?"
  },
  {
    "objectID": "slides/lecture02-2.html#projections-best-fit",
    "href": "slides/lecture02-2.html#projections-best-fit",
    "title": "Bayesian Statistics and Probability Models",
    "section": "Projections (Best Fit)",
    "text": "Projections (Best Fit)\n\n\nCode\nmodel_sim_85 = ebm(forcing_non_aerosol_85[sim_idx], forcing_aerosol_85[sim_idx], p=θ[1:end-1])\nmodel_sim_26 = ebm(forcing_non_aerosol_26[sim_idx], forcing_aerosol_26[sim_idx], p=θ[1:end-1])\n\n\nplot(sim_years, model_sim_26, color=:blue, linewidth=3, label=\"RCP 2.6 Projection\", fillalpha=0.3, xlabel=\"Year\", ylabel=\"Temperature anomaly (°C)\", guidefontsize=18, tickfontsize=16, legendfontsize=16, bottom_margin=5mm, left_margin=5mm)\nplot!(sim_years, model_sim_85, color=:red, linewidth=3, label=\"RCP 8.5 Projection\", fillalpha=0.3)\nscatter!(hind_years, temp_obs, color=:black, label=\"Data\")\nylims!(-0.5, 5)\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Best fit for different SSPs"
  },
  {
    "objectID": "slides/lecture02-2.html#projections-residual-uncertainty",
    "href": "slides/lecture02-2.html#projections-residual-uncertainty",
    "title": "Bayesian Statistics and Probability Models",
    "section": "Projections (Residual Uncertainty)",
    "text": "Projections (Residual Uncertainty)\n\n\nCode\nresids = rand(Normal(0, θ[end]), (n_samples, length(sim_years)))\nq90_26 = mapslices(col -&gt; quantile(col, [0.05, 0.95]), resids .+ model_sim_26'; dims=1) # compute 90% prediction interval\nq90_85 = mapslices(col -&gt; quantile(col, [0.05, 0.95]), resids .+ model_sim_85'; dims=1) # compute 90% prediction interval\n\nplot(sim_years, model_sim_26, color=:blue, linewidth=3, label=\"RCP 2.6 Projection\", fillalpha=0.3, ribbon=(model_sim_26 .- q90_26[1, :], q90_26[2, :] .- model_sim_26), xlabel=\"Year\", ylabel=\"Temperature anomaly (°C)\", guidefontsize=18, tickfontsize=16, legendfontsize=16, bottom_margin=5mm, left_margin=5mm)\nplot!(sim_years, model_sim_85, color=:red, linewidth=3, ribbon=(model_sim_85 .- q90_85[1, :], q90_85[2, :] .- model_sim_85), label=\"RCP 8.5 Projection\", fillalpha=0.3)\nscatter!(hind_years, temp_obs, color=:black, label=\"Data\")\nylims!(-0.5, 5)\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Best fit for different SSPs"
  },
  {
    "objectID": "slides/lecture02-2.html#checking-the-model-residuals",
    "href": "slides/lecture02-2.html#checking-the-model-residuals",
    "title": "Bayesian Statistics and Probability Models",
    "section": "Checking The Model Residuals",
    "text": "Checking The Model Residuals\nOne mantra in this class: check your residuals!\nWe assumed our models were independently and identically distributed according to a normal distribution.\nHow can we check these assumptions?"
  },
  {
    "objectID": "slides/lecture02-2.html#diagnostics-normality",
    "href": "slides/lecture02-2.html#diagnostics-normality",
    "title": "Bayesian Statistics and Probability Models",
    "section": "Diagnostics: Normality",
    "text": "Diagnostics: Normality\n\nCode\nresiduals = model_out .- temp_obs # calculate residuals\np1 = histogram(residuals, tickfontsize=16, guidefontsize=18, legend=false, xlabel=\"Residual (°C)\", ylabel=\"Count\") # plot histogram to check distributional assumption\np2 = qqplot(Normal, residuals, tickfontsize=16, guidefontsize=18, xlabel=\"Normal Theoretical Quantile\", ylabel=\"Sample Residual Quantile\")\n\nplot!(p1, size=(500, 500))\nplot!(p2, size=(500, 500))\ndisplay(p1)\ndisplay(p2)\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Distribution\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Q-Q Plot\n\n\n\n\n\n\n\nFigure 6: Checking residual assumptions of normality"
  },
  {
    "objectID": "slides/lecture02-2.html#diagnostics-independence",
    "href": "slides/lecture02-2.html#diagnostics-independence",
    "title": "Bayesian Statistics and Probability Models",
    "section": "Diagnostics: Independence",
    "text": "Diagnostics: Independence\n\nCode\np1 = plot(pacf(residuals, 1:5), marker=:circle, line=:stem, linewidth=3, markersize=8, tickfontsize=16, guidefontsize=18, legend=false, ylabel=\"Partial Autocorrelation\", xlabel=\"Time Lag\")\np2 = scatter(residuals[1:end-1], residuals[2:end], legend=false, xlabel=L\"Residual $t_i$ (°C)\", ylabel=L\"Residual $t_{i+1}$ (°C)\", guidefontsize=18, tickfontsize=16)  \ndat = DataFrame(X=residuals[1:end-1], Y=residuals[2:end])\nfit = lm(@formula(Y~X), dat)\npred = predict(fit,dat)\nplot!(p2, dat.X, pred, linewidth=2)\n\nplot!(p1, size=(500, 500))\nplot!(p2, size=(500, 500))\ndisplay(p1)\ndisplay(p2)\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Partial autocorrelation\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Scatterplot of lag-1 residuals\n\n\n\n\n\n\n\nFigure 7: Checking residual assumptions for independence and autocorrelation"
  },
  {
    "objectID": "slides/lecture02-2.html#diagnostics-constant-variance",
    "href": "slides/lecture02-2.html#diagnostics-constant-variance",
    "title": "Bayesian Statistics and Probability Models",
    "section": "Diagnostics: Constant Variance",
    "text": "Diagnostics: Constant Variance\n\nCode\np1 = scatter(residuals,  tickfontsize=16, guidefontsize=18, legend=false, ylabel=\"Residual (°C)\", xlabel=\"Time\")\ndat = DataFrame(Y=residuals, X=1:length(residuals))\nfit = lm(@formula(Y~X), dat)\npred = predict(fit,dat)\nplot!(p1, dat.X, pred, linewidth=2)\np2 = scatter(model_out, residuals, legend=false, xlabel=\"Modeled Temperature (°C)\", ylabel=\"Residual (°C)\", guidefontsize=18, tickfontsize=16)  \ndat = DataFrame(Y=residuals, X=model_out)\nfit = lm(@formula(Y~X), dat)\npred = predict(fit,dat)\nplot!(p2, dat.X, pred, linewidth=2)\n\nplot!(p1, size=(500, 500))\nplot!(p2, size=(500, 500))\ndisplay(p1)\ndisplay(p2)\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Time dependence\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Temperature depednence\n\n\n\n\n\n\n\nFigure 8: Checking residual assumptions for constant variance"
  },
  {
    "objectID": "slides/lecture02-2.html#a-model-for-residual-autocorrelation",
    "href": "slides/lecture02-2.html#a-model-for-residual-autocorrelation",
    "title": "Bayesian Statistics and Probability Models",
    "section": "A Model for Residual Autocorrelation",
    "text": "A Model for Residual Autocorrelation\nAn autoregressive with lag 1 model (AR(1)):\n\\[\n\\begin{gather*}\nr_{i+1} = \\rho r_i + \\varepsilon_i \\\\\n\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma)\n\\end{gather*}\n\\]"
  },
  {
    "objectID": "slides/lecture02-2.html#rearranging-for-likelihood",
    "href": "slides/lecture02-2.html#rearranging-for-likelihood",
    "title": "Bayesian Statistics and Probability Models",
    "section": "Rearranging for Likelihood",
    "text": "Rearranging for Likelihood\n\\[\\begin{gather*}\nr_{i+1} \\sim \\mathcal{N}(\\rho r_i, \\sigma) \\\\\nr_1 \\sim \\mathcal{N}\\left(0, \\frac{\\sigma}{\\sqrt{1-\\rho^2}}\\right)\n\\end{gather*}\n\\]"
  },
  {
    "objectID": "slides/lecture02-2.html#fitting-ar1-model",
    "href": "slides/lecture02-2.html#fitting-ar1-model",
    "title": "Bayesian Statistics and Probability Models",
    "section": "Fitting AR(1) Model",
    "text": "Fitting AR(1) Model\n\n# p are the model parameters, σ the standard deviation of the AR(1) errors, ρ is the autocorrelation coefficient, y is the data, m the model function\nfunction ar_log_likelihood(p, σ, ρ, y, m)\n    y_pred = m(p)\n    ll = 0 # initialize log-likelihood counter\n    residuals = y_pred .- y\n    ll += logpdf(Normal(0, σ/sqrt(1-ρ^2)), residuals[1])\n    for i = 1:length(residuals)-1\n        residuals_whitened = residuals[i+1] - ρ * residuals[i]\n        ll += logpdf(Normal(0, σ), residuals_whitened)\n    end\n    return ll\nend\n\nebm_wrap(params) = ebm(forcing_non_aerosol_85[idx], forcing_aerosol_85[idx], p = params)\n\n# maximize log-likelihood within some range\n# important to make everything a Float instead of an Int \nlower = [1.0, 50.0, 0.0, 0.0, -1.0]\nupper = [4.0, 150.0, 2.0, 10.0, 1.0]\np0 = [2.0, 100.0, 1.0, 1.0, 0.0]\nresult = Optim.optimize(params -&gt; -ar_log_likelihood(params[1:end-2], params[end-1], params[end], temp_obs, ebm_wrap), lower, upper, p0)\nθ_ar1 = result.minimizer"
  },
  {
    "objectID": "slides/lecture02-2.html#comparison-of-iid-and-ar1-fits",
    "href": "slides/lecture02-2.html#comparison-of-iid-and-ar1-fits",
    "title": "Bayesian Statistics and Probability Models",
    "section": "Comparison of IID and AR(1) Fits",
    "text": "Comparison of IID and AR(1) Fits\n\n\nNormal IID:\n\n\n4-element Vector{Float64}:\n  1.9437468626999923\n 86.39077196261964\n  0.789333067296\n  0.12104289634093851\n\n\n\nAR(1):\n\n\n5-element Vector{Float64}:\n   2.006808911185199\n 109.38026352760137\n   0.791544898198773\n   0.10196962494888041\n   0.542159499126698"
  },
  {
    "objectID": "slides/lecture02-2.html#comparison-of-model-simulation",
    "href": "slides/lecture02-2.html#comparison-of-model-simulation",
    "title": "Bayesian Statistics and Probability Models",
    "section": "Comparison of Model Simulation",
    "text": "Comparison of Model Simulation\n\n\nCode\n# iid simulations\nresiduals_iid = rand(Normal(0, θ[end]), (n_samples, length(hind_years)))\nmodel_iid = ebm_wrap(θ[1:end-1])\nmodel_sim_iid = residuals_iid .+ model_iid'\n\n# ar1 simulations\nresiduals_ar1 = zeros(n_samples, length(hind_years))\nresiduals_ar1[:, 1] = rand(Normal(0, θ_ar1[end-1] / sqrt(1-θ_ar1[end]^2)), n_samples)\nfor i = 2:size(residuals_ar1)[2]\n    residuals_ar1[:, i] .= rand.(Normal.(θ_ar1[end] * residuals_ar1[:, i-1], θ_ar1[end-1]))\nend\nmodel_ar1 = ebm_wrap(θ_ar1[1:end-2])\nmodel_sim_ar1 = residuals_ar1 .+ model_ar1'\n\nq90_iid = mapslices(col -&gt; quantile(col, [0.05, 0.95]), model_sim_iid; dims=1) # compute 90% prediction interval\nq90_ar1 = mapslices(col -&gt; quantile(col, [0.05, 0.95]), model_sim_ar1; dims=1) # compute 90% prediction interval\n\nplot(hind_years, model_iid, color=:red, linewidth=3, label=\"IID Model Simulation\", ribbon=(model_iid .- q90_iid[1, :], q90_iid[2, :] .- model_iid), fillalpha=0.3, xlabel=\"Year\", ylabel=\"Temperature anomaly (°C)\", guidefontsize=18, tickfontsize=16, legendfontsize=16, bottom_margin=5mm, left_margin=5mm)\nplot!(hind_years, model_ar1, color=:blue, linewidth=3, label=\"AR(1) Model Simulation\", ribbon=(model_ar1 .- q90_ar1[1, :], q90_ar1[2, :] .- model_out), fillalpha=0.3)\nscatter!(hind_years, temp_obs, color=:black, label=\"Data\")\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 9: Comparison of best fit with uncertain realization for the EBM with normal residuals."
  },
  {
    "objectID": "slides/lecture02-2.html#ar1-surprise-index",
    "href": "slides/lecture02-2.html#ar1-surprise-index",
    "title": "Bayesian Statistics and Probability Models",
    "section": "AR(1) Surprise Index",
    "text": "AR(1) Surprise Index\n\n\nCode\nsurprises = 0 # initialize surprise counter\n# go through the data and check which points are outside of the 90% interval\nfor i = 1:length(temp_obs)\n    ## The || operator is an OR, so returns true if either of the terms are true\n    if (temp_obs[i] &lt; q90_ar1[1, i]) || (q90_ar1[2, i] &lt; temp_obs[i])\n        surprises += 1\n    end\nend\nsurprises / length(temp_obs)\n\n\n0.14035087719298245"
  },
  {
    "objectID": "slides/lecture02-2.html#impacts-on-projections-rcp-8.5",
    "href": "slides/lecture02-2.html#impacts-on-projections-rcp-8.5",
    "title": "Bayesian Statistics and Probability Models",
    "section": "Impacts on Projections (RCP 8.5)",
    "text": "Impacts on Projections (RCP 8.5)\n\n\nCode\n# iid simulations\nresiduals_iid = rand(Normal(0, θ[end]), (n_samples, length(sim_years)))\nmodel_iid = ebm(forcing_non_aerosol_85[sim_idx], forcing_aerosol_85[sim_idx], p=θ[1:end-1])\nmodel_sim_iid = residuals_iid .+ model_iid'\n\n# ar1 simulations\nresiduals_ar1 = zeros(n_samples, length(sim_years))\nresiduals_ar1[:, 1] = rand(Normal(0, θ_ar1[end-1] / sqrt(1-θ_ar1[end]^2)), n_samples)\nfor i = 2:size(residuals_ar1)[2]\n    residuals_ar1[:, i] .= rand.(Normal.(θ_ar1[end] * residuals_ar1[:, i-1], θ_ar1[end-1]))\nend\nmodel_ar1 = ebm(forcing_non_aerosol_85[sim_idx], forcing_aerosol_85[sim_idx], p=θ_ar1[1:end-2])\nmodel_sim_ar1 = residuals_ar1 .+ model_ar1'\n\nq90_iid = mapslices(col -&gt; quantile(col, [0.05, 0.95]), model_sim_iid; dims=1) # compute 90% prediction interval\nq90_ar1 = mapslices(col -&gt; quantile(col, [0.05, 0.95]), model_sim_ar1; dims=1) # compute 90% prediction interval\n\nplot(sim_years, model_iid, color=:red, linewidth=3, label=\"IID Model Simulation\", ribbon=(model_iid .- q90_iid[1, :], q90_iid[2, :] .- model_iid), fillalpha=0.3, xlabel=\"Year\", ylabel=\"Temperature anomaly (°C)\", guidefontsize=18, tickfontsize=16, legendfontsize=16, bottom_margin=5mm, left_margin=5mm, right_margin=8mm)\nplot!(sim_years, model_ar1, color=:blue, linewidth=3, label=\"AR(1) Model Simulation\", ribbon=(model_ar1 .- q90_ar1[1, :], q90_ar1[2, :] .- model_ar1), fillalpha=0.3)\nxlims!(2000, 2100)\nylims!(0, 5)\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 10: Comparisons of different residual structures for RCP 8.5 projections."
  },
  {
    "objectID": "slides/lecture02-2.html#ok-but-what-about-parameter-uncertainty",
    "href": "slides/lecture02-2.html#ok-but-what-about-parameter-uncertainty",
    "title": "Bayesian Statistics and Probability Models",
    "section": "Ok, But What About Parameter Uncertainty?",
    "text": "Ok, But What About Parameter Uncertainty?\nThe frameworks we’ve been using so far: no parametric uncertainty and minimal prior information.\nThis can sometimes lead to odd outcomes, e.g. low climate sensitivities.\nIn a few weeks, we will look at approaches to incorporate parametric uncertainty and prior information."
  },
  {
    "objectID": "slides/lecture02-2.html#key-points-1",
    "href": "slides/lecture02-2.html#key-points-1",
    "title": "Bayesian Statistics and Probability Models",
    "section": "Key Points",
    "text": "Key Points\n\nProbability models for data let us generate alternate datasets/realizations.\nDifferent probability models may have more or less impact on simulated outcomes but may result in different parameter values."
  },
  {
    "objectID": "slides/lecture02-2.html#next-classes",
    "href": "slides/lecture02-2.html#next-classes",
    "title": "Bayesian Statistics and Probability Models",
    "section": "Next Class(es)",
    "text": "Next Class(es)\nNext Week: Exploratory data analysis"
  },
  {
    "objectID": "slides/lecture02-2.html#assessments",
    "href": "slides/lecture02-2.html#assessments",
    "title": "Bayesian Statistics and Probability Models",
    "section": "Assessments",
    "text": "Assessments\nFriday: HW1 and Exercise 1 due by 9pm."
  },
  {
    "objectID": "slides/lecture05-1.html#bayesian-probability",
    "href": "slides/lecture05-1.html#bayesian-probability",
    "title": "Extreme Value Theory & Modeling",
    "section": "Bayesian Probability",
    "text": "Bayesian Probability\n\nExplicitly based on conditional probability: \\(p(\\theta | \\mathbf{y})\\)\nModel structures, parameters, and unobserved data all random"
  },
  {
    "objectID": "slides/lecture05-1.html#bayesian-model-components",
    "href": "slides/lecture05-1.html#bayesian-model-components",
    "title": "Extreme Value Theory & Modeling",
    "section": "Bayesian Model Components",
    "text": "Bayesian Model Components\nA fully specified Bayesian model includes:\n\nProbability model for the data given the parameters (the likelihood), \\(p(y | \\theta)\\)t\nPrior distributions over the parameters, \\(p(\\theta)\\)"
  },
  {
    "objectID": "slides/lecture05-1.html#prior-selection",
    "href": "slides/lecture05-1.html#prior-selection",
    "title": "Extreme Value Theory & Modeling",
    "section": "Prior Selection",
    "text": "Prior Selection\n\nMay need to play with different priors\nPrior predictive checks to help refine"
  },
  {
    "objectID": "slides/lecture05-1.html#two-ways-to-frame-extreme-values",
    "href": "slides/lecture05-1.html#two-ways-to-frame-extreme-values",
    "title": "Extreme Value Theory & Modeling",
    "section": "Two Ways To Frame “Extreme” Values",
    "text": "Two Ways To Frame “Extreme” Values\n\n“Block” extremes, e.g. annual maxima (block maxima)?\nValues which exceed a certain threshold (peaks over threshold)?"
  },
  {
    "objectID": "slides/lecture05-1.html#example-tide-gauge-data",
    "href": "slides/lecture05-1.html#example-tide-gauge-data",
    "title": "Extreme Value Theory & Modeling",
    "section": "Example: Tide Gauge Data",
    "text": "Example: Tide Gauge Data\n\n\nCode\nfunction load_data(fname)\n    date_format = \"yyyy-mm-dd HH:MM\"\n    # this uses the DataFramesMeta package -- it's pretty cool\n    return @chain fname begin\n        CSV.File(; dateformat=date_format)\n        DataFrame\n        rename(\n            \"Time (GMT)\" =&gt; \"time\", \"Predicted (m)\" =&gt; \"harmonic\", \"Verified (m)\" =&gt; \"gauge\"\n        )\n        @transform :datetime = (Date.(:Date, \"yyyy/mm/dd\") + Time.(:time))\n        select(:datetime, :gauge, :harmonic)\n        @transform :weather = :gauge - :harmonic\n        @transform :month = (month.(:datetime))\n    end\nend\n\ndat = load_data(\"data/surge/norfolk-hourly-surge-2015.csv\")\n\np1 = plot(dat.datetime, dat.gauge; ylabel=\"Gauge Measurement (m)\", label=\"Observed\", legend=:topleft, tickfontsize=14, guidefontsize=16, legendfontsize=14, xlabel=\"Date/Time\", bottom_margin=5mm, left_margin=5mm, right_margin=5mm)\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: 2015 tide gauge data from the Norfolk, VA tide gauge."
  },
  {
    "objectID": "slides/lecture05-1.html#example-tide-gauge-data-1",
    "href": "slides/lecture05-1.html#example-tide-gauge-data-1",
    "title": "Extreme Value Theory & Modeling",
    "section": "Example: Tide Gauge Data",
    "text": "Example: Tide Gauge Data\n\n\nCode\nplot!(p1, dat.datetime, dat.harmonic, label=\"Predicted\", alpha=0.7)\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: 2015 tide gauge data with predicted harmonics from the Norfolk, VA tide gauge."
  },
  {
    "objectID": "slides/lecture05-1.html#example-detrended-data",
    "href": "slides/lecture05-1.html#example-detrended-data",
    "title": "Extreme Value Theory & Modeling",
    "section": "Example: Detrended Data",
    "text": "Example: Detrended Data\n\n\nCode\nplot(dat.datetime, dat.weather; ylabel=\"Gauge Weather Variability (m)\", label=\"Detrended Data\", linewidth=3, legend=:topleft, tickfontsize=14, guidefontsize=16, legendfontsize=14, xlabel=\"Date/Time\", bottom_margin=5mm, left_margin=5mm, right_margin=5mm)\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: 2015 detrended tide gauge data from the Norfolk, VA tide gauge."
  },
  {
    "objectID": "slides/lecture05-1.html#example-block-maxima",
    "href": "slides/lecture05-1.html#example-block-maxima",
    "title": "Extreme Value Theory & Modeling",
    "section": "Example: Block Maxima",
    "text": "Example: Block Maxima\n\n\nCode\np1 = plot(dat.datetime, dat.weather; ylabel=\"Gauge Weather Variability (m)\", label=\"Detrended Data\", linewidth=2, legend=:topleft, tickfontsize=14, guidefontsize=16, legendfontsize=14, xlabel=\"Date/Time\", bottom_margin=5mm, left_margin=5mm, right_margin=5mm)\nmax_dat = combine(dat -&gt; dat[argmax(dat.weather), :], groupby(transform(dat, :datetime =&gt; x-&gt;yearmonth.(x)), :datetime_function))\nscatter!(max_dat.datetime, max_dat.weather, label=\"Monthly Maxima\", markersize=5)\nmonth_start = collect(Date(2015, 01, 01):Dates.Month(1):Date(2015, 12, 01))\nvline!(DateTime.(month_start), color=:black, label=:false, linestyle=:dash)\n\np2 = histogram(\n    max_dat.weather,\n    normalize=:pdf,\n    orientation=:horizontal,\n    label=:false,\n    xlabel=\"PDF\",\n    ylabel=\"\",\n    yticks=[],\n    tickfontsize=16,\n    guidefontsize=18\n)\n\nl = @layout [a{0.7w} b{0.3w}]\nplot(p1, p2; layout=l, link=:y, ylims=(-0.4, 1.4), bottom_margin=5mm, left_margin=5mm)\nplot!(size=(1000, 450))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: 2015 detrended tide gauge data from the Norfolk, VA tide gauge."
  },
  {
    "objectID": "slides/lecture05-1.html#example-peaks-over-threshold",
    "href": "slides/lecture05-1.html#example-peaks-over-threshold",
    "title": "Extreme Value Theory & Modeling",
    "section": "Example: Peaks Over Threshold",
    "text": "Example: Peaks Over Threshold\n\n\nCode\nthresh = 0.5\np1 = plot(dat.datetime, dat.weather; linewidth=2, ylabel=\"Gauge Weather Variability (m)\", label=\"Observations\", legend=:top, tickfontsize=14, guidefontsize=16, legendfontsize=14, xlabel=\"Date/Time\")\nhline!([thresh], color=:red, linestyle=:dash, label=\"Threshold\")\nscatter!(dat.datetime[dat.weather .&gt; thresh], dat.weather[dat.weather .&gt; thresh], markershape=:x, color=:black, markersize=3, label=\"Exceedances\")\n\np2 = histogram(\n    dat.weather[dat.weather .&gt; thresh],\n    normalize=:pdf,\n    orientation=:horizontal,\n    label=:false,\n    xlabel=\"PDF\",\n    ylabel=\"\",\n    yticks=[],\n    tickfontsize=16,\n    guidefontsize=18\n)\n\nl = @layout [a{0.7w} b{0.3w}]\nplot(p1, p2; layout=l, link=:y, ylims=(-0.4, 1.4), bottom_margin=5mm, left_margin=5mm)\nplot!(size=(1000, 450))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: 2015 detrended tide gauge data from the Norfolk, VA tide gauge."
  },
  {
    "objectID": "slides/lecture05-1.html#block-maxima-1",
    "href": "slides/lecture05-1.html#block-maxima-1",
    "title": "Extreme Value Theory & Modeling",
    "section": "Block Maxima",
    "text": "Block Maxima\nGiven independent and identically-distributed random variables \\(X_1, X_2, \\ldots, X_{mk}\\), what is the distribution of maxima of “blocks” of size \\(m\\):\n\\[\\tilde{X}_i = \\max_{(i-1)m &lt; j \\leq im} X_j,\\]\nfor \\(i = 1, 2, \\ldots, k\\)?"
  },
  {
    "objectID": "slides/lecture05-1.html#analogy-central-limit-theorem",
    "href": "slides/lecture05-1.html#analogy-central-limit-theorem",
    "title": "Extreme Value Theory & Modeling",
    "section": "Analogy: Central Limit Theorem",
    "text": "Analogy: Central Limit Theorem\nRecall that the Central Limit Theorem tells us:\nIf we have independent and identically-distributed variables \\[X_1, X_2, \\ldots, X_n\\] from some population with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), the sample mean \\(\\bar{X}\\) has the approximate distribution\n\\[\\bar{X} \\sim \\text{Normal}(\\mu, \\sigma/\\sqrt{n}).\\]"
  },
  {
    "objectID": "slides/lecture05-1.html#extreme-value-theorem",
    "href": "slides/lecture05-1.html#extreme-value-theorem",
    "title": "Extreme Value Theory & Modeling",
    "section": "Extreme Value Theorem",
    "text": "Extreme Value Theorem\nThe Extreme Value Theorem is the equivalent for block maxima.\nIf the limiting distribution exists, it can only by given as a Generalized Extreme Value (GEV) distribution:\n\\[H(y) = \\exp\\left\\{-\\left[1 + \\xi\\left(\\frac{y-\\mu}{\\sigma}\\right)\\right]^{-1/\\xi}\\right\\},\\] defined for \\(y\\) such that \\(1 + \\xi(y-\\mu)/\\sigma &gt; 0\\)."
  },
  {
    "objectID": "slides/lecture05-1.html#gev-distributions",
    "href": "slides/lecture05-1.html#gev-distributions",
    "title": "Extreme Value Theory & Modeling",
    "section": "GEV Distributions",
    "text": "GEV Distributions\nGEV distributions have three parameters:\n\nlocation \\(\\mu\\);\nscale \\(\\sigma &gt; 0\\);\nshape \\(\\xi\\)."
  },
  {
    "objectID": "slides/lecture05-1.html#gev-types",
    "href": "slides/lecture05-1.html#gev-types",
    "title": "Extreme Value Theory & Modeling",
    "section": "GEV “Types”",
    "text": "GEV “Types”\n\n\n\n\\(\\xi &gt; 0\\): Frèchet (heavy-tailed)\n\\(\\xi = 0\\): Gumbel (light-tailed)\n\\(\\xi &lt; 0\\): Weibull (bounded)\n\n\n\n\nCode\np1 = plot(-2:0.1:6, GeneralizedExtremeValue(0, 1, 0.5), linewidth=3, color=:red, label=L\"$\\xi = 1/2$\", guidefontsize=18, legendfontsize=16, tickfontsize=16)\nplot!(-4:0.1:6, GeneralizedExtremeValue(0, 1, 0), linewidth=3, color=:green, label=L\"$\\xi = 0$\")\nplot!(-4:0.1:2, GeneralizedExtremeValue(0, 1, -0.5), linewidth=3, color=:blue, label=L\"$\\xi = -1/2$\")\nscatter!((-2, 0), color=:red, label=:false)\nscatter!((2, 0), color=:blue, label=:false)\nylabel!(\"Density\")\nxlabel!(L\"$x$\")\nplot!(size=(600, 450))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Shape of the GEV distribution with different choices of \\(\\xi\\)."
  },
  {
    "objectID": "slides/lecture05-1.html#gev-types-1",
    "href": "slides/lecture05-1.html#gev-types-1",
    "title": "Extreme Value Theory & Modeling",
    "section": "GEV Types",
    "text": "GEV Types\n\n\\(\\xi &lt; 0\\): extremes are bounded (the Weibull distribution comes up in the context of temperature and wind speed extremes).\n\\(\\xi &gt; 0\\): tails are heavy, and there is no expectation if \\(\\xi &gt; 1\\). Common for streamflow, storm surge, precipitation.\nThe Gumbel distribution (\\(\\xi = 0\\)) is common for extremes from normal distributions, doesn’t occur often in real-world data."
  },
  {
    "objectID": "slides/lecture05-1.html#san-francisco-tide-gauge-data",
    "href": "slides/lecture05-1.html#san-francisco-tide-gauge-data",
    "title": "Extreme Value Theory & Modeling",
    "section": "San Francisco Tide Gauge Data",
    "text": "San Francisco Tide Gauge Data\n\n\nCode\n# load SF tide gauge data\n# read in data and get annual maxima\nfunction load_data(fname)\n    date_format = DateFormat(\"yyyy-mm-dd HH:MM:SS\")\n    # This uses the DataFramesMeta.jl package, which makes it easy to string together commands to load and process data\n    df = @chain fname begin\n        CSV.read(DataFrame; header=false)\n        rename(\"Column1\" =&gt; \"year\", \"Column2\" =&gt; \"month\", \"Column3\" =&gt; \"day\", \"Column4\" =&gt; \"hour\", \"Column5\" =&gt; \"gauge\")\n        # need to reformat the decimal date in the data file\n        @transform :datetime = DateTime.(:year, :month, :day, :hour)\n        # replace -99999 with missing\n        @transform :gauge = ifelse.(abs.(:gauge) .&gt;= 9999, missing, :gauge)\n        select(:datetime, :gauge)\n    end\n    return df\nend\n\ndat = load_data(\"data/surge/h551.csv\")\n\n# detrend the data to remove the effects of sea-level rise and seasonal dynamics\nma_length = 366\nma_offset = Int(floor(ma_length/2))\nmoving_average(series,n) = [mean(@view series[i-n:i+n]) for i in n+1:length(series)-n]\ndat_ma = DataFrame(datetime=dat.datetime[ma_offset+1:end-ma_offset], residual=dat.gauge[ma_offset+1:end-ma_offset] .- moving_average(dat.gauge, ma_offset))\n\n# group data by year and compute the annual maxima\ndat_ma = dropmissing(dat_ma) # drop missing data\ndat_annmax = combine(dat_ma -&gt; dat_ma[argmax(dat_ma.residual), :], groupby(transform(dat_ma, :datetime =&gt; x-&gt;year.(x)), :datetime_function))\ndelete!(dat_annmax, nrow(dat_annmax)) # delete 2023; haven't seen much of that year yet\nrename!(dat_annmax, :datetime_function =&gt; :Year)\nselect!(dat_annmax, [:Year, :residual])\ndat_annmax.residual = dat_annmax.residual / 1000 # convert to m\n\n# make plots\np1 = plot(\n    dat_annmax.Year,\n    dat_annmax.residual;\n    xlabel=\"Year\",\n    ylabel=\"Annual Max Tide Level (m)\",\n    label=false,\n    marker=:circle,\n    markersize=5,\n    tickfontsize=16,\n    guidefontsize=18\n)\np2 = histogram(\n    dat_annmax.residual,\n    normalize=:pdf,\n    orientation=:horizontal,\n    label=:false,\n    xlabel=\"PDF\",\n    ylabel=\"\",\n    yticks=[],\n    tickfontsize=16,\n    guidefontsize=18\n)\n\nl = @layout [a{0.7w} b{0.3w}]\nplot(p1, p2; layout=l, link=:y, ylims=(1, 1.7), bottom_margin=5mm, left_margin=5mm)\nplot!(size=(1000, 450))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: Annual maxima surge data from the San Francisco, CA tide gauge."
  },
  {
    "objectID": "slides/lecture05-1.html#block-maxima-fit",
    "href": "slides/lecture05-1.html#block-maxima-fit",
    "title": "Extreme Value Theory & Modeling",
    "section": "Block Maxima Fit",
    "text": "Block Maxima Fit\n\n\nCode\n# find GEV fit\n# for most distributions we could use Distributions.fit(), but this isn't implemented in Distributions.jl for GEV\ninit_θ = [1.0, 1.0, 1.0]\ngev_lik(θ) = -sum(logpdf(GeneralizedExtremeValue(θ[1], θ[2], θ[3]), dat_annmax.residual))\nθ_mle = Optim.optimize(gev_lik, init_θ).minimizer\n\np = histogram(\n    dat_annmax.residual,\n    normalize=:pdf,\n    label=\"Data\",\n    xlabel=\"Annual Maximum (m)\",\n    ylabel=\"PDF\",\n    yticks=[],\n    tickfontsize=16,\n    guidefontsize=18,\n    legendfontsize=16,\n    left_margin=10mm, \n    right_margin=10mm,\n    bottom_margin=5mm\n)\nplot!(GeneralizedExtremeValue(θ_mle[1], θ_mle[2], θ_mle[3]), linewidth=3, label=\"GEV Fit\")\nplot!(fit(LogNormal, dat_annmax.residual), linewidth=3, label=\"LogNormal Fit\", color=:black)\nxlims!((1, 1.75))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8: GEV fit to annual maxima of San Francisco Tide Gauge Data"
  },
  {
    "objectID": "slides/lecture05-1.html#gev-q-q-plot",
    "href": "slides/lecture05-1.html#gev-q-q-plot",
    "title": "Extreme Value Theory & Modeling",
    "section": "GEV Q-Q Plot",
    "text": "GEV Q-Q Plot\n\nCode\np1 = qqplot(GeneralizedExtremeValue(θ_mle[1], θ_mle[2], θ_mle[3]), dat_annmax.residual, \n    linewidth=3, markersize=5,\n    xlabel=\"Theoretical Quantile\",\n    ylabel=\"Empirical Quantile\",\n    tickfontsize=16,\n    guidefontsize=18,\n    legendfontsize=16\n)\nplot!(p1, size=(600, 450))\n\nreturn_periods = 2:500\n# get GEV return levels\nreturn_levels = quantile.(GeneralizedExtremeValue(θ_mle[1], θ_mle[2], θ_mle[3]), 1 .- (1 ./ return_periods))\n# fit lognormal to get return levels for comparison\nlognormal_fit = fit(LogNormal, dat_annmax.residual)\nreturn_levels_lognormal = quantile.(lognormal_fit, 1 .- (1 ./ return_periods))\n\n# function to calculate exceedance probability and plot positions based on data quantile\nfunction exceedance_plot_pos(y)\n    N = length(y)\n    ys = sort(y; rev=false) # sorted values of y\n    nxp = xp = [r / (N + 1) for r in 1:N] # exceedance probability\n    xp = 1 .- nxp\n    return xp, ys\nend\nxp, ys = exceedance_plot_pos(dat_annmax.residual)\n\np2 = plot(return_periods, return_levels, linewidth=3, color=:blue, label=\"GEV Model Fit\", tickfontsize=16, legendfontsize=18, guidefontsize=18, bottom_margin=5mm, left_margin=5mm, right_margin=10mm, legend=:bottomright)\nplot!(p2, return_periods, return_levels_lognormal, linewidth=3, color=:orange, label=\"LogNormal Model Fit\")\nscatter!(p2, 1 ./ xp, ys, label=\"Observations\", color=:black, markersize=5)\nxlabel!(p2, \"Return Period (yrs)\")\nylabel!(p2, \"Return Level (m)\")\nxlims!(-1, 300)\nplot!(p2, size=(600, 450))\n\ndisplay(p1)\ndisplay(p2)\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) GEV fit to annual maxima of San Francisco Tide Gauge Data\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure 9"
  },
  {
    "objectID": "slides/lecture05-1.html#be-careful-about-the-shape-parameter",
    "href": "slides/lecture05-1.html#be-careful-about-the-shape-parameter",
    "title": "Extreme Value Theory & Modeling",
    "section": "Be Careful About The Shape Parameter!",
    "text": "Be Careful About The Shape Parameter!\n\n\n\n\nHouse flood risk sensitivity\n\n\n\n\nSource: Zarekarizi et al. (2020)"
  },
  {
    "objectID": "slides/lecture05-1.html#drawbacks-of-block-maxima",
    "href": "slides/lecture05-1.html#drawbacks-of-block-maxima",
    "title": "Extreme Value Theory & Modeling",
    "section": "Drawbacks of Block Maxima",
    "text": "Drawbacks of Block Maxima\nThe block-maxima approach has two potential drawbacks:\n\nUses a limited amount of data;\nDoesn’t capture the potential for multiple exceedances within a block."
  },
  {
    "objectID": "slides/lecture05-1.html#peaks-over-thresholds-1",
    "href": "slides/lecture05-1.html#peaks-over-thresholds-1",
    "title": "Extreme Value Theory & Modeling",
    "section": "Peaks Over Thresholds",
    "text": "Peaks Over Thresholds\nConsider the conditional excess distribution function\n\\[F_u(y) = \\mathbb{P}(X &gt; u + y \\ |\\  X &gt; u),\\]\nwhich is the cumulative distribution of values by which \\(X\\) exceeds \\(u\\) (given that the exceedance has occurred)."
  },
  {
    "objectID": "slides/lecture05-1.html#generalized-pareto-distribution-gpd",
    "href": "slides/lecture05-1.html#generalized-pareto-distribution-gpd",
    "title": "Extreme Value Theory & Modeling",
    "section": "Generalized Pareto Distribution (GPD)",
    "text": "Generalized Pareto Distribution (GPD)\nFor a large number of underlying distributions of \\(X\\), \\(F_u(y)\\) is well-approximated by a Generalized Pareto Distribution (GPD):\n\\[F\\_u(y) \\to G(y) = 1 - \\left[1 + \\xi\\left(\\frac{y-\\mu}{\\sigma}\\right)^{-1/\\xi}\\right],\\] defined for \\(y\\) such that \\(1 + \\xi(y-\\mu)/\\sigma &gt; 0\\)."
  },
  {
    "objectID": "slides/lecture05-1.html#generalized-pareto-distribution-gpd-1",
    "href": "slides/lecture05-1.html#generalized-pareto-distribution-gpd-1",
    "title": "Extreme Value Theory & Modeling",
    "section": "Generalized Pareto Distribution (GPD)",
    "text": "Generalized Pareto Distribution (GPD)\nSimilarly to the GEV distribution, the GPD distribution has three parameters:\n\nlocation \\(\\mu\\);\nscale \\(\\sigma &gt; 0\\);\nshape \\(\\xi\\)."
  },
  {
    "objectID": "slides/lecture05-1.html#gpd-types",
    "href": "slides/lecture05-1.html#gpd-types",
    "title": "Extreme Value Theory & Modeling",
    "section": "GPD Types",
    "text": "GPD Types\n\n\n\n\\(\\xi &gt; 0\\): heavy-tailed\n\\(\\xi = 0\\): light-tailed\n\\(\\xi &lt; 0\\): bounded\n\n\n\n\nCode\np1 = plot(-2:0.1:6, GeneralizedPareto(0, 1, 0.5), linewidth=3, color=:red, label=L\"$\\xi = 1/2$\", guidefontsize=18, legendfontsize=16, tickfontsize=16, left_margin=5mm, bottom_margin=10mm)\nplot!(-4:0.1:6, GeneralizedPareto(0, 1, 0), linewidth=3, color=:green, label=L\"$\\xi = 0$\")\nplot!(-4:0.1:2, GeneralizedPareto(0, 1, -0.5), linewidth=3, color=:blue, label=L\"$\\xi = -1/2$\")\nscatter!((-2, 0), color=:red, label=:false)\nscatter!((2, 0), color=:blue, label=:false)\nylabel!(\"Density\")\nxlabel!(L\"$x$\")\nplot!(size=(600, 450))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 10: Shape of the GPD distribution with different choices of \\(\\xi\\)."
  },
  {
    "objectID": "slides/lecture05-1.html#exceedances-can-occur-in-clusters",
    "href": "slides/lecture05-1.html#exceedances-can-occur-in-clusters",
    "title": "Extreme Value Theory & Modeling",
    "section": "Exceedances Can Occur In Clusters",
    "text": "Exceedances Can Occur In Clusters\n\n\nCode\nthresh = 1.0\ndat_ma_plot = @subset(dat_ma, year.(:datetime) .&gt; 2020)\ndat_ma_plot.residual = dat_ma_plot.residual ./ 1000\np1 = plot(dat_ma_plot.datetime, dat_ma_plot.residual; linewidth=2, ylabel=\"Gauge Weather Variability (m)\", label=\"Observations\", legend=:bottom, tickfontsize=16, guidefontsize=18, legendfontsize=16, xlabel=\"Date/Time\", right_margin=10mm, left_margin=5mm, bottom_margin=5mm)\nhline!([thresh], color=:red, linestyle=:dash, label=\"Threshold\")\nscatter!(dat_ma_plot.datetime[dat_ma_plot.residual .&gt; thresh], dat_ma_plot.residual[dat_ma_plot.residual .&gt; thresh], markershape=:x, color=:black, markersize=3, label=\"Exceedances\")\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 11: Peaks Over Thresholds for the SF Tide Gauge Data"
  },
  {
    "objectID": "slides/lecture05-1.html#declustering",
    "href": "slides/lecture05-1.html#declustering",
    "title": "Extreme Value Theory & Modeling",
    "section": "Declustering",
    "text": "Declustering\nArns et al. (2013) note: there is no clear declustering time period to use: need to rely on physical understanding of events and “typical” durations.\nIf we have prior knowledge about the duration of physical processes leading to clustered extremes (e.g. storm durations), can use this. Otherwise, need some way to estimate cluster duration from the data."
  },
  {
    "objectID": "slides/lecture05-1.html#extremal-index",
    "href": "slides/lecture05-1.html#extremal-index",
    "title": "Extreme Value Theory & Modeling",
    "section": "Extremal Index",
    "text": "Extremal Index\nThe most common is the extremal index \\(\\theta(u)\\), which measures the inter-exceedance time for a given threshold \\(u\\).\n\\[0 \\leq \\theta(u) \\leq 1,\\]\nwhere \\(\\theta(u) = 1\\) means independence and \\(\\theta(u) = 0\\) means the entire dataset is one cluster."
  },
  {
    "objectID": "slides/lecture05-1.html#extremal-index-1",
    "href": "slides/lecture05-1.html#extremal-index-1",
    "title": "Extreme Value Theory & Modeling",
    "section": "Extremal Index",
    "text": "Extremal Index\n\\(\\theta(u)\\) has two meanings:\n\nThe “propensity to cluster”: \\(\\theta\\) is the probability that the process has left one exceedance cluster;\nThe “reciprocal of the clustering duration”: \\(1/\\theta\\) is the mean time between clusters."
  },
  {
    "objectID": "slides/lecture05-1.html#computing-the-extremal-index",
    "href": "slides/lecture05-1.html#computing-the-extremal-index",
    "title": "Extreme Value Theory & Modeling",
    "section": "Computing the Extremal Index",
    "text": "Computing the Extremal Index\nThis estimator is taken from Ferro & Segers (2003).\nLet \\(N = \\sum_{i=1}^n \\mathbb{I}(X_i &gt; u)\\) be the total number of exceedances.\nDenote by \\(1 \\leq S_1 &lt; \\ldots &lt; S_N \\leq n\\) the exceedance times.\nThen the inter-exceedance times are \\[T_i = S_{i+1} - S_i, \\quad 1 \\leq i \\leq N-1.\\]"
  },
  {
    "objectID": "slides/lecture05-1.html#computing-the-extremal-index-1",
    "href": "slides/lecture05-1.html#computing-the-extremal-index-1",
    "title": "Extreme Value Theory & Modeling",
    "section": "Computing the Extremal Index",
    "text": "Computing the Extremal Index\n\\[\\hat{\\theta}(u) = \\frac{2\\left(\\sum_{i-1}^{N-1} T_i\\right)^2}{(N-1)\\sum_{i=1}^{N-1}T_i^2}\\]\n\n\nCode\n# find total number of exceedances and exceedance times\ndat_ma.residual = dat_ma.residual ./ 1000 # convert to m\nS = findall(dat_ma.residual .&gt; thresh)\nN = length(S)\nT = diff(S) # get difference between adjacent exceedances\nθ = 2 * sum(T)^2 / ((N-1) * sum(T.^2)) # extremal index\n\n\n0.23700514680023474\n\n\nFor the SF tide gauge data and \\(u=1.0 \\text{m}\\), we get the declustering time is 4.0 hours."
  },
  {
    "objectID": "slides/lecture05-1.html#mapping-data-to-clusters",
    "href": "slides/lecture05-1.html#mapping-data-to-clusters",
    "title": "Extreme Value Theory & Modeling",
    "section": "Mapping Data To Clusters",
    "text": "Mapping Data To Clusters\n\n# cluster data points which occur within period\nfunction assign_cluster(dat, period)\n    cluster_index = 1\n    clusters = zeros(Int, length(dat))\n    for i in 1:length(dat)\n        if clusters[i] == 0\n            clusters[findall(abs.(dat .- dat[i]) .&lt;= period)] .= cluster_index\n            cluster_index += 1\n        end\n    end\n    return clusters\nend\n\n# cluster exceedances that occur within a four-hour window\n# @transform is a macro from DataFramesMeta.jl which adds a new column based on a data transformation\ndat_exceed = dat_ma[dat_ma.residual .&gt; thresh, :]\ndat_exceed = @transform dat_exceed :cluster = assign_cluster(:datetime, Dates.Hour(4))\n# find maximum value within cluster\ndat_decluster = combine(dat_exceed -&gt; dat_exceed[argmax(dat_exceed.residual), :], \n    groupby(dat_exceed, :cluster))\ndat_decluster"
  },
  {
    "objectID": "slides/lecture05-1.html#declustered-distribution",
    "href": "slides/lecture05-1.html#declustered-distribution",
    "title": "Extreme Value Theory & Modeling",
    "section": "Declustered Distribution",
    "text": "Declustered Distribution\n\n\nCode\np = histogram(dat_decluster.residual .- thresh,\n    normalize = :pdf,\n    label=\"Data\",\n    xlabel=\"Threshold Exceedance (m)\",\n    ylabel=\"PDF\",\n    yticks=[],\n    tickfontsize=16,\n    guidefontsize=18,\n    legendfontsize=16,\n    left_margin=10mm, \n    right_margin=10mm,\n    bottom_margin=5mm\n    )\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 12: Histogram of clustered exceedances for SF tide gauge data."
  },
  {
    "objectID": "slides/lecture05-1.html#gpd-fit",
    "href": "slides/lecture05-1.html#gpd-fit",
    "title": "Extreme Value Theory & Modeling",
    "section": "GPD Fit",
    "text": "GPD Fit\n\nCode\n# fit GPD\ninit_θ = [1.0, 1.0]\nlow_bds = [0.0, -Inf]\nup_bds = [Inf, Inf]\ngpd_lik(θ) = -sum(logpdf(GeneralizedPareto(0.0, θ[1], θ[2]), dat_decluster.residual .- thresh))\nθ_mle = Optim.optimize(gpd_lik, low_bds, up_bds, init_θ).minimizer\np1 = plot!(p, GeneralizedPareto(0.0, θ_mle[1], θ_mle[2]), linewidth=3, label=\"GPD Fit\")\nplot!(size=(600, 450))\n\n# Q-Q Plot\np2 = qqplot(GeneralizedPareto(0.0, θ_mle[1], θ_mle[2]), dat_decluster.residual .- thresh, \n    xlabel=\"Theoretical Quantile\",\n    ylabel=\"Empirical Quantile\",\n    linewidth=3,\n    tickfontsize=16,\n    guidefontsize=18,\n    legendfontsize=16,\n    left_margin=5mm, \n    right_margin=10mm,\n    bottom_margin=5mm)\nplot!(size=(600, 450))\n\ndisplay(p1)\ndisplay(p2)\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) GPD fit to tide gauge readings over 1m of San Francisco Tide Gauge Data\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure 13"
  },
  {
    "objectID": "slides/lecture05-1.html#but-what-about-exceedance-frequency",
    "href": "slides/lecture05-1.html#but-what-about-exceedance-frequency",
    "title": "Extreme Value Theory & Modeling",
    "section": "But What About Exceedance Frequency?",
    "text": "But What About Exceedance Frequency?\n\n\nThe GPD fit gives a distribution for how extreme threshold exceedances are when they occur.\nBut how often do they occur?\n\n\n\nCode\n# add column with years of occurrence\ndat_decluster = @transform dat_decluster :year = Dates.year.(dat_decluster.datetime)\n# group by year and add up occurrences\nexceed_counts = combine(groupby(dat_decluster, :year), nrow =&gt; :count)\ndelete!(exceed_counts, nrow(exceed_counts)) # including 2023 will bias the count estimate\np = histogram(exceed_counts.count, legend=:false, \n    xlabel=\"Yearly Exceedances\",\n    ylabel=\"Count\",\n    guidefontsize=18,\n    tickfontsize=16,\n    left_margin=5mm,\n    bottom_margin=10mm\n)\nplot!(size=(600, 400))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 14: histogram of number of exceedances in each year"
  },
  {
    "objectID": "slides/lecture05-1.html#poisson---generalized-pareto-process",
    "href": "slides/lecture05-1.html#poisson---generalized-pareto-process",
    "title": "Extreme Value Theory & Modeling",
    "section": "Poisson - Generalized Pareto Process",
    "text": "Poisson - Generalized Pareto Process\nModel the number of new exceedances with a Poisson distribution\n\\[n \\sim \\text{Poisson}(\\lambda_u),\\]\nThe MLE for \\(\\lambda_u\\) is the mean of the count data, in this case 49.5.\nThen, for each \\(i=1, \\ldots, n\\), sample \\[X_i \\sim \\text{GeneralizedPareto}(u, \\sigma, \\xi).\\]\nto get the level for each exceedance."
  },
  {
    "objectID": "slides/lecture05-1.html#poisson---generalized-pareto-process-return-levels",
    "href": "slides/lecture05-1.html#poisson---generalized-pareto-process-return-levels",
    "title": "Extreme Value Theory & Modeling",
    "section": "Poisson - Generalized Pareto Process Return Levels",
    "text": "Poisson - Generalized Pareto Process Return Levels\nThen the return level for return period \\(m\\) years can be obtained by solving the quantile equation (see Coles (2001) for details):\n\\[\\text{RL}_m = \\begin{cases}u + \\frac{\\sigma}{\\xi} \\left((m\\lambda_u)^\\xi - 1\\right) & \\text{if}\\  \\xi \\neq 0 \\\\ u + \\sigma \\log(m\\lambda_u) & \\text{if}\\  \\xi = 0.\\end{cases}\\]"
  },
  {
    "objectID": "slides/lecture05-1.html#for-more-on-extremes",
    "href": "slides/lecture05-1.html#for-more-on-extremes",
    "title": "Extreme Value Theory & Modeling",
    "section": "For More on Extremes…",
    "text": "For More on Extremes…\nColes (2001) is the gold standard textbook."
  },
  {
    "objectID": "slides/lecture05-1.html#what-is-the-problem-with-default-evt",
    "href": "slides/lecture05-1.html#what-is-the-problem-with-default-evt",
    "title": "Extreme Value Theory & Modeling",
    "section": "What Is The Problem With “Default EVT”?",
    "text": "What Is The Problem With “Default EVT”?\nExtreme Value Theory (EVT) assumes each draw is i.i.d. from the relevant distribution.\nWhy might this not hold?"
  },
  {
    "objectID": "slides/lecture05-1.html#stationarity",
    "href": "slides/lecture05-1.html#stationarity",
    "title": "Extreme Value Theory & Modeling",
    "section": "Stationarity",
    "text": "Stationarity\nA **stationary process* is a stochastic process whose unconditional joint probability distribution does not change when shifted by time.\nBut often environmental processes are not stationary (e.g. El Niño)."
  },
  {
    "objectID": "slides/lecture05-1.html#example-climate-impacts",
    "href": "slides/lecture05-1.html#example-climate-impacts",
    "title": "Extreme Value Theory & Modeling",
    "section": "Example: Climate Impacts",
    "text": "Example: Climate Impacts\n\nAre storm frequencies and intensities changing?\nFrequency of cold-weather extremes?\nHeat waves frequencies and intensities"
  },
  {
    "objectID": "slides/lecture05-1.html#regression-models",
    "href": "slides/lecture05-1.html#regression-models",
    "title": "Extreme Value Theory & Modeling",
    "section": "Regression Models",
    "text": "Regression Models\nWe often model non-stationary extremes using a regression model, e.g.\n\\[\\text{GEV}(\\mu_0 + \\mu_1 t, \\sigma, \\xi)\\] or\n\\[\\text{GEV}(\\mu_0 + \\mu_1 T(t), \\sigma, \\xi).\\]"
  },
  {
    "objectID": "slides/lecture05-1.html#choosing-covariates",
    "href": "slides/lecture05-1.html#choosing-covariates",
    "title": "Extreme Value Theory & Modeling",
    "section": "Choosing Covariates",
    "text": "Choosing Covariates\nImportant to rely on domain knowledge, but often the covariates and types of dependence are hypotheses we would like to test.\n\nMore on this later (model selection!).\nOften accompanied by large parameter uncertainties."
  },
  {
    "objectID": "slides/lecture05-1.html#key-points",
    "href": "slides/lecture05-1.html#key-points",
    "title": "Extreme Value Theory & Modeling",
    "section": "Key Points",
    "text": "Key Points\n\nExtreme values can be modeled as block maxima or peaks-over-thresholds.\nBlock Maxima: Generalized Extreme Value distributions.\nPeaks-Over-Thresholds: Generalized Pareto distributions (plus maybe Poisson processes).\nStatistical models are highly sensitive to details: shape parameters \\(\\xi\\), thresholds \\(u\\), etc.\nModels assume independent variables."
  },
  {
    "objectID": "slides/lecture05-1.html#what-we-havent-discussed",
    "href": "slides/lecture05-1.html#what-we-havent-discussed",
    "title": "Extreme Value Theory & Modeling",
    "section": "What We Haven’t Discussed",
    "text": "What We Haven’t Discussed\n\nNonstationary models\nMultivariate extremes are difficult: what does this even mean?"
  },
  {
    "objectID": "slides/lecture05-1.html#upcoming-schedule",
    "href": "slides/lecture05-1.html#upcoming-schedule",
    "title": "Extreme Value Theory & Modeling",
    "section": "Upcoming Schedule",
    "text": "Upcoming Schedule\nWednesday: Clusters and Mixture models\nMonday: February Break!\nNext Wednesday: In-Class Figure Discussion"
  },
  {
    "objectID": "slides/lecture05-1.html#assessments",
    "href": "slides/lecture05-1.html#assessments",
    "title": "Extreme Value Theory & Modeling",
    "section": "Assessments",
    "text": "Assessments\nFriday:\n\nSubmit figures for discussion (Exercise 5)\nHW2 Due\nProject proposal"
  },
  {
    "objectID": "slides/lecture05-1.html#references",
    "href": "slides/lecture05-1.html#references",
    "title": "Extreme Value Theory & Modeling",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n\n\nArns, A., Wahl, T., Haigh, I. D., Jensen, J., & Pattiaratchi, C. (2013). Estimating extreme water level probabilities: A comparison of the direct methods and recommendations for best practise. Coast. Eng., 81, 51–66. https://doi.org/10.1016/j.coastaleng.2013.07.003\n\n\nColes, S. (2001). An Introduction to Statistical Modeling of Extreme Values. London: Springer-Verlag London.\n\n\nFerro, C. A. T., & Segers, J. (2003). Inference for clusters of extreme values. J. R. Stat. Soc. Series B Stat. Methodol., 65, 545–556. https://doi.org/10.1111/1467-9868.00401\n\n\nZarekarizi, M., Srikrishnan, V., & Keller, K. (2020). Neglecting uncertainties biases house-elevation decisions to manage riverine flood risks. Nat. Commun., 11, 5361. https://doi.org/10.1038/s41467-020-19188-9"
  },
  {
    "objectID": "slides/lecture05-2.html#two-ways-to-frame-extreme-values",
    "href": "slides/lecture05-2.html#two-ways-to-frame-extreme-values",
    "title": "Data Visualization",
    "section": "Two Ways To Frame “Extreme” Values",
    "text": "Two Ways To Frame “Extreme” Values\n\n“Block” extremes, e.g. annual maxima (block maxima)?\nValues which exceed a certain threshold (peaks over threshold)?"
  },
  {
    "objectID": "slides/lecture05-2.html#two-ways-to-frame-extreme-values-1",
    "href": "slides/lecture05-2.html#two-ways-to-frame-extreme-values-1",
    "title": "Data Visualization",
    "section": "Two Ways To Frame “Extreme” Values",
    "text": "Two Ways To Frame “Extreme” Values\n\nBlock Maxima: Generalized Extreme Value (GEV) distributions.\nPeaks-Over-Thresholds: Generalized Pareto distributions (GP) (plus maybe Poisson processes).\nStatistical models are highly sensitive to details: shape parameters \\(\\xi\\), thresholds \\(u\\), etc.\nModels assume independent variables."
  },
  {
    "objectID": "slides/lecture05-2.html#purposes-of-visualizing-data",
    "href": "slides/lecture05-2.html#purposes-of-visualizing-data",
    "title": "Data Visualization",
    "section": "Purposes of Visualizing Data",
    "text": "Purposes of Visualizing Data\n\n\nExploratory Analysis\nCommunication\nInterpretation"
  },
  {
    "objectID": "slides/lecture05-2.html#quantitative-summaries-can-be-insufficient",
    "href": "slides/lecture05-2.html#quantitative-summaries-can-be-insufficient",
    "title": "Data Visualization",
    "section": "Quantitative Summaries Can Be Insufficient",
    "text": "Quantitative Summaries Can Be Insufficient\n\n\nCode\n# load Anscombe's Quartet data\ndf = dataset(\"datasets\", \"anscombe\")\n\nmodel1 = lm(@formula(Y1 ~ X1), df)\nmodel2 = lm(@formula(Y2 ~ X2), df)\nmodel3 = lm(@formula(Y3 ~ X3), df)\nmodel4 = lm(@formula(Y4 ~ X4), df)\n\nyHat(model, X) = coef(model)' * [ 1 , X ]\nxlims = [0, 20]\n\np1 = scatter(df.X1, df.Y1, c=:blue, msw=0, ms=8)\np1 = plot!(xlims, [yHat(model1, x) for x in xlims], c=:red, xlims=(xlims), linewidth=2)\n\np2 = scatter(df.X2, df.Y2, c=:blue, msw=0, ms=8)\np2 = plot!(xlims, [yHat(model2, x) for x in xlims], c=:red, xlims=(xlims), linewidth=2)\n\np3 = scatter(df.X3, df.Y3, c=:blue, msw=0, ms=8)\np3 = plot!(xlims, [yHat(model3, x) for x in xlims], c=:red, xlims=(xlims), linewidth=2)\n\np4 = scatter(df.X4, df.Y4, c=:blue, msw=0, ms=8)\np4 = plot!(xlims, [yHat(model4, x) for x in xlims], c=:red, msw=0, xlims=(xlims), linewidth=2)\n\nplot(p1, p2, p3, p4, layout = (1,4), xlims=(0,20), ylims=(0,14), \n    legend=:none, xlabel = \"x\", ylabel = \"y\",\n    tickfontsize=16, guidefontsize=18,\n    left_margin=5mm, bottom_margin=10mm, right_margin=5mm)\nplot!(size=(1200, 400))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Anscombe’s Quartet"
  },
  {
    "objectID": "slides/lecture05-2.html#challenges-for-effective-visualization",
    "href": "slides/lecture05-2.html#challenges-for-effective-visualization",
    "title": "Data Visualization",
    "section": "Challenges for Effective Visualization",
    "text": "Challenges for Effective Visualization\n\n\nLimits From Cognitive Processes\nNo “Optimal” Visualization\nTemptation To Overload Figures\nEasy to “Lie” About The Data"
  },
  {
    "objectID": "slides/lecture05-2.html#further-challenges",
    "href": "slides/lecture05-2.html#further-challenges",
    "title": "Data Visualization",
    "section": "Further Challenges",
    "text": "Further Challenges\nFollowing Munzner (2014):\n\n\nPossible designs are a bad match with human perceptual and cognitive systems;\nPossible designs are a bad match with the intended task;\nOnly a small number of possibilities are reasonable choices;\n“Randomly choosing possibilities is a bad idea because the odds of finding a very good solution are very low.”"
  },
  {
    "objectID": "slides/lecture05-2.html#what-can-go-wrong",
    "href": "slides/lecture05-2.html#what-can-go-wrong",
    "title": "Data Visualization",
    "section": "What Can Go Wrong?",
    "text": "What Can Go Wrong?\nHealy (2018):\n\n\nBad Taste;\nBad Data;\nBad Perception"
  },
  {
    "objectID": "slides/lecture05-2.html#remember-data-never-speaks-for-itself",
    "href": "slides/lecture05-2.html#remember-data-never-speaks-for-itself",
    "title": "Data Visualization",
    "section": "Remember: Data Never Speaks For Itself",
    "text": "Remember: Data Never Speaks For Itself\nData must be understood in a particular context. You need to understand your data and what it says (or does not say!) based on your hypotheses.\n\n\nWhat question(s) does your data address?\nWhat transformations make the representation of the data as salient as possible?\nWhat scales or channels are most appropriate?"
  },
  {
    "objectID": "slides/lecture05-2.html#some-caveats",
    "href": "slides/lecture05-2.html#some-caveats",
    "title": "Data Visualization",
    "section": "Some Caveats",
    "text": "Some Caveats\n\n\nThere is no recipe to effective visualization. Everything depends on your data and the story you want to tell.\nThis also means that defaults from data visualization packages are usually bad.\nThese principles are largely based on Western (American/European) norms.\nA lot of these guidelines are based on average outcomes, there is likely to be a lot of individual variation."
  },
  {
    "objectID": "slides/lecture05-2.html#stages-of-human-visual-perception",
    "href": "slides/lecture05-2.html#stages-of-human-visual-perception",
    "title": "Data Visualization",
    "section": "Stages of Human Visual Perception",
    "text": "Stages of Human Visual Perception\n\n\nRapid, pre-attentive parallel processing to extract basic features;\nSlow serial processing for extraction of patterns;\nGoal-based retention of a few pieces of information in working memory related to a question at hand."
  },
  {
    "objectID": "slides/lecture05-2.html#working-memory-is-limited",
    "href": "slides/lecture05-2.html#working-memory-is-limited",
    "title": "Data Visualization",
    "section": "Working Memory Is Limited!",
    "text": "Working Memory Is Limited!\nEstimates of the number of “bits” we can keep in working memory vary, but:\n\n\nLimit is small;\nExceeding limit results in cognitive load;\nWorking memory is subject to “change blindness”\n\n\n\nThe more cognitive work you ask of your viewer, the less they are likely to take away and retain!"
  },
  {
    "objectID": "slides/lecture05-2.html#gestalt-principles",
    "href": "slides/lecture05-2.html#gestalt-principles",
    "title": "Data Visualization",
    "section": "Gestalt Principles",
    "text": "Gestalt Principles\nThe Gestalt school of psychology identified several principles of perception.\nCore idea: Humans are very good at finding structure.\n\nAs a result, you need to evaluate the totality of a visual field, not just each component."
  },
  {
    "objectID": "slides/lecture05-2.html#gestalt-principles-1",
    "href": "slides/lecture05-2.html#gestalt-principles-1",
    "title": "Data Visualization",
    "section": "Gestalt Principles",
    "text": "Gestalt Principles\n\n\n\nProximity\nSimilarity\nParallelism\nCommon Fate\nCommon Region\nContinuity\nClosure\n\n\n\n\n\nIllustration of Gestalt principles\n\n\n\nIllustration of several Gestalt principles. Adapted from Healy (2018)."
  },
  {
    "objectID": "slides/lecture05-2.html#dont-add-unnecessary-artifacts",
    "href": "slides/lecture05-2.html#dont-add-unnecessary-artifacts",
    "title": "Data Visualization",
    "section": "Don’t Add Unnecessary Artifacts!",
    "text": "Don’t Add Unnecessary Artifacts!\n\n\nUnnecessary artifacts can be “chartjunk.”\nBut worse, they might mislead the viewer.\n\n\n\nCode\np1 = plot(x_pred, y_ci_low, fillrange=y_ci_hi, xlabel=L\"$x$\", ylabel=L\"$y$\", fillalpha=0.3, fillcolor=:blue, label=\"95% Prediction Interval\", legend=:topleft, linealpha=0, legendfontsize=12, tickfontsize=14, guidefontsize=14) \nplot!(p1, x_pred, y_med, color=:blue, label=\"Prediction Median\")\nscatter!(p1, x, y, color=:red, markershape=:x, label=\"Data\")\n\np2 = plot(x_pred, y_ci_low, fillrange=y_ci_hi, xlabel=L\"$x$\", ylabel=L\"$y$\", fillalpha=0.3, fillcolor=:blue, label=\"95% Prediction Interval\", legend=:topleft, linealpha=0, legendfontsize=12, tickfontsize=14, guidefontsize=14) \nplot!(p2, x_pred, y_med, color=:blue, label=\"Prediction Median\")\nplot!(p2, x, y, color=:red, seriestype=:line, label=:false)\nscatter!(p2, x, y, color=:red, markershape=:x, label=\"Data\")\n\nplot(p1, p2, layout=(1, 2), size=(800, 600))"
  },
  {
    "objectID": "slides/lecture05-2.html#channels",
    "href": "slides/lecture05-2.html#channels",
    "title": "Data Visualization",
    "section": "Channels",
    "text": "Channels\nA channel is a mechanism for encoding information.\n\nExamples:\n\nColor (Hue/Saturation/Luminescence)\nPosition (1D/2D/3D)\nSize (Length/Area/Volume)\nAngle"
  },
  {
    "objectID": "slides/lecture05-2.html#ordered-vs.-categorical-attributes",
    "href": "slides/lecture05-2.html#ordered-vs.-categorical-attributes",
    "title": "Data Visualization",
    "section": "Ordered vs. Categorical Attributes",
    "text": "Ordered vs. Categorical Attributes\nThe channels available depend on the type of attribute:\n\nOrdered attributes can be\n\nOrdinal: Ranking, no meaning to distance;\nQuantitative: Measure of magnitude which supports arithmetic comparison;\n\nCategorical attributes are unordered."
  },
  {
    "objectID": "slides/lecture05-2.html#channel-effectiveness-ordered-data",
    "href": "slides/lecture05-2.html#channel-effectiveness-ordered-data",
    "title": "Data Visualization",
    "section": "Channel Effectiveness: Ordered Data",
    "text": "Channel Effectiveness: Ordered Data\n\n\n\n\n\n\n\n\n\nChannels for ordered data, arranged top-to-bottom from more to less effective (channels in the right column are less effective than those in the left). Modified from Healy (2018) after Munzner (2014)."
  },
  {
    "objectID": "slides/lecture05-2.html#channel-effectiveness-categorical-data",
    "href": "slides/lecture05-2.html#channel-effectiveness-categorical-data",
    "title": "Data Visualization",
    "section": "Channel Effectiveness: Categorical Data",
    "text": "Channel Effectiveness: Categorical Data\n\n\n\n\n\n\n\n\n\nChannels for categorical data, arranged top-to-bottom from more to less effective. Modified from Healy (2018) after Munzer (2014)."
  },
  {
    "objectID": "slides/lecture05-2.html#preattentive-popout",
    "href": "slides/lecture05-2.html#preattentive-popout",
    "title": "Data Visualization",
    "section": "Preattentive Popout",
    "text": "Preattentive Popout\n\n\nTry to make your key features “pop out” to the viewer during the pre-attentive scan.\n\nSearching for the blue circle becomes harder. Adapted from Healy (2018).\n\n\n\n\nCode\nnpt = 20\ndist = Distributions.Product(Uniform.([0, 0], [1, 1]))\npts = Tuple.(eachcol(rand(dist, npt)))\nblueidx = rand(1:npt)\np1 = scatter(pts[1:end .!= blueidx], color=:red, xticks=:false, yticks=:false, legend=:false, markersize=5, title=\"Color Only, N=20\", framestyle=:box)\nscatter!(p1, pts[blueidx, :], color=:blue, markersize=5)\n\nnpt = 100\npts = Tuple.(eachcol(rand(dist, npt)))\nblueidx = rand(1:npt)\np2 = scatter(pts[1:end .!= blueidx], color=:red, xticks=:false, yticks=:false, legend=:false, markersize=5, title=\"Color Only, N=100\", framestyle=:box)\nscatter!(p2, pts[blueidx, :], color=:blue, markersize=5)\n\nnpt = 20\npts = Tuple.(eachcol(rand(dist, npt)))\nblueidx = rand(1:npt)\np3 = scatter(pts[1:end .!= blueidx], color=:blue, markershape=:utriangle, xticks=:false, yticks=:false, legend=:false, markersize=5, title=\"Shape Only, N=20\", framestyle=:box)\nscatter!(p3, pts[blueidx, :], color=:blue, markersize=5, markershape=:circle)\n\nnpt = 100\npts = Tuple.(eachcol(rand(dist, npt)))\nblueidx = rand(1:npt)\np4 = scatter(pts[1:end .!= blueidx], color=:blue, markershape=:utriangle, xticks=:false, yticks=:false, legend=:false, markersize=5, title=\"Shape Only, N=100\", framestyle=:box)\nscatter!(p4, pts[blueidx, :], color=:blue, markersize=5, markershape=:circle)\n\nplot(p1, p2, p3, p4, layout=(2, 2), size=(800, 500))"
  },
  {
    "objectID": "slides/lecture05-2.html#channel-interference",
    "href": "slides/lecture05-2.html#channel-interference",
    "title": "Data Visualization",
    "section": "Channel Interference",
    "text": "Channel Interference\n\n\nWhen using multiple channels, be careful about interference: reducing the effectiveness of both channels."
  },
  {
    "objectID": "slides/lecture05-2.html#color-schemes",
    "href": "slides/lecture05-2.html#color-schemes",
    "title": "Data Visualization",
    "section": "Color Schemes",
    "text": "Color Schemes\nDifferent color schemes are appropriate depending on whether the data is sequential, divergent, or unordered.\n\n\n\n\n\n\nAppropriate Color Schemes\n\n\nColor schemes should be perceptually uniform to preserve a mapping between changes in perceived colors and changes in attribute values.\nTry to also choose color schemes which avoid confusing people who are color blind."
  },
  {
    "objectID": "slides/lecture05-2.html#color-schemes-1",
    "href": "slides/lecture05-2.html#color-schemes-1",
    "title": "Data Visualization",
    "section": "Color Schemes",
    "text": "Color Schemes\nGood news: Most plotting libraries include a wide variety of perceptually uniform, color-blind safe color schemes.\nBad news: These are not usually the defaults (in particular, avoid “rainbow” color schemes)."
  },
  {
    "objectID": "slides/lecture05-2.html#sequential-color-schemes",
    "href": "slides/lecture05-2.html#sequential-color-schemes",
    "title": "Data Visualization",
    "section": "Sequential Color Schemes",
    "text": "Sequential Color Schemes\nSequential schemes change in intensity from low to high as the value changes."
  },
  {
    "objectID": "slides/lecture05-2.html#divergent-color-schemes",
    "href": "slides/lecture05-2.html#divergent-color-schemes",
    "title": "Data Visualization",
    "section": "Divergent Color Schemes",
    "text": "Divergent Color Schemes\nDivergent schemes intensify in two directions from a zero or mean value."
  },
  {
    "objectID": "slides/lecture05-2.html#unordered-color-schemes",
    "href": "slides/lecture05-2.html#unordered-color-schemes",
    "title": "Data Visualization",
    "section": "Unordered Color Schemes",
    "text": "Unordered Color Schemes\nUnordered schemes are appropriate for categorical data."
  },
  {
    "objectID": "slides/lecture05-2.html#thoughts-on-this-plot",
    "href": "slides/lecture05-2.html#thoughts-on-this-plot",
    "title": "Data Visualization",
    "section": "Thoughts On This Plot?",
    "text": "Thoughts On This Plot?\n\n\n\n\nFirst Street Foundation Return Period Trends\n\n\n\n\nSource: Shu et al. (2023)"
  },
  {
    "objectID": "slides/lecture05-2.html#how-about-this-one",
    "href": "slides/lecture05-2.html#how-about-this-one",
    "title": "Data Visualization",
    "section": "How About This One?",
    "text": "How About This One?\n\n\n\n\nTrump Polling Average vs. Employment in Swing States\n\n\n\n\nSource: Joe Weisenthal"
  },
  {
    "objectID": "slides/lecture05-2.html#last-one",
    "href": "slides/lecture05-2.html#last-one",
    "title": "Data Visualization",
    "section": "Last One!",
    "text": "Last One!\n\n\n\n\nModeled Flood Risk vs. Perception\n\n\n\n\nSource: Bakkensen & Barrage (2022)"
  },
  {
    "objectID": "slides/lecture05-2.html#recommendations",
    "href": "slides/lecture05-2.html#recommendations",
    "title": "Data Visualization",
    "section": "Recommendations",
    "text": "Recommendations\n\n\nDon’t add extraneous artifacts.\nMake key features “pop out,” or annotate them.\nSummarize data to reduce complexity.\nTry to prioritize high-effectiveness channels.\nDon’t use 3d!\nMix channels sparingly (but redundancy is good!).\nIs the figure an improvement over a table?"
  },
  {
    "objectID": "slides/lecture05-2.html#but-the-biggest-recommendation-of-all",
    "href": "slides/lecture05-2.html#but-the-biggest-recommendation-of-all",
    "title": "Data Visualization",
    "section": "But the Biggest Recommendation of All…",
    "text": "But the Biggest Recommendation of All…\nBe intentional with your choices based on your storytelling goal!\nRelying on defaults will usually steer you wrong, and all “rules” can be broken if they help you tell your story more effectively."
  },
  {
    "objectID": "slides/lecture05-2.html#what-about-exploratory-analysis",
    "href": "slides/lecture05-2.html#what-about-exploratory-analysis",
    "title": "Data Visualization",
    "section": "What About Exploratory Analysis?",
    "text": "What About Exploratory Analysis?\nWhen exploring data, try lots of things.\n\nDon’t over-interpret one visualization.\nTry to rely on hypotheses about what you might see instead of dredging through the data."
  },
  {
    "objectID": "slides/lecture05-2.html#upcoming-schedule",
    "href": "slides/lecture05-2.html#upcoming-schedule",
    "title": "Data Visualization",
    "section": "Upcoming Schedule",
    "text": "Upcoming Schedule\nMonday: February Break!\nNext Wednesday: In-Class Figure Discussion"
  },
  {
    "objectID": "slides/lecture05-2.html#assessments",
    "href": "slides/lecture05-2.html#assessments",
    "title": "Data Visualization",
    "section": "Assessments",
    "text": "Assessments\nFriday:\n\nSubmit figures for discussion (Exercise 5)\nHW2 Due"
  },
  {
    "objectID": "slides/lecture05-2.html#references",
    "href": "slides/lecture05-2.html#references",
    "title": "Data Visualization",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n\n\nBakkensen, L. A., & Barrage, L. (2022). Going Underwater? Flood Risk Belief Heterogeneity and Coastal Home Price Dynamics. Rev. Financ. Stud., 35, 3666–3709. https://doi.org/10.1093/rfs/hhab122\n\n\nHealy, K. (2018). Data visualization: A practical introduction. Princeton University Press.\n\n\nMunzner, T. (2014). Visualization analysis and design. CRC Press.\n\n\nShu, E., Hauer, M., & Porter, J. (2023, November). Future population exposure to flood risk: A decomposition approach across Shared-Socioeconomic pathways (SSPs). Research Square. https://doi.org/10.21203/rs.3.rs-3628132/v1"
  },
  {
    "objectID": "slides/lecture03-1.html#probability-models",
    "href": "slides/lecture03-1.html#probability-models",
    "title": "Exploratory Data Analysis, Correlations, and Autocorrelations",
    "section": "Probability Models",
    "text": "Probability Models\nGoal: Write down a probability model for the data-generating process for \\(\\mathbf{y}\\).\n\nDirect statistical model, \\[\\mathbf{y} \\sim \\mathcal{D}(\\theta).\\]\nModel for the residuals of a numerical model, \\[\\mathbf{r} = \\mathbf{y} - F(\\mathbf{x}) \\sim \\mathcal{D}(\\theta).\\]"
  },
  {
    "objectID": "slides/lecture03-1.html#model-fitting-as-maximum-likelihood-estimation",
    "href": "slides/lecture03-1.html#model-fitting-as-maximum-likelihood-estimation",
    "title": "Exploratory Data Analysis, Correlations, and Autocorrelations",
    "section": "Model Fitting as Maximum Likelihood Estimation",
    "text": "Model Fitting as Maximum Likelihood Estimation\nWe can interpret fitting a model (reducing error according to some loss or error metric) as maximizing the probability of observing our data from this data generating process."
  },
  {
    "objectID": "slides/lecture03-1.html#eda-assessing-model-data-fit",
    "href": "slides/lecture03-1.html#eda-assessing-model-data-fit",
    "title": "Exploratory Data Analysis, Correlations, and Autocorrelations",
    "section": "EDA: Assessing Model-Data Fit",
    "text": "EDA: Assessing Model-Data Fit\nExamples:\n\n\nSkewness\nTails\nClusters\n(Auto)correlations"
  },
  {
    "objectID": "slides/lecture03-1.html#curve-fitting",
    "href": "slides/lecture03-1.html#curve-fitting",
    "title": "Exploratory Data Analysis, Correlations, and Autocorrelations",
    "section": "Curve Fitting",
    "text": "Curve Fitting\n\n\n\n\n\nSource: XKCD #2048"
  },
  {
    "objectID": "slides/lecture03-1.html#is-eda-ever-model-free",
    "href": "slides/lecture03-1.html#is-eda-ever-model-free",
    "title": "Exploratory Data Analysis, Correlations, and Autocorrelations",
    "section": "Is EDA Ever Model-Free?",
    "text": "Is EDA Ever Model-Free?\nSome characterize EDA as model-free (versus “confirmatory” data analysis).\nIs this right?"
  },
  {
    "objectID": "slides/lecture03-1.html#visual-approaches-to-eda",
    "href": "slides/lecture03-1.html#visual-approaches-to-eda",
    "title": "Exploratory Data Analysis, Correlations, and Autocorrelations",
    "section": "Visual Approaches to EDA",
    "text": "Visual Approaches to EDA\nMake plots!\n\nHistograms\nScatterplots/Pairs plots\nQuantile-Quantile Plots"
  },
  {
    "objectID": "slides/lecture03-1.html#skew",
    "href": "slides/lecture03-1.html#skew",
    "title": "Exploratory Data Analysis, Correlations, and Autocorrelations",
    "section": "Skew",
    "text": "Skew\n\nCode\n# specify regression model\nf(x) = 2 + 2 * x\npred = rand(Uniform(0, 10), 20)\ntrend = f.(pred)\n\n# generate normal residuals\nnormal_residuals = rand(Normal(0, 3), length(pred))\nnormal_obs = trend .+ normal_residuals\n\n## generate skewed residuals\nskewed_residuals = rand(SkewNormal(0, 3, 2), length(pred))\nskewed_obs = trend .+ skewed_residuals\n\n# make plots\n# scatterplot of observations\np1 = plot(0:10, f.(0:10), color=:black, label=\"Trend Line\", linewidth=2, guidefontsize=18, tickfontsize=16, legendfontsize=16) # trend line\nscatter!(p1, pred, normal_obs, color=:orange, markershape=:circle, label=\"Normal Residuals\")\nscatter!(p1, pred, skewed_obs, color=:green, markershape=:square, label=\"Skewed Residuals\")\nxlabel!(p1, \"Predictors\")\nylabel!(p1, \"Observations\")\nplot!(p1, size=(600, 450))\n\n# densities of residual distributions\np2 = histogram(rand(Normal(0, 3), 1000), color=:orange, alpha=0.5, label=\"Normal Distribution\", guidefontsize=18, tickfontsize=16, legendfontsize=16)\nhistogram!(p2, rand(SkewNormal(0, 3, 2), 1000), color=:green, alpha=0.5, label=\"SkewNormal Distribution\")\nxlabel!(\"Value\")\nylabel!(\"Count\")\nplot!(p2, size=(600, 450))\n\ndisplay(p1)\ndisplay(p2)\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Normal vs. Skewed Data\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Histograms\n\n\n\n\n\n\n\nFigure 1: Skew"
  },
  {
    "objectID": "slides/lecture03-1.html#fat-tails",
    "href": "slides/lecture03-1.html#fat-tails",
    "title": "Exploratory Data Analysis, Correlations, and Autocorrelations",
    "section": "Fat Tails",
    "text": "Fat Tails\n\nCode\n# generate normal residuals\nnormal_residuals = rand(Normal(0, 3), length(pred))\nnormal_obs = trend .+ normal_residuals\n\n## generate fat-tailed residuals\ncauchy_residuals = rand(Cauchy(0, 1), length(pred))\ncauchy_obs = trend .+ cauchy_residuals\n\n# make plots\n# scatterplot of observations\np1 = plot(0:10, f.(0:10), color=:black, label=\"Trend Line\", linewidth=2, guidefontsize=18, tickfontsize=16, legendfontsize=16) # trend line\nscatter!(p1, pred, normal_obs, color=:orange, markershape=:circle, label=\"Normal Residuals\")\nscatter!(p1, pred, cauchy_obs, color=:green, markershape=:square, label=\"Fat-Tailed Residuals\")\nxlabel!(p1, \"Predictors\")\nylabel!(p1, \"Observations\")\nplot!(p1, size=(600, 450))\n\n# densities of residual distributions\np2 = histogram(rand(Normal(0, 3), 1000), color=:orange, alpha=0.5, label=\"Normal Distribution\", guidefontsize=18, tickfontsize=16, legendfontsize=16)\nhistogram!(p2, rand(Cauchy(0, 1), 1000), color=:green, alpha=0.5, label=\"Cauchy Distribution\")\nxlabel!(\"Value\")\nylabel!(\"Count\")\nplot!(p2, size=(600, 450))\nxlims!(-20, 20)\n\ndisplay(p1)\ndisplay(p2)\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Normal vs. Fat-Tailed Data\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Histograms\n\n\n\n\n\n\n\nFigure 2: Fat Tails"
  },
  {
    "objectID": "slides/lecture03-1.html#quantile-quantile-q-q-plots",
    "href": "slides/lecture03-1.html#quantile-quantile-q-q-plots",
    "title": "Exploratory Data Analysis, Correlations, and Autocorrelations",
    "section": "Quantile-Quantile (Q-Q) Plots",
    "text": "Quantile-Quantile (Q-Q) Plots\n\n\nParticularly with small sample sizes, just staring at data can be unhelpful.\nQ-Q plots are useful for checking goodness of fit of a proposed distribution.\n\n\n\nCode\nsamps = rand(Normal(0, 3), 20)\nqqplot(Normal, samps, tickfontsize=16, guidefontsize=18, linewidth=3, markersize=6)\nxlabel!(\"Theoretical Quantiles\")\nylabel!(\"Empirical Quantiles\")\nplot!(size=(500, 450))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3"
  },
  {
    "objectID": "slides/lecture03-1.html#q-q-plot-example",
    "href": "slides/lecture03-1.html#q-q-plot-example",
    "title": "Exploratory Data Analysis, Correlations, and Autocorrelations",
    "section": "Q-Q Plot Example",
    "text": "Q-Q Plot Example\n\nCode\n## generate fat-tailed residuals\ncauchy_samps = rand(Cauchy(0, 1), 20)\n\n# make plots\n# scatterplot of observations\np1 = plot(Normal(0, 2), linewidth=3, color=:green, label=\"Normal Distribution\", yaxis=false, tickfontsize=16, guidefontsize=18, legendfontsize=18)\nplot!(p1, Cauchy(0, 1), linewidth=3, color=:orange, linestyle=:dash, label=\"Cauchy Distribution\")\nxlims!(p1, (-10, 10))\nxlabel!(\"Value\")\nplot!(p1, size=(500, 450))\n\n# densities of residual distributions\np2 = qqplot(Normal, cauchy_samps, tickfontsize=16, guidefontsize=18, linewidth=3, markersize=6)\nxlabel!(p2, \"Theoretical Quantiles\")\nylabel!(p2, \"Empirical Quantiles\")\nplot!(p2, size=(500, 450))\n\ndisplay(p1)\ndisplay(p2)\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Normal vs Cauchy Distribution\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Q-Q Plot\n\n\n\n\n\n\n\nFigure 4: Q-Q Plot for Cauchy Data and Normal Distribution"
  },
  {
    "objectID": "slides/lecture03-1.html#predictive-checks",
    "href": "slides/lecture03-1.html#predictive-checks",
    "title": "Exploratory Data Analysis, Correlations, and Autocorrelations",
    "section": "Predictive Checks",
    "text": "Predictive Checks\nHow well do projections match data (Gelman, 2004)?\nCan be quantitative or qualitative."
  },
  {
    "objectID": "slides/lecture03-1.html#predictive-check-ebm-example",
    "href": "slides/lecture03-1.html#predictive-check-ebm-example",
    "title": "Exploratory Data Analysis, Correlations, and Autocorrelations",
    "section": "Predictive Check: EBM Example",
    "text": "Predictive Check: EBM Example\n\n\nCode\n# set number of sampled simulations\nn_samples = 1000\nresiduals = rand(Normal(0, θ[end]), (n_samples, length(temp_obs)))\nmodel_out = ebm_wrap(θ[1:end-1])\n# this uses broadcasting to \"sweep\" the model simulation across the sampled residual matrix\nmodel_sim = residuals .+ model_out' # need to transpose the model output vector due to how Julia treats vector dimensions\n\nq_90 = mapslices(col -&gt; quantile(col, [0.05, 0.95]), model_sim,; dims=1) # compute 90% prediction interval\n\nplot(hind_years, model_out, color=:red, linewidth=3, label=\"Model Simulation\", ribbon=(model_out .- q_90[1, :], q_90[2, :] .- model_out), fillalpha=0.3, xlabel=\"Year\", ylabel=\"Temperature anomaly (°C)\", guidefontsize=18, tickfontsize=16, legendfontsize=16, bottom_margin=5mm, left_margin=5mm)\nscatter!(hind_years, temp_obs, color=:black, label=\"Data\")\nylims!(-0.5, 1.2)\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Comparison of best fit with uncertain realization for the EBM with normal residuals."
  },
  {
    "objectID": "slides/lecture03-1.html#predictive-checks-1",
    "href": "slides/lecture03-1.html#predictive-checks-1",
    "title": "Exploratory Data Analysis, Correlations, and Autocorrelations",
    "section": "Predictive Checks",
    "text": "Predictive Checks\nCan also do predictive checks with summary statistics:\n\nReturn periods\nMaximum/minimum values\nPredictions for out-of-sample data"
  },
  {
    "objectID": "slides/lecture03-1.html#what-is-correlation",
    "href": "slides/lecture03-1.html#what-is-correlation",
    "title": "Exploratory Data Analysis, Correlations, and Autocorrelations",
    "section": "What Is Correlation?",
    "text": "What Is Correlation?\nCorrelation refers to whether two variables increase or decrease simultaneously.\nTypically measured with Pearson’s coefficient:\n\\[r = \\frac{\\text{Cov}(X, Y)}{\\sigma_X \\sigma_Y} \\in (-1, 1)\\]"
  },
  {
    "objectID": "slides/lecture03-1.html#correlation-examples",
    "href": "slides/lecture03-1.html#correlation-examples",
    "title": "Exploratory Data Analysis, Correlations, and Autocorrelations",
    "section": "Correlation Examples",
    "text": "Correlation Examples\n\nCode\n# sample 1000 independent variables from a given normal distribution\nsample_independent = rand(Normal(0, 1), (2, 1000))\np1 = scatter(sample_independent[1, :], sample_independent[2, :], label=:false, title=\"Independent Variables\", tickfontsize=14, titlefontsize=18, guidefontsize=18)\nxlabel!(p1, L\"$x_1$\")\nylabel!(p1, L\"$x_2$\")\nplot!(p1, size=(400, 500))\n\n# sample 1000 correlated variables, with r=0.7\nsample_correlated = rand(MvNormal([0; 0], [1 0.7; 0.7 1]), 1000)\np2 = scatter(sample_correlated[1, :], sample_correlated[2, :], label=:false, title=L\"Correlated ($r=0.7$)\", tickfontsize=14, titlefontsize=18, guidefontsize=18)\nxlabel!(p2, L\"$x_1$\")\nylabel!(p2, L\"$x_2$\")\nplot!(p2, size=(400, 500))\n\n# sample 1000 anti-correlated variables, with r=-0.7\nsample_anticorrelated = rand(MvNormal([0; 0], [1 -0.7; -0.7 1]), 1000)\np3 = scatter(sample_anticorrelated[1, :], sample_anticorrelated[2, :], label=:false, title=L\"Anticorrelated ($r=-0.7$)\", tickfontsize=14, titlefontsize=18, guidefontsize=18)\nxlabel!(p3, L\"$x_1$\")\nylabel!(p3, L\"$x_2$\")\nplot!(p3, size=(400, 500))\n\ndisplay(p1)\ndisplay(p2)\ndisplay(p3)\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Independent Variables\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Correlated Variables (\\(r=0.7\\))\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Anti-Correlated Variables (\\(r=-0.7\\))\n\n\n\n\n\n\n\nFigure 6: Independent vs. Correlated Normal Variables"
  },
  {
    "objectID": "slides/lecture03-1.html#correlation-vs.-causation",
    "href": "slides/lecture03-1.html#correlation-vs.-causation",
    "title": "Exploratory Data Analysis, Correlations, and Autocorrelations",
    "section": "Correlation vs. Causation",
    "text": "Correlation vs. Causation\n\nXKCD 552\nSource: XKCD #552"
  },
  {
    "objectID": "slides/lecture03-1.html#correlation-does-not-imply-causation",
    "href": "slides/lecture03-1.html#correlation-does-not-imply-causation",
    "title": "Exploratory Data Analysis, Correlations, and Autocorrelations",
    "section": "“Correlation Does Not Imply Causation”",
    "text": "“Correlation Does Not Imply Causation”\n\nData can be correlated randomly (a spurious correlation)\nData can be correlated due to a mutual causal factor"
  },
  {
    "objectID": "slides/lecture03-1.html#spurious-correlations",
    "href": "slides/lecture03-1.html#spurious-correlations",
    "title": "Exploratory Data Analysis, Correlations, and Autocorrelations",
    "section": "Spurious Correlations",
    "text": "Spurious Correlations\n\nSpurious Correlation Example\nSource: Spurious Correlations #7602"
  },
  {
    "objectID": "slides/lecture03-1.html#correlated-data-likelihood",
    "href": "slides/lecture03-1.html#correlated-data-likelihood",
    "title": "Exploratory Data Analysis, Correlations, and Autocorrelations",
    "section": "Correlated Data Likelihood",
    "text": "Correlated Data Likelihood\n\nMarginal distributions normal: use a Multivariate Normal distribution\nOtherwise: Use copulas to “glue” marginal distributions together."
  },
  {
    "objectID": "slides/lecture03-1.html#autocorrelation",
    "href": "slides/lecture03-1.html#autocorrelation",
    "title": "Exploratory Data Analysis, Correlations, and Autocorrelations",
    "section": "Autocorrelation",
    "text": "Autocorrelation\nAn important concept for time series is autocorrelation between \\(y_t\\) and \\(y_{t-1}\\).\n\nCode\n# sample independent series from a given normal distribution\nsample_independent = rand(Normal(0, 1), 50)\np1 = plot(sample_independent, linewidth=3, ylabel=L\"$y$\", xlabel=L\"$t$\", title=\"Independent Series\", guidefontsize=18, legend=:false, tickfontsize=16, titlefontsize=18)\nplot!(p1, size=(500, 300))\n\n# sample an autocorrelated series\nsample_ar = zeros(50)\nsample_ar[1] = rand(Normal(0, 1 / sqrt(1-0.6^2)))\nfor i = 2:50\n    sample_ar[i] = 0.6 * sample_ar[i-1] + rand(Normal(0, 1))\nend\np2 = plot(sample_ar, linewidth=3, ylabel=L\"$y$\", xlabel=L\"$t$\", title=\"Autocorrelated Series\", guidefontsize=18, legend=:false, tickfontsize=16, titlefontsize=18)\nplot!(p2, size=(500, 300))\n\ndisplay(p1)\ndisplay(p2)\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Independent Variables\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Autocorrelated Variables\n\n\n\n\n\n\n\nFigure 7: Autocorrelated vs. Independent Samples"
  },
  {
    "objectID": "slides/lecture03-1.html#lagged-regression",
    "href": "slides/lecture03-1.html#lagged-regression",
    "title": "Exploratory Data Analysis, Correlations, and Autocorrelations",
    "section": "Lagged Regression",
    "text": "Lagged Regression\n\nCode\n# independent samples\ndat = DataFrame(Y=sample_independent[2:end], X=sample_independent[1:end-1])\nfit = lm(@formula(Y~X), dat)\npred = predict(fit,dat)\np1 = scatter(sample_independent[1:end-1], sample_independent[2:end], label=:false, title=\"Independent Variables: r=$(round(coef(fit)[2], digits=2))\", tickfontsize=16, titlefontsize=18, guidefontsize=18)\nplot!(p1, dat.X, pred, linewidth=2, label=:false)\nylabel!(p1, L\"$y_t$\")\nxlabel!(p1, L\"$y_{t-1}$\")\nplot!(p1, size=(600, 500))\n\n# autocorrelated\ndat = DataFrame(Y=sample_ar[2:end], X=sample_ar[1:end-1])\nfit = lm(@formula(Y~X), dat)\npred = predict(fit,dat)\np2 = scatter(sample_ar[1:end-1], sample_ar[2:end], label=:false, title=\"Autocorrelated Variables: r=$(round(coef(fit)[2], digits=2))\", tickfontsize=16, titlefontsize=18, guidefontsize=18)\nplot!(p2, dat.X, pred, linewidth=2, label=:false)\nylabel!(p2, L\"$y_t$\")\nxlabel!(p2, L\"$y_{t-1}$\")\nplot!(p2, size=(600, 500))\n\ndisplay(p1)\ndisplay(p2)\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Independent Variables\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Autocorrelated Variables\n\n\n\n\n\n\n\nFigure 8: Autocorrelated vs. Independent Samples"
  },
  {
    "objectID": "slides/lecture03-1.html#autocorrelation-function",
    "href": "slides/lecture03-1.html#autocorrelation-function",
    "title": "Exploratory Data Analysis, Correlations, and Autocorrelations",
    "section": "Autocorrelation Function",
    "text": "Autocorrelation Function\nMany modern programming languages have autocorrelation functions (e.g. StatsBase.autocor() in Julia) which calculates the autocorrelation at varying lags.\n\n\nautocor(sample_ar, 1:10)\n\n\n10-element Vector{Float64}:\n  0.5584861939778754\n  0.2715973028691899\n -0.024160973730345363\n -0.20302169868921682\n -0.27701812018221406\n -0.22360530073259474\n  0.0653064827991369\n  0.12749901066451022\n  0.16800165913435977\n  0.0607066386595902"
  },
  {
    "objectID": "slides/lecture03-1.html#autocorrelation-function-1",
    "href": "slides/lecture03-1.html#autocorrelation-function-1",
    "title": "Exploratory Data Analysis, Correlations, and Autocorrelations",
    "section": "Autocorrelation Function",
    "text": "Autocorrelation Function\nBut the autocorrelation function just computes the autocorrelations at every lag. Why is this a modeling problem?"
  },
  {
    "objectID": "slides/lecture03-1.html#partial-autocorrelation-function",
    "href": "slides/lecture03-1.html#partial-autocorrelation-function",
    "title": "Exploratory Data Analysis, Correlations, and Autocorrelations",
    "section": "Partial Autocorrelation Function",
    "text": "Partial Autocorrelation Function\nThis is solved by using the partial autocorrelation function (e.g. StatsBase.pacf()):\n\n\npacf(sample_ar, 1:10)\n\n\n10-element Vector{Float64}:\n  0.5610578719938641\n -0.07132505760984406\n -0.23567190666036678\n -0.13760028534427998\n -0.07852404120927237\n -0.03265094625098848\n  0.317641607455578\n -0.13655492164639593\n  0.002413001297937716\n -0.1569823772883165"
  },
  {
    "objectID": "slides/lecture03-1.html#autoregressive-models",
    "href": "slides/lecture03-1.html#autoregressive-models",
    "title": "Exploratory Data Analysis, Correlations, and Autocorrelations",
    "section": "Autoregressive Models",
    "text": "Autoregressive Models\nAn autoregressive-with-lag-\\(p\\) (AR(\\(p\\))) model: \\[y_t = \\sum_{i=1}^{p} \\rho_i y_{t-i} + \\varepsilon_t\\]\n\nExample: An AR(1) model: \\[\n\\begin{gather*}\ny_t = \\rho y_{t-1} + \\varepsilon_t \\\\\n\\varepsilon_t \\sim \\mathcal{N}(0, \\sigma)\n\\end{gather*}\n\\]"
  },
  {
    "objectID": "slides/lecture03-1.html#ar1-likelihood",
    "href": "slides/lecture03-1.html#ar1-likelihood",
    "title": "Exploratory Data Analysis, Correlations, and Autocorrelations",
    "section": "AR(1) Likelihood",
    "text": "AR(1) Likelihood\nThere are two ways to write down an AR(1) likelihood.\n\n“Whiten” the series:\n\n\\[\n\\begin{gather*}\n\\varepsilon_t = y_t - y_{t-1} \\sim \\mathcal{N}(0, \\sigma)\\\\\ny_1 \\sim \\mathcal{N}\\left(0, \\frac{\\sigma}{\\sqrt{1-\\rho^2}}\\right)\n\\end{gather*}\n\\]"
  },
  {
    "objectID": "slides/lecture03-1.html#ar1-likelihood-1",
    "href": "slides/lecture03-1.html#ar1-likelihood-1",
    "title": "Exploratory Data Analysis, Correlations, and Autocorrelations",
    "section": "AR(1) Likelihood",
    "text": "AR(1) Likelihood\n\nJoint likelihood:\n\n\\[\n\\mathbf{y} \\sim \\mathcal{N}(\\mathbf{0}, \\Sigma) \\qquad\n\\Sigma = \\begin{pmatrix}\\frac{\\sigma^2}{1-\\rho^2} & \\rho & \\rho^2 & \\ldots & \\rho^{T-1} \\\\ \\rho & \\frac{\\sigma^2}{1-\\rho^2} & \\rho & \\ldots & \\rho^{T-2} \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\rho^{T-1} & \\rho^{T-2} & \\rho^{T-3} & \\ldots & \\frac{\\sigma^2}{1-\\rho^2} \\end{pmatrix}\n\\]"
  },
  {
    "objectID": "slides/lecture03-1.html#model-data-discrepancy-1",
    "href": "slides/lecture03-1.html#model-data-discrepancy-1",
    "title": "Exploratory Data Analysis, Correlations, and Autocorrelations",
    "section": "Model-Data Discrepancy",
    "text": "Model-Data Discrepancy\nModel-data discrepancy: systematic disagreements between model and the modeled system state (Brynjarsdóttir & O’Hagan, 2014).\nLet \\(F(\\mathbf{x}; \\mathbf{\\theta})\\) be the simulation model:\n\n\\(\\mathbf{x}\\) are the “control variables”;\n\\(\\mathbf{\\theta}\\) are the “calibration variables”."
  },
  {
    "objectID": "slides/lecture03-1.html#model-data-discrepancy-2",
    "href": "slides/lecture03-1.html#model-data-discrepancy-2",
    "title": "Exploratory Data Analysis, Correlations, and Autocorrelations",
    "section": "Model-Data Discrepancy",
    "text": "Model-Data Discrepancy\nIf \\(\\mathbf{y}\\) are the “observations,”” we can model these as: \\[\\mathbf{y} = z(\\mathbf{x}) + \\mathbf{\\varepsilon},\\] where\n\n\\(z(\\mathbf{x})\\) is the “true” system state at control variable \\(\\mathbf{x}\\);\n\\(\\mathbf{\\varepsilon}\\) are observation errors."
  },
  {
    "objectID": "slides/lecture03-1.html#model-data-discrepancy-3",
    "href": "slides/lecture03-1.html#model-data-discrepancy-3",
    "title": "Exploratory Data Analysis, Correlations, and Autocorrelations",
    "section": "Model-Data Discrepancy",
    "text": "Model-Data Discrepancy\nThen the discrepancy \\(\\zeta\\) between the simulation and the modeled system is: \\[\\zeta(\\mathbf{x}; \\mathbf{\\theta}) = z(\\mathbf{x}) - F(\\mathbf{x}; \\mathbf{\\theta}).\\]\n\nThen observations can be written as:\n\\[\\mathbf{y} =  F(\\mathbf{x}; \\mathbf{\\theta}) + \\zeta(\\mathbf{x}; \\mathbf{\\theta}) + \\mathbf{\\varepsilon}.\\]"
  },
  {
    "objectID": "slides/lecture03-1.html#simple-discrepancy-example",
    "href": "slides/lecture03-1.html#simple-discrepancy-example",
    "title": "Exploratory Data Analysis, Correlations, and Autocorrelations",
    "section": "Simple Discrepancy Example",
    "text": "Simple Discrepancy Example\nCommon to model observation errors as normally-distributed: \\(\\varepsilon_t \\sim \\mathcal{N}(0, \\omega)\\)\nIf the discrepancy is also i.i.d. normal: \\(\\zeta_t \\sim \\mathcal{N}(0, \\sigma)\\).\nResiduals: \\[\\zeta_t + \\varepsilon_t \\sim \\mathcal{N}(0, \\sqrt{\\omega^2 + \\sigma^2})\\]"
  },
  {
    "objectID": "slides/lecture03-1.html#complex-discrepancy-example",
    "href": "slides/lecture03-1.html#complex-discrepancy-example",
    "title": "Exploratory Data Analysis, Correlations, and Autocorrelations",
    "section": "Complex Discrepancy Example",
    "text": "Complex Discrepancy Example\nNow suppose the discrepancy is AR(1).\n\\[\\begin{gather*}\n\\mathbf{\\zeta} + \\mathbf{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\Sigma) \\\\\n\\Sigma = \\begin{pmatrix}\\frac{\\sigma^2}{1-\\rho^2} + {\\color{red}\\omega^2} & \\rho & \\rho^2 & \\ldots & \\rho^{T-1} \\\\ \\rho & \\frac{\\sigma^2}{1-\\rho^2} + {\\color{red}\\omega^2} & \\rho & \\ldots & \\rho^{T-2} \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\rho^{T-1} & \\rho^{T-2} & \\rho^{T-3} & \\ldots & \\frac{\\sigma^2}{1-\\rho^2} + {\\color{red}\\omega^2} \\end{pmatrix}\n\\end{gather*}\n\\]"
  },
  {
    "objectID": "slides/lecture03-1.html#fitting-discrepancies",
    "href": "slides/lecture03-1.html#fitting-discrepancies",
    "title": "Exploratory Data Analysis, Correlations, and Autocorrelations",
    "section": "Fitting Discrepancies",
    "text": "Fitting Discrepancies\nIn many cases, separating discrepancy from observation error is tricky without prior information about variances.\nExample: \\[\\zeta_t + \\varepsilon_t \\sim \\mathcal{N}(0, \\sqrt{\\omega^2 + \\sigma^2})\\]\nNot being able to separate \\(\\omega\\) from \\(\\sigma\\) is a problem called non-identifiability."
  },
  {
    "objectID": "slides/lecture03-1.html#discrepancies-and-machine-learning",
    "href": "slides/lecture03-1.html#discrepancies-and-machine-learning",
    "title": "Exploratory Data Analysis, Correlations, and Autocorrelations",
    "section": "Discrepancies and Machine Learning",
    "text": "Discrepancies and Machine Learning\nCan use machine learning models to “emulate” complex discrepancies and error structures.\nMore on ML as an emulation/error tool later in the semester."
  },
  {
    "objectID": "slides/lecture03-1.html#key-points-eda",
    "href": "slides/lecture03-1.html#key-points-eda",
    "title": "Exploratory Data Analysis, Correlations, and Autocorrelations",
    "section": "Key Points: EDA",
    "text": "Key Points: EDA\n\nEDA helps identify a good or bad probability model;\nNo black-box workflow: make plots, compare samples from proposed model to data, try to be skeptical"
  },
  {
    "objectID": "slides/lecture03-1.html#key-points-correlation",
    "href": "slides/lecture03-1.html#key-points-correlation",
    "title": "Exploratory Data Analysis, Correlations, and Autocorrelations",
    "section": "Key Points: Correlation",
    "text": "Key Points: Correlation\n\nCheck for correlations, but think mechanistically.\nUse partial autocorrelation functions to find autoregressive model orders."
  },
  {
    "objectID": "slides/lecture03-1.html#key-points-discrepancy",
    "href": "slides/lecture03-1.html#key-points-discrepancy",
    "title": "Exploratory Data Analysis, Correlations, and Autocorrelations",
    "section": "Key Points: Discrepancy",
    "text": "Key Points: Discrepancy\n\nSeparate out “model bias” from observation errors.\nOften neglected: can be hard to fit without prior information on parameters.\nUse provided observation error variances when available to avoid non-identifiability."
  },
  {
    "objectID": "slides/lecture03-1.html#next-classes",
    "href": "slides/lecture03-1.html#next-classes",
    "title": "Exploratory Data Analysis, Correlations, and Autocorrelations",
    "section": "Next Class(es)",
    "text": "Next Class(es)\nWednesday: Bayesian Statistics and Prior Information"
  },
  {
    "objectID": "slides/lecture03-1.html#assessments",
    "href": "slides/lecture03-1.html#assessments",
    "title": "Exploratory Data Analysis, Correlations, and Autocorrelations",
    "section": "Assessments",
    "text": "Assessments\nFriday: Exercise 1 due by 9pm.\nHW2 assigned this week (will announce on Ed), due 2/23 by 9pm."
  },
  {
    "objectID": "slides/lecture03-1.html#references-1",
    "href": "slides/lecture03-1.html#references-1",
    "title": "Exploratory Data Analysis, Correlations, and Autocorrelations",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n\n\nBrynjarsdóttir, J., & O’Hagan, A. (2014). Learning about physical parameters: the importance of model discrepancy. Inverse Problems, 30, 114007. https://doi.org/10.1088/0266-5611/30/11/114007\n\n\nGelman, A. (2004). Exploratory Data Analysis for Complex Models. J. Comput. Graph. Stat., 13, 755–779. https://doi.org/10.1198/106186004X11435"
  },
  {
    "objectID": "slides/lecture03-2.html#model-data-discrepancy",
    "href": "slides/lecture03-2.html#model-data-discrepancy",
    "title": "Model-Data Discrepancy and Bayesian Statistics",
    "section": "Model-Data Discrepancy",
    "text": "Model-Data Discrepancy\nSystematic mismatch between model and system state (not observation error!)\n\\[\\mathbf{y} =  \\underbrace{F(\\mathbf{x}; \\mathbf{\\theta})}_\\text{simulation} + \\underbrace{\\zeta(\\mathbf{x}; \\mathbf{\\theta})}_\\text{discrepancy} + \\underbrace{\\mathbf{\\varepsilon}}_\\text{observation error}.\\]"
  },
  {
    "objectID": "slides/lecture03-2.html#non-identifiability",
    "href": "slides/lecture03-2.html#non-identifiability",
    "title": "Model-Data Discrepancy and Bayesian Statistics",
    "section": "Non-Identifiability",
    "text": "Non-Identifiability\nInability to distinguish between statistical parameters, e.g.\n\\[\\zeta_t + \\varepsilon_t \\sim \\mathcal{N}(0, \\sqrt{\\omega^2 + \\sigma^2})\\]"
  },
  {
    "objectID": "slides/lecture03-2.html#ebm-with-ar1-discrepancy",
    "href": "slides/lecture03-2.html#ebm-with-ar1-discrepancy",
    "title": "Model-Data Discrepancy and Bayesian Statistics",
    "section": "EBM With AR(1) Discrepancy",
    "text": "EBM With AR(1) Discrepancy\n\\[\n\\begin{gather*}\ny_t = \\text{EBM}(F_t; \\theta) + \\zeta_t + \\varepsilon_t \\\\\n\\zeta_t = \\rho \\zeta_{t-1} \\sim \\mathcal{N}(0, \\sigma)\\\\\n\\varepsilon_t \\sim \\mathcal{N}(0, \\omega_t)\n\\end{gather*}\n\\]\nwhere \\(\\omega_t\\) is estimated as half the 95% CI of the temperature data."
  },
  {
    "objectID": "slides/lecture03-2.html#covariance-matrix",
    "href": "slides/lecture03-2.html#covariance-matrix",
    "title": "Model-Data Discrepancy and Bayesian Statistics",
    "section": "Covariance Matrix",
    "text": "Covariance Matrix\nThe likelihood is \\(\\zeta + \\varepsilon \\sim N(\\mathbf{0}, \\Sigma)\\).\n\\[\n\\begin{gather*}\n\\Sigma = V + D \\\\\nD = \\text{diag}(\\omega_t^2), \\quad V = \\frac{\\sigma^2}{1-\\rho^2}\\begin{pmatrix}1 & \\rho & \\ldots & \\rho^{T-1} \\\\ \\rho & 1 & \\ldots & \\rho^{T-2} \\\\ \\rho^2 & \\rho & \\ldots & \\rho^{T-3} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\rho^{T-1} & \\rho^{T-2} & \\ldots & 1\\end{pmatrix}\n\\end{gather*}\n\\]"
  },
  {
    "objectID": "slides/lecture03-2.html#in-code",
    "href": "slides/lecture03-2.html#in-code",
    "title": "Model-Data Discrepancy and Bayesian Statistics",
    "section": "In Code…",
    "text": "In Code…\n\n# p are the model parameters, σ the standard deviation of the AR(1) errors, ρ is the autocorrelation coefficient, y is the data, m the model function\nfunction ar_covariance_mat(σ, ρ, y_err)\n    H = abs.((1:length(y_err)) .- (1:(length(y_err)))') # compute the outer product to get the correlation lags\n    ζ_var = σ^2 / (1-ρ^2)\n    Σ = ρ.^H * ζ_var\n    for i in 1:length(y_err)\n        Σ[i, i] += y_err[i]^2\n    end\n    return Σ\nend\n\nar_covariance_mat (generic function with 1 method)"
  },
  {
    "objectID": "slides/lecture03-2.html#maximize-likelihood",
    "href": "slides/lecture03-2.html#maximize-likelihood",
    "title": "Model-Data Discrepancy and Bayesian Statistics",
    "section": "Maximize Likelihood",
    "text": "Maximize Likelihood\n\nfunction ar_discrep_log_likelihood(p, σ, ρ, y, m, y_err)\n    y_pred = m(p)\n    residuals = y_pred .- y\n    Σ = ar_covariance_mat(σ, ρ, y_err)\n    ll = logpdf(MvNormal(zeros(length(y)), Σ), residuals)\n    return ll\nend\n\nebm_wrap(params) = ebm(forcing_non_aerosol_85[idx], forcing_aerosol_85[idx], p = params)\n\n# maximize log-likelihood within some range\n# important to make everything a Float instead of an Int \nlower = [1.0, 50.0, 0.0, 0.0, -1.0]\nupper = [4.0, 150.0, 2.0, 10.0, 1.0]\np0 = [2.0, 100.0, 1.0, 1.0, 0.0]\nresult = Optim.optimize(params -&gt; -ar_discrep_log_likelihood(params[1:end-2], params[end-1], params[end], temp_obs, ebm_wrap, temp_sd), lower, upper, p0)\nθ_discrep = result.minimizer"
  },
  {
    "objectID": "slides/lecture03-2.html#comparison-of-model-fits",
    "href": "slides/lecture03-2.html#comparison-of-model-fits",
    "title": "Model-Data Discrepancy and Bayesian Statistics",
    "section": "Comparison of Model Fits",
    "text": "Comparison of Model Fits\n\n\nNormal IID:\n\n\n4-element Vector{Float64}:\n  1.943746862749113\n 86.39077197089858\n  0.789333067313891\n  0.12104289634011653\n\n\n\nTotal-Residual AR(1):\n\n\n5-element Vector{Float64}:\n   2.0068089111198297\n 109.38026355047445\n   0.7915448981319909\n   0.10196962494870278\n   0.5421594991244063\n\n\n\nDiscrepancy AR(1):\n\n\n5-element Vector{Float64}:\n  1.9063093043619712\n 88.29298073898423\n  0.7492605984867408\n  0.07978677011193558\n  0.5379510507012369"
  },
  {
    "objectID": "slides/lecture03-2.html#hindcast",
    "href": "slides/lecture03-2.html#hindcast",
    "title": "Model-Data Discrepancy and Bayesian Statistics",
    "section": "Hindcast",
    "text": "Hindcast\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Hindcasts from different EBM fits."
  },
  {
    "objectID": "slides/lecture03-2.html#projections-rcp-2.6",
    "href": "slides/lecture03-2.html#projections-rcp-2.6",
    "title": "Model-Data Discrepancy and Bayesian Statistics",
    "section": "Projections (RCP 2.6)",
    "text": "Projections (RCP 2.6)\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Projections from different EBM fits."
  },
  {
    "objectID": "slides/lecture03-2.html#prior-information",
    "href": "slides/lecture03-2.html#prior-information",
    "title": "Model-Data Discrepancy and Bayesian Statistics",
    "section": "Prior Information",
    "text": "Prior Information\nSo far: no way to use prior information about parameters (other than bounds on MLE optimization)."
  },
  {
    "objectID": "slides/lecture03-2.html#bayes-rule",
    "href": "slides/lecture03-2.html#bayes-rule",
    "title": "Model-Data Discrepancy and Bayesian Statistics",
    "section": "Bayes’ Rule",
    "text": "Bayes’ Rule\nOriginal version (Bayes, 1763):\n\\[P(A | B) = \\frac{P(B | A) \\times P(A)}{P(B)} \\quad \\text{if} \\quad P(B) \\neq 0.\\]"
  },
  {
    "objectID": "slides/lecture03-2.html#bayes-rule-1",
    "href": "slides/lecture03-2.html#bayes-rule-1",
    "title": "Model-Data Discrepancy and Bayesian Statistics",
    "section": "Bayes’ Rule",
    "text": "Bayes’ Rule\n“Modern” version (Laplace, 1774):\n\\[p(\\theta | y) = \\frac{p(y | \\theta)}{p(y)} p(\\theta)\\]"
  },
  {
    "objectID": "slides/lecture03-2.html#bayes-rule-2",
    "href": "slides/lecture03-2.html#bayes-rule-2",
    "title": "Model-Data Discrepancy and Bayesian Statistics",
    "section": "Bayes’ Rule",
    "text": "Bayes’ Rule\n“Modern” version (Laplace, 1774):\n\\[\\underbrace{{p(\\theta | y)}}_{\\text{posterior}} = \\frac{\\overbrace{p(y | \\theta)}^{\\text{likelihood}}}{\\underbrace{p(y)}_\\text{normalization}} \\overbrace{p(\\theta)}^\\text{prior}\\]"
  },
  {
    "objectID": "slides/lecture03-2.html#on-the-normalizing-constant",
    "href": "slides/lecture03-2.html#on-the-normalizing-constant",
    "title": "Model-Data Discrepancy and Bayesian Statistics",
    "section": "On The Normalizing Constant",
    "text": "On The Normalizing Constant\nThe normalizing constant (also called the marginal likelihood) is the integral \\[p(y) = \\int_\\Theta p(y | \\theta) p(\\theta) d\\theta.\\]\nSince this generally doesn’t depend on \\(\\theta\\), it can often be ignored, as the relative probabilities don’t change."
  },
  {
    "objectID": "slides/lecture03-2.html#bayes-rule-ignoring-normalizing-constants",
    "href": "slides/lecture03-2.html#bayes-rule-ignoring-normalizing-constants",
    "title": "Model-Data Discrepancy and Bayesian Statistics",
    "section": "Bayes’ Rule (Ignoring Normalizing Constants)",
    "text": "Bayes’ Rule (Ignoring Normalizing Constants)\nThe version of Bayes’ rule which matters the most for 95% (approximate) of Bayesian statistics:\n\\[p(\\theta | y) \\propto p(y | \\theta) \\times p(\\theta)\\]\n\n“The posterior is the prior times the likelihood…”"
  },
  {
    "objectID": "slides/lecture03-2.html#how-to-choose-a-prior",
    "href": "slides/lecture03-2.html#how-to-choose-a-prior",
    "title": "Model-Data Discrepancy and Bayesian Statistics",
    "section": "How To Choose A Prior?",
    "text": "How To Choose A Prior?\nOne perspective: Priors should reflect “actual knowledge” independent of the analysis (Jaynes, 2003)\nAnother: Priors are part of the probability model, and can be specified/changed accordingly based on predictive skill (Gelman et al., 2017; Gelman & Shalizi, 2013)"
  },
  {
    "objectID": "slides/lecture03-2.html#what-makes-a-good-prior",
    "href": "slides/lecture03-2.html#what-makes-a-good-prior",
    "title": "Model-Data Discrepancy and Bayesian Statistics",
    "section": "What Makes A Good Prior?",
    "text": "What Makes A Good Prior?\n\nReflects level of understanding (informative vs. weakly informative vs. non-informative).\nDoes not zero out probability of plausible values.\nRegularization (extreme values should be less probable)"
  },
  {
    "objectID": "slides/lecture03-2.html#what-makes-a-bad-prior",
    "href": "slides/lecture03-2.html#what-makes-a-bad-prior",
    "title": "Model-Data Discrepancy and Bayesian Statistics",
    "section": "What Makes A Bad Prior?",
    "text": "What Makes A Bad Prior?\n\nAssigns probability zero to plausible values;\nWeights implausible values equally as more plausible ones;\nDouble counts information (e.g. fitting a prior to data which is also used in the likelihood)\nChosen based on vibes."
  },
  {
    "objectID": "slides/lecture03-2.html#a-coin-flipping-example",
    "href": "slides/lecture03-2.html#a-coin-flipping-example",
    "title": "Model-Data Discrepancy and Bayesian Statistics",
    "section": "A Coin Flipping Example",
    "text": "A Coin Flipping Example\nWe would like to understand if a coin-flipping game is fair. We’ve observed the following sequence of flips:\n\nflips = [\"H\", \"H\", \"H\", \"T\", \"H\", \"H\", \"H\", \"H\", \"H\"]\n\n9-element Vector{String}:\n \"H\"\n \"H\"\n \"H\"\n \"T\"\n \"H\"\n \"H\"\n \"H\"\n \"H\"\n \"H\""
  },
  {
    "objectID": "slides/lecture03-2.html#coin-flipping-likelihood",
    "href": "slides/lecture03-2.html#coin-flipping-likelihood",
    "title": "Model-Data Discrepancy and Bayesian Statistics",
    "section": "Coin Flipping Likelihood",
    "text": "Coin Flipping Likelihood\nThe data-generating process here is straightforward: we can represent a coin flip with a heads-probability of \\(\\theta\\) as a sample from a Bernoulli distribution,\n\\[y_i \\sim \\text{Bernoulli}(\\theta).\\]\n\nflip_ll(θ) = sum(logpdf(Bernoulli(θ), flips .== \"H\"))\nθ_mle = Optim.optimize(θ -&gt; -flip_ll(θ), 0, 1).minimizer\nround(θ_mle, digits=2)\n\n0.89"
  },
  {
    "objectID": "slides/lecture03-2.html#coin-flipping-prior",
    "href": "slides/lecture03-2.html#coin-flipping-prior",
    "title": "Model-Data Discrepancy and Bayesian Statistics",
    "section": "Coin Flipping Prior",
    "text": "Coin Flipping Prior\nSuppose that we spoke to a friend who knows something about coins, and she tells us that it is extremely difficult to make a passable weighted coin which comes up heads more than 75% of the time."
  },
  {
    "objectID": "slides/lecture03-2.html#coin-flipping-prior-1",
    "href": "slides/lecture03-2.html#coin-flipping-prior-1",
    "title": "Model-Data Discrepancy and Bayesian Statistics",
    "section": "Coin Flipping Prior",
    "text": "Coin Flipping Prior\n\n\nSince \\(\\theta\\) is bounded between 0 and 1, we’ll use a Beta distribution for our prior, specifically \\(\\text{Beta}(4,4)\\).\n\n\n\nCode\nprior_dist = Beta(5, 5)\nplot(prior_dist; label=false, xlabel=L\"$θ$\", ylabel=L\"$p(θ)$\", linewidth=3, tickfontsize=16, guidefontsize=18)\nplot!(size=(500, 500))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Beta prior for coin flipping example"
  },
  {
    "objectID": "slides/lecture03-2.html#maximum-a-posteriori-estimate",
    "href": "slides/lecture03-2.html#maximum-a-posteriori-estimate",
    "title": "Model-Data Discrepancy and Bayesian Statistics",
    "section": "Maximum A Posteriori Estimate",
    "text": "Maximum A Posteriori Estimate\nCombining using Bayes’ rule lets us calculate the maximum a posteriori (MAP) estimate:\n\nflip_ll(θ) = sum(logpdf(Bernoulli(θ), flips .== \"H\"))\nflip_lprior(θ) = logpdf(Beta(5, 5), θ)\nflip_lposterior(θ) = flip_ll(θ) + flip_lprior(θ)\nθ_map = Optim.optimize(θ -&gt; -(flip_lposterior(θ)), 0, 1).minimizer\nround(θ_map, digits=2)\n\n0.71"
  },
  {
    "objectID": "slides/lecture03-2.html#coin-flipping-posterior-distribution",
    "href": "slides/lecture03-2.html#coin-flipping-posterior-distribution",
    "title": "Model-Data Discrepancy and Bayesian Statistics",
    "section": "Coin Flipping Posterior Distribution",
    "text": "Coin Flipping Posterior Distribution\n\n\nCode\nθ_range = 0:0.01:1\nplot(θ_range, flip_lposterior.(θ_range), color=:black, label=\"Posterior\", linewidth=3, tickfontsize=16, legendfontsize=16, guidefontsize=18, bottom_margin=5mm, left_margin=5mm)\nvline!([θ_map], color=:red, label=\"MAP\", linewidth=2)\nvline!([θ_mle], color=:blue, label=\"MLE\", linewidth=2)\nxlabel!(L\"$\\theta$\")\nylabel!(\"Posterior Density\")\nplot!(size=(1000, 450))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Posterior distribution for the coin-flipping example"
  },
  {
    "objectID": "slides/lecture03-2.html#bayes-and-parametric-uncertainty",
    "href": "slides/lecture03-2.html#bayes-and-parametric-uncertainty",
    "title": "Model-Data Discrepancy and Bayesian Statistics",
    "section": "Bayes and Parametric Uncertainty",
    "text": "Bayes and Parametric Uncertainty\nFrequentist: Parametric uncertainty is purely the result of sampling variability\nBayesian: Parameters have probabilities based on consistency with data and priors."
  },
  {
    "objectID": "slides/lecture03-2.html#bayesian-updating",
    "href": "slides/lecture03-2.html#bayesian-updating",
    "title": "Model-Data Discrepancy and Bayesian Statistics",
    "section": "Bayesian Updating",
    "text": "Bayesian Updating\n\nThe posterior is a “compromise” between the prior and the data.\nThe posterior mean is a weighted combination of the data and the prior mean.\nThe weights depend on the prior and the likelihood variances.\nMore data usually makes the posterior more confident."
  },
  {
    "objectID": "slides/lecture03-2.html#san-francisco-tide-gauge-data",
    "href": "slides/lecture03-2.html#san-francisco-tide-gauge-data",
    "title": "Model-Data Discrepancy and Bayesian Statistics",
    "section": "San Francisco Tide Gauge Data",
    "text": "San Francisco Tide Gauge Data\n\n\nCode\n# read in data and get annual maxima\nfunction load_data(fname)\n    date_format = DateFormat(\"yyyy-mm-dd HH:MM:SS\")\n    # This uses the DataFramesMeta.jl package, which makes it easy to string together commands to load and process data\n    df = @chain fname begin\n        CSV.read(DataFrame; header=false)\n        rename(\"Column1\" =&gt; \"year\", \"Column2\" =&gt; \"month\", \"Column3\" =&gt; \"day\", \"Column4\" =&gt; \"hour\", \"Column5\" =&gt; \"gauge\")\n        # need to reformat the decimal date in the data file\n        @transform :datetime = DateTime.(:year, :month, :day, :hour)\n        # replace -99999 with missing\n        @transform :gauge = ifelse.(abs.(:gauge) .&gt;= 9999, missing, :gauge)\n        select(:datetime, :gauge)\n    end\n    return df\nend\n\ndat = load_data(\"data/surge/h551.csv\")\n\n# detrend the data to remove the effects of sea-level rise and seasonal dynamics\nma_length = 366\nma_offset = Int(floor(ma_length/2))\nmoving_average(series,n) = [mean(@view series[i-n:i+n]) for i in n+1:length(series)-n]\ndat_ma = DataFrame(datetime=dat.datetime[ma_offset+1:end-ma_offset], residual=dat.gauge[ma_offset+1:end-ma_offset] .- moving_average(dat.gauge, ma_offset))\n\n# group data by year and compute the annual maxima\ndat_ma = dropmissing(dat_ma) # drop missing data\ndat_annmax = combine(dat_ma -&gt; dat_ma[argmax(dat_ma.residual), :], groupby(transform(dat_ma, :datetime =&gt; x-&gt;year.(x)), :datetime_function))\ndelete!(dat_annmax, nrow(dat_annmax)) # delete 2023; haven't seen much of that year yet\nrename!(dat_annmax, :datetime_function =&gt; :Year)\nselect!(dat_annmax, [:Year, :residual])\ndat_annmax.residual = dat_annmax.residual / 1000 # convert to m\n\n# make plots\np1 = plot(\n    dat_annmax.Year,\n    dat_annmax.residual;\n    xlabel=\"Year\",\n    ylabel=\"Annual Max Tide Level (m)\",\n    label=false,\n    marker=:circle,\n    markersize=5,\n    tickfontsize=16,\n    guidefontsize=18\n)\np2 = histogram(\n    dat_annmax.residual,\n    normalize=:pdf,\n    orientation=:horizontal,\n    label=:false,\n    xlabel=\"PDF\",\n    ylabel=\"\",\n    yticks=[],\n    tickfontsize=16,\n    guidefontsize=18\n)\n\nl = @layout [a{0.7w} b{0.3w}]\nplot(p1, p2; layout=l, link=:y, ylims=(1, 1.7), bottom_margin=5mm, left_margin=5mm)\nplot!(size=(1000, 450))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Annual maxima surge data from the San Francisco, CA tide gauge."
  },
  {
    "objectID": "slides/lecture03-2.html#proposed-probability-model",
    "href": "slides/lecture03-2.html#proposed-probability-model",
    "title": "Model-Data Discrepancy and Bayesian Statistics",
    "section": "Proposed Probability Model",
    "text": "Proposed Probability Model\n\\[\n\\begin{align*}\n& y \\sim LogNormal(\\mu, \\sigma) \\tag{likelihood}\\\\\n& \\left. \\begin{aligned}\n& \\mu \\sim Normal(0, 1) \\\\\n& \\sigma \\sim HalfNormal(0, 5)\n\\end{aligned} \\right\\} \\tag{priors}\n\\end{align*}\n\\]\nWant to find:\n\\[p(\\mu, \\sigma | y) \\propto p(y | \\mu, \\sigma) p(\\mu)p(\\sigma)\\]"
  },
  {
    "objectID": "slides/lecture03-2.html#are-our-priors-reasonable",
    "href": "slides/lecture03-2.html#are-our-priors-reasonable",
    "title": "Model-Data Discrepancy and Bayesian Statistics",
    "section": "Are Our Priors Reasonable?",
    "text": "Are Our Priors Reasonable?\nHard to tell! Let’s simulate data to see we get plausible outcomes.\nThis is called a prior predictive check."
  },
  {
    "objectID": "slides/lecture03-2.html#prior-predictive-check",
    "href": "slides/lecture03-2.html#prior-predictive-check",
    "title": "Model-Data Discrepancy and Bayesian Statistics",
    "section": "Prior Predictive Check",
    "text": "Prior Predictive Check\n\n\nCode\n# sample from priors\nμ_sample = rand(Normal(0, 1), 1_000)\nσ_sample = rand(truncated(Normal(0, 5), 0, +Inf), 1_000)\n\n# define return periods and cmopute return levels for parameters\nreturn_periods = 2:100\nreturn_levels = zeros(1_000, length(return_periods))\nfor i in 1:1_000\n    return_levels[i, :] = quantile.(LogNormal(μ_sample[i], σ_sample[i]), 1 .- (1 ./ return_periods))\nend\n\nplt_prior_1 = plot(; yscale=:log10, yticks=10 .^ collect(0:2:16), ylabel=\"Return Level (m)\", xlabel=\"Return Period (yrs)\",\n    tickfontsize=16, legendfontsize=18, guidefontsize=18, bottom_margin=10mm, left_margin=10mm, legend=:topleft)\nfor idx in 1:1_000\n    label = idx == 1 ? \"Prior\" : false\n    plot!(plt_prior_1, return_periods, return_levels[idx, :]; color=:black, alpha=0.1, label=label)\nend\nplt_prior_1\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Prior predictive check of return periods with revised model"
  },
  {
    "objectID": "slides/lecture03-2.html#lets-revise-the-prior",
    "href": "slides/lecture03-2.html#lets-revise-the-prior",
    "title": "Model-Data Discrepancy and Bayesian Statistics",
    "section": "Let’s Revise the Prior",
    "text": "Let’s Revise the Prior\n\\[\n\\begin{align*}\n& y \\sim LogNormal(\\mu, \\sigma) \\tag{likelihood}\\\\\n& \\left. \\begin{aligned}\n& \\mu \\sim Normal(0, 0.5) \\\\\n& \\sigma \\sim HalfNormal(0, 0.1)\n\\end{aligned} \\right\\} \\tag{priors}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/lecture03-2.html#prior-predictive-check-2",
    "href": "slides/lecture03-2.html#prior-predictive-check-2",
    "title": "Model-Data Discrepancy and Bayesian Statistics",
    "section": "Prior Predictive Check 2",
    "text": "Prior Predictive Check 2\n\n\nCode\n# sample from priors\nμ_sample = rand(Normal(0, 0.5), 1_000)\nσ_sample = rand(truncated(Normal(0, 0.1), 0, +Inf), 1_000)\n\nreturn_periods = 2:100\nreturn_levels = zeros(1_000, length(return_periods))\nfor i in 1:1_000\n    return_levels[i, :] = quantile.(LogNormal(μ_sample[i], σ_sample[i]), 1 .- (1 ./ return_periods))\nend\n\nplt_prior_2 = plot(; ylabel=\"Return Level (m)\", xlabel=\"Return Period (yrs)\", tickfontsize=16, legendfontsize=18, guidefontsize=18, bottom_margin=10mm, left_margin=10mm)\nfor idx in 1:1_000\n    label = idx == 1 ? \"Prior\" : false\n    plot!(plt_prior_2, return_periods, return_levels[idx, :]; color=:black, alpha=0.1, label=label)\nend\nplt_prior_2\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: Prior predictive check of return periods with revised model"
  },
  {
    "objectID": "slides/lecture03-2.html#compute-posterior",
    "href": "slides/lecture03-2.html#compute-posterior",
    "title": "Model-Data Discrepancy and Bayesian Statistics",
    "section": "Compute Posterior",
    "text": "Compute Posterior\n\n\n\n\nCode\nll(μ, σ) = sum(logpdf(LogNormal(μ, σ), dat_annmax.residual))\nlprior(μ, σ) = logpdf(Normal(0, 0.5), μ) + logpdf(truncated(Normal(0, 0.1), 0, Inf), σ)\nlposterior(μ, σ) = ll(μ, σ) + lprior(μ, σ)\n\np_map = optimize(p -&gt; -lposterior(p[1], p[2]), [0.0, 0.0], [1.0, 1.0], [0.5, 0.5]).minimizer\n\nμ = 0.15:0.005:0.35\nσ = 0.04:0.01:0.1\nposterior_vals = @. lposterior(μ', σ)\n\ncontour(μ, σ, posterior_vals, \n    levels=100, \n    clabels=false, \n    cbar=true, lw=1, \n    fill=(true,cgrad(:grays,[0,0.1,1.0])),\n    tickfontsize=16,\n    guidefontsize=18,\n    legendfontsize=18,\n    right_margin=20mm,\n    bottom_margin=5mm,\n    left_margin=5mm\n)\nscatter!([p_map[1]], [p_map[2]], label=\"MAP\", markersize=10, marker=:star)\nxlabel!(L\"$\\mu$\")\nylabel!(L\"$\\sigma$\")\nplot!(size=(900, 400))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8: Posterior samples from surge model.\n\n\n\n\n\n\n\n2-element Vector{Float64}:\n 0.2546874075930782\n 0.055422136861588964"
  },
  {
    "objectID": "slides/lecture03-2.html#assess-map-fit",
    "href": "slides/lecture03-2.html#assess-map-fit",
    "title": "Model-Data Discrepancy and Bayesian Statistics",
    "section": "Assess MAP Fit",
    "text": "Assess MAP Fit\n\nCode\np1 = histogram(\n    dat_annmax.residual,\n    normalize=:pdf,\n    legend=:false,\n    ylabel=\"PDF\",\n    xlabel=\"Annual Max Tide Level (m)\",\n    tickfontsize=16,\n    guidefontsize=18,\n    bottom_margin=5mm, left_margin=5mm\n)\nplot!(p1, LogNormal(p_map[1], p_map[2]),\n    linewidth=3,\n    color=:red)\nxlims!(p1, (1, 1.7))\nplot!(p1, size=(600, 450))\n\nreturn_periods = 2:500\nreturn_levels = quantile.(LogNormal(p_map[1], p_map[2]), 1 .- (1 ./ return_periods))\n\n# function to calculate exceedance probability and plot positions based on data quantile\nfunction exceedance_plot_pos(y)\n    N = length(y)\n    ys = sort(y; rev=false) # sorted values of y\n    nxp = xp = [r / (N + 1) for r in 1:N] # exceedance probability\n    xp = 1 .- nxp\n    return xp, ys\nend\nxp, ys = exceedance_plot_pos(dat_annmax.residual)\n\np2 = plot(return_periods, return_levels, linewidth=3, color=:blue, label=\"Model Fit\", tickfontsize=16, legendfontsize=18, guidefontsize=18, bottom_margin=5mm, left_margin=5mm, right_margin=10mm, legend=:bottomright)\nscatter!(p2, 1 ./ xp, ys, label=\"Observations\", color=:black, markersize=5)\nxlabel!(p2, \"Return Period (yrs)\")\nylabel!(p2, \"Return Level (m)\")\nxlims!(-1, 300)\nplot!(p2, size=(600, 450))\n\ndisplay(p1)\ndisplay(p2)\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Checks for model fit.\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure 9"
  },
  {
    "objectID": "slides/lecture03-2.html#what-about-the-posterior-distribution",
    "href": "slides/lecture03-2.html#what-about-the-posterior-distribution",
    "title": "Model-Data Discrepancy and Bayesian Statistics",
    "section": "What About The Posterior Distribution?",
    "text": "What About The Posterior Distribution?\nOne of the points of Bayesian statistics is we get a distribution over parameters."
  },
  {
    "objectID": "slides/lecture03-2.html#conjugate-priors",
    "href": "slides/lecture03-2.html#conjugate-priors",
    "title": "Model-Data Discrepancy and Bayesian Statistics",
    "section": "Conjugate Priors",
    "text": "Conjugate Priors\nWhen the mathematical forms of the likelihood and the prior(s) are conjugate, the posterior is a nice closed-form distribution.\nExamples:\n\nNormal \\(p(y | \\mu)\\), Normal \\(p(\\mu)\\) ⇒ Normal \\(p(\\mu | y)\\)\nBinomial \\(p(y | \\theta)\\), Beta \\((p\\theta)\\), ⇒ Beta \\(p(\\theta | y)\\)"
  },
  {
    "objectID": "slides/lecture03-2.html#but-in-general",
    "href": "slides/lecture03-2.html#but-in-general",
    "title": "Model-Data Discrepancy and Bayesian Statistics",
    "section": "But In General…",
    "text": "But In General…\nConjugate priors are often convenient, but may be poor choices.\nWe will talk about how to sample more generally with Monte Carlo later."
  },
  {
    "objectID": "slides/lecture03-2.html#key-points-discrepancy",
    "href": "slides/lecture03-2.html#key-points-discrepancy",
    "title": "Model-Data Discrepancy and Bayesian Statistics",
    "section": "Key Points: Discrepancy",
    "text": "Key Points: Discrepancy\n\nModeling discrepancy can separate system state-relevant errors from observation errors.\nFurther decomposition of data generating process.\nWhen hindcasting, include observation errors; do not when projecting!"
  },
  {
    "objectID": "slides/lecture03-2.html#key-points-bayesian-statistics",
    "href": "slides/lecture03-2.html#key-points-bayesian-statistics",
    "title": "Model-Data Discrepancy and Bayesian Statistics",
    "section": "Key Points: Bayesian Statistics",
    "text": "Key Points: Bayesian Statistics\n\nBayesian probability: parameters have probabilities conditional on data\nNeed to specify prior distribution (think generatively!).\nBe transparent and principled about prior choices (sensitivity analyses?).\nMaximum a posteriori gives “most probable” parameter values\nWill talk more about general sampling later."
  },
  {
    "objectID": "slides/lecture03-2.html#next-classes",
    "href": "slides/lecture03-2.html#next-classes",
    "title": "Model-Data Discrepancy and Bayesian Statistics",
    "section": "Next Class(es)",
    "text": "Next Class(es)\nMonday: Extreme Value Theory and Models\nWednesday: No class! Develop project proposals."
  },
  {
    "objectID": "slides/lecture03-2.html#assessments",
    "href": "slides/lecture03-2.html#assessments",
    "title": "Model-Data Discrepancy and Bayesian Statistics",
    "section": "Assessments",
    "text": "Assessments\nFriday: Exercise 1 due by 9pm.\nHW2 available! Due 2/23 by 9pm."
  },
  {
    "objectID": "slides/lecture03-2.html#references-1",
    "href": "slides/lecture03-2.html#references-1",
    "title": "Model-Data Discrepancy and Bayesian Statistics",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n\n\nBayes, T. (1763). An Essay towards Solving a Problem in the Doctrine of Chance. Philosophical Transactions of the Royal Society of London, 53, 370–418.\n\n\nGelman, A., & Shalizi, C. R. (2013). Philosophy and the practice of Bayesian statistics. Br. J. Math. Stat. Psychol., 66, 8–38. https://doi.org/10.1111/j.2044-8317.2011.02037.x\n\n\nGelman, A., Simpson, D., & Betancourt, M. (2017). The prior can often only be understood in the context of the likelihood. Entropy, 19, 555. https://doi.org/10.3390/e19100555\n\n\nJaynes, E. T. (2003). Probability theory: the Logic of Science. (G. L. Bretthorst, Ed.). Cambridge, UK ; New York, NY: Cambridge University Press. Retrieved from https://market.android.com/details?id=book-tTN4HuUNXjgC\n\n\nLaplace, P. S. (1774). Mémoire sur la Probabilité des Causes par les évènemens. In Mémoires de Mathematique et de Physique, Presentés à l’Académie Royale des Sciences, Par Divers Savans & Lus Dans ses Assemblées (pp. 621–656)."
  },
  {
    "objectID": "slides/lecture14-1.html#what-is-the-goal-of-model-selection",
    "href": "slides/lecture14-1.html#what-is-the-goal-of-model-selection",
    "title": "Model Complexity and Emulation",
    "section": "What Is The Goal of Model Selection?",
    "text": "What Is The Goal of Model Selection?\nKey Idea: Model selection consists of navigating the bias-variance tradeoff."
  },
  {
    "objectID": "slides/lecture14-1.html#bias-and-variance",
    "href": "slides/lecture14-1.html#bias-and-variance",
    "title": "Model Complexity and Emulation",
    "section": "Bias and Variance",
    "text": "Bias and Variance\nModel error (e.g. MSE) is a combination of irreducible error, bias, and variance.\n\nBias can come from under-dispersion (too little complexity) or neglected processes;\nVariance can come from over-dispersion (too much complexity) or poor identifiability."
  },
  {
    "objectID": "slides/lecture14-1.html#cross-validation",
    "href": "slides/lecture14-1.html#cross-validation",
    "title": "Model Complexity and Emulation",
    "section": "Cross-Validation",
    "text": "Cross-Validation\nCross-validation is the gold standard for predictive accuracy: how well does the fitted model predict out of sample data?"
  },
  {
    "objectID": "slides/lecture14-1.html#cross-validation-1",
    "href": "slides/lecture14-1.html#cross-validation-1",
    "title": "Model Complexity and Emulation",
    "section": "Cross-Validation",
    "text": "Cross-Validation\nThe problems:\n\nLeave-one-out CV can be very computationally expensive!\nWe often don’t have a lot of data for calibration, so holding some back can be a problem.\nHow to divide data with spatial or temporal structure? This can be addressed by partitioning the data more cleverly (e.g. leaving out future observations), but makes the data problem worse."
  },
  {
    "objectID": "slides/lecture14-1.html#information-criteria",
    "href": "slides/lecture14-1.html#information-criteria",
    "title": "Model Complexity and Emulation",
    "section": "Information Criteria",
    "text": "Information Criteria\nApproximate LOO-CV by computing fit on calibration/training data and “correcting” for expected overfitting.\nExamples (differ in the parameter values used and correction factor(s)):\n\nAIC\nDIC\nWAIC"
  },
  {
    "objectID": "slides/lecture14-1.html#model-weighting",
    "href": "slides/lecture14-1.html#model-weighting",
    "title": "Model Complexity and Emulation",
    "section": "Model Weighting",
    "text": "Model Weighting\nInformation Criteria can be used to get averaging weights across a model set \\(\\mathcal{M} = \\{M_1, \\ldots, M_m\\}\\):\n\\[w_i = \\frac{\\exp(-\\Delta_i/2)}{\\sum_{m=1}^M \\exp(-\\Delta_m/2)}.\\]"
  },
  {
    "objectID": "slides/lecture14-1.html#bayesian-loo-cv-advanced-methods",
    "href": "slides/lecture14-1.html#bayesian-loo-cv-advanced-methods",
    "title": "Model Complexity and Emulation",
    "section": "Bayesian LOO-CV: Advanced Methods",
    "text": "Bayesian LOO-CV: Advanced Methods\nThere are approximations to Leave-One-Out Cross-Validation which use importance sampling to avoid this, and these can be extended to time series.\nSee:\n\nVehtari et al. (2017) on approximations to LOO-CV;\nBürkner et al. (2020) on “leave-future-out” time series CV;\nYao et al. (2018) on modeling stacking for averaging."
  },
  {
    "objectID": "slides/lecture14-1.html#parsimony-as-a-modeling-virtue",
    "href": "slides/lecture14-1.html#parsimony-as-a-modeling-virtue",
    "title": "Model Complexity and Emulation",
    "section": "Parsimony as A Modeling Virtue",
    "text": "Parsimony as A Modeling Virtue\nParsimony (fewer model terms/components) can reduce the chance of overfitting and increased variance, all else being equal.\nModel simplicity has another advantange: simpler models are less computationally expensive."
  },
  {
    "objectID": "slides/lecture14-1.html#computational-budgets",
    "href": "slides/lecture14-1.html#computational-budgets",
    "title": "Model Complexity and Emulation",
    "section": "Computational Budgets",
    "text": "Computational Budgets\n\n\n\n\nComputational Budgets\n\n\n\nSource: Helgeson et al. (2021)"
  },
  {
    "objectID": "slides/lecture14-1.html#benefits-of-model-simplicity",
    "href": "slides/lecture14-1.html#benefits-of-model-simplicity",
    "title": "Model Complexity and Emulation",
    "section": "Benefits of Model Simplicity",
    "text": "Benefits of Model Simplicity\n\n\n\nMore thorough representation of uncertainties\nCan focus on “important” characteristics for problem at hand\nPotential increase in generalizability\n\n\n\n\n\nComputational Complexity\n\n\n\nSource: Helgeson et al. (2021)"
  },
  {
    "objectID": "slides/lecture14-1.html#downsides-of-model-simplicity",
    "href": "slides/lecture14-1.html#downsides-of-model-simplicity",
    "title": "Model Complexity and Emulation",
    "section": "Downsides of Model Simplicity",
    "text": "Downsides of Model Simplicity\n\nPotential loss of salience\nMay miss important dynamics (creating bias)\nParameter/dynamical compensation can result in loss of interpretability"
  },
  {
    "objectID": "slides/lecture14-1.html#simplicity-tradeoffs",
    "href": "slides/lecture14-1.html#simplicity-tradeoffs",
    "title": "Model Complexity and Emulation",
    "section": "Simplicity Tradeoffs",
    "text": "Simplicity Tradeoffs\nSimple models can be epistemically and practically valuable.\nBut:\nNeed to carefully select which processes/parameters are included in the simplified representation, and at what resolution."
  },
  {
    "objectID": "slides/lecture14-1.html#approximating-complex-models",
    "href": "slides/lecture14-1.html#approximating-complex-models",
    "title": "Model Complexity and Emulation",
    "section": "Approximating Complex Models",
    "text": "Approximating Complex Models\nChallenge: How do we simplify complex models to keep key dynamics but reduce computational expense?\n\nApproximate (or emulate) the model response surface.\n\nEvaluate original model at an ensemble of points (design of experiment)\nCalibrate emulator against those points.\nUse emulator for UQ with MCMC or other methods."
  },
  {
    "objectID": "slides/lecture14-1.html#emulation-of-a-1-d-toy-model",
    "href": "slides/lecture14-1.html#emulation-of-a-1-d-toy-model",
    "title": "Model Complexity and Emulation",
    "section": "Emulation of a 1-D Toy Model",
    "text": "Emulation of a 1-D Toy Model\n\n\n\n\nEmulation of a Toy Model\n\n\n\nSource: Haran et al. (2017)"
  },
  {
    "objectID": "slides/lecture14-1.html#emulator-of-a-spatial-ice-sheet-model",
    "href": "slides/lecture14-1.html#emulator-of-a-spatial-ice-sheet-model",
    "title": "Model Complexity and Emulation",
    "section": "Emulator of a Spatial (Ice Sheet) Model",
    "text": "Emulator of a Spatial (Ice Sheet) Model\n\n\n\n\nEmulation of a Toy Model\n\n\n\nSource: Haran et al. (2017)"
  },
  {
    "objectID": "slides/lecture14-1.html#is-emulation-always-the-right-choice",
    "href": "slides/lecture14-1.html#is-emulation-always-the-right-choice",
    "title": "Model Complexity and Emulation",
    "section": "Is Emulation Always The Right Choice?",
    "text": "Is Emulation Always The Right Choice?\n\n\n\n\nEmulation of a Toy Model\n\n\n\nSource: Lee et al. (2020)"
  },
  {
    "objectID": "slides/lecture14-1.html#impacts-of-poor-emulation",
    "href": "slides/lecture14-1.html#impacts-of-poor-emulation",
    "title": "Model Complexity and Emulation",
    "section": "Impacts of “Poor” Emulation",
    "text": "Impacts of “Poor” Emulation\nThis error can have large knock-on effects for risk analysis:\n\n\n\n\nEmulation of a Toy Model\n\n\n\nSource: Lee et al. (2020)"
  },
  {
    "objectID": "slides/lecture14-1.html#key-takeaways-simplicity",
    "href": "slides/lecture14-1.html#key-takeaways-simplicity",
    "title": "Model Complexity and Emulation",
    "section": "Key Takeaways (Simplicity)",
    "text": "Key Takeaways (Simplicity)\n\nModel simplicity can be valuable for focusing on key dynamics and uncertainty representation.\nTradeoff between computational expense and fidelity of approximation."
  },
  {
    "objectID": "slides/lecture14-1.html#key-takeaways-emulation",
    "href": "slides/lecture14-1.html#key-takeaways-emulation",
    "title": "Model Complexity and Emulation",
    "section": "Key Takeaways (Emulation)",
    "text": "Key Takeaways (Emulation)\n\nEmulation can “simplify” complex models by approximating response surfaces\nEmulator methods have different pros and cons which can make them more or less important.\nEmulator error can strongly influence resulting risk estimates."
  },
  {
    "objectID": "slides/lecture14-1.html#upcoming-schedule",
    "href": "slides/lecture14-1.html#upcoming-schedule",
    "title": "Model Complexity and Emulation",
    "section": "Upcoming Schedule",
    "text": "Upcoming Schedule\nWednesday: Emulation Methods\nFriday: HW4 due\nNext Monday: Project Presentations, email slides by Saturday."
  },
  {
    "objectID": "slides/lecture14-1.html#references-1",
    "href": "slides/lecture14-1.html#references-1",
    "title": "Model Complexity and Emulation",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n\n\nBürkner, P.-C., Gabry, J., & Vehtari, A. (2020). Approximate leave-future-out cross-validation for bayesian time series models. J. Stat. Comput. Simul., (14), 2499–2523. https://doi.org/10.1080/00949655.2020.1783262\n\n\nHaran, M., Chang, W., Keller, K., Nicholas, R., & Pollard, D. (2017). Statistics and the future of the antarctic ice sheet. Chance, 30(4), 37–44. https://doi.org/10.1080/09332480.2017.1406758\n\n\nHelgeson, C., Srikrishnan, V., Keller, K., & Tuana, N. (2021). Why simpler computer simulation models can be epistemically better for informing decisions. Philos. Sci., 88(2), 213–233. https://doi.org/10.1086/711501\n\n\nLee, B. S., Haran, M., Fuller, R. W., Pollard, D., & Keller, K. (2020). A fast particle-based approach for calibrating a 3-D model of the antarctic ice sheet. Ann. Appl. Stat., 14(2), 605–634. https://doi.org/10.1214/19-AOAS1305\n\n\nVehtari, A., Gelman, A., & Gabry, J. (2017). Practical bayesian model evaluation using leave-one-out cross-validation and WAIC. Stat. Comput., 27(5), 1413–1432. https://doi.org/10.1007/s11222-016-9696-4\n\n\nYao, Y., Vehtari, A., Simpson, D., & Gelman, A. (2018). Using stacking to average Bayesian predictive distributions. arXiv [Stat.ME]. https://doi.org/10.1214/17-BA1091"
  },
  {
    "objectID": "slides/lecture09-2.html#markov-chain-strategy",
    "href": "slides/lecture09-2.html#markov-chain-strategy",
    "title": "MCMC: Convergence and Example",
    "section": "Markov Chain Strategy",
    "text": "Markov Chain Strategy\n\nGenerate an appropriate Markov chain so that its stationary distribution of the target distribution \\(\\pi\\);\nRun its dynamics long enough to converge to the stationary distribution;\nUse the resulting ensemble of states as Monte Carlo samples from \\(\\pi\\) ."
  },
  {
    "objectID": "slides/lecture09-2.html#markov-chain-convergence",
    "href": "slides/lecture09-2.html#markov-chain-convergence",
    "title": "MCMC: Convergence and Example",
    "section": "Markov Chain Convergence",
    "text": "Markov Chain Convergence\nGiven a Markov chain \\(\\{X_t\\}_{t=1, \\ldots, T}\\) returned from this procedure, sampling from distribution \\(\\pi\\):\n\n\\(\\mathbb{P}(X_t = y) \\to \\pi(y)\\) as \\(t \\to \\infty\\)\nThis means the chain can be considered a dependent sample approximately distributed from \\(\\pi\\).\nThe first values (the transient portion) of the chain are highly dependent on the initial value."
  },
  {
    "objectID": "slides/lecture09-2.html#the-metropolis-hastings-algorithm",
    "href": "slides/lecture09-2.html#the-metropolis-hastings-algorithm",
    "title": "MCMC: Convergence and Example",
    "section": "The Metropolis-Hastings Algorithm",
    "text": "The Metropolis-Hastings Algorithm\nGiven \\(X_t = x_t\\):\n\nGenerate \\(Y_t \\sim q(y | x_t)\\);\nSet \\(X_{t+1} = Y_t\\) with probability \\(\\rho(x_t, Y_t)\\), where \\[\\rho(x, y) = \\min \\left\\{\\frac{\\pi(y)}{\\pi(x)}\\frac{q(x | y)}{q(y | x)}, 1\\right\\},\\] else set \\(X_{t+1} = x_t\\)."
  },
  {
    "objectID": "slides/lecture09-2.html#proposals",
    "href": "slides/lecture09-2.html#proposals",
    "title": "MCMC: Convergence and Example",
    "section": "Proposals",
    "text": "Proposals\n\n“Goldilocks” proposal: acceptance rate 30-45%.\nProposal distribution \\(q\\) plays a big role in the effective sample size (ESS): \\[N_\\text{eff} = \\frac{N}{1+2\\sum_{t=1}^\\infty \\rho_t}\\]"
  },
  {
    "objectID": "slides/lecture09-2.html#sampling-efficiency-example",
    "href": "slides/lecture09-2.html#sampling-efficiency-example",
    "title": "MCMC: Convergence and Example",
    "section": "Sampling Efficiency Example",
    "text": "Sampling Efficiency Example\n\n\n\n\nMCMC Sampling for Various Proposals"
  },
  {
    "objectID": "slides/lecture09-2.html#transient-chain-portion",
    "href": "slides/lecture09-2.html#transient-chain-portion",
    "title": "MCMC: Convergence and Example",
    "section": "Transient Chain Portion",
    "text": "Transient Chain Portion\nWhat do we do with the transient portion of the chain?\n\n\nDiscard as burn-in;\nJust run the chain longer."
  },
  {
    "objectID": "slides/lecture09-2.html#how-to-identify-convergence",
    "href": "slides/lecture09-2.html#how-to-identify-convergence",
    "title": "MCMC: Convergence and Example",
    "section": "How To Identify Convergence?",
    "text": "How To Identify Convergence?\nShort answer: There is no guarantee! Judgement based on an accumulation of evidence from various heuristics.\n\nThe good news — getting the precise “right” end of the transient chain doesn’t matter.\nIf a few transient iterations remain, the effect will be washed out with a large enough post-convergence chain."
  },
  {
    "objectID": "slides/lecture09-2.html#heuristics-for-convergence",
    "href": "slides/lecture09-2.html#heuristics-for-convergence",
    "title": "MCMC: Convergence and Example",
    "section": "Heuristics for Convergence",
    "text": "Heuristics for Convergence\nCompare distribution (histogram/kernel density plot) after half of the chain to full chain.\n\n\n\n\n\n\n\n\n2000 Iterations\n\n\n\n\n\n\n\n10000 Iterations\n\n\n\n\n\n\nFigure 1"
  },
  {
    "objectID": "slides/lecture09-2.html#gelman-rubin-diagnostic",
    "href": "slides/lecture09-2.html#gelman-rubin-diagnostic",
    "title": "MCMC: Convergence and Example",
    "section": "Gelman-Rubin Diagnostic",
    "text": "Gelman-Rubin Diagnostic\nGelman & Rubin (1992)\n\nRun multiple chains from “overdispersed” starting points\nCompare intra-chain and inter-chain variances\nSummarized as \\(\\hat{R}\\) statistic: closer to 1 implies better convergence.\nCan also check distributions across multiple chains vs. the half-chain check."
  },
  {
    "objectID": "slides/lecture09-2.html#on-multiple-chains",
    "href": "slides/lecture09-2.html#on-multiple-chains",
    "title": "MCMC: Convergence and Example",
    "section": "On Multiple Chains",
    "text": "On Multiple Chains\nUnless a specific scheme is used, multiple chains are not a solution for issues of convergence, as each individual chain needs to converge and have burn-in discarded/watered-down.\nThis means multiple chains are more useful for diagnostics, but once they’ve all been run long enough, can mix samples freely."
  },
  {
    "objectID": "slides/lecture09-2.html#heuristics-for-convergence-1",
    "href": "slides/lecture09-2.html#heuristics-for-convergence-1",
    "title": "MCMC: Convergence and Example",
    "section": "Heuristics for Convergence",
    "text": "Heuristics for Convergence\n\nIf you’re more interested in the mean estimate, can also look at the its stability by iteration or the Monte Carlo standard error.\nLook at traceplots; do you see sudden “jumps”?\nWhen in doubt, run the chain longer."
  },
  {
    "objectID": "slides/lecture09-2.html#adaptive-metropolis-hastings",
    "href": "slides/lecture09-2.html#adaptive-metropolis-hastings",
    "title": "MCMC: Convergence and Example",
    "section": "Adaptive Metropolis-Hastings",
    "text": "Adaptive Metropolis-Hastings\nAdjust proposal density to hit target acceptance rate.\n\nNeed to be cautious about detailed balance.\nTypical strategy is to adapt for a portion of the initial chain (part of the burn-in), then run longer with that proposal."
  },
  {
    "objectID": "slides/lecture09-2.html#hamiltonian-monte-carlo",
    "href": "slides/lecture09-2.html#hamiltonian-monte-carlo",
    "title": "MCMC: Convergence and Example",
    "section": "Hamiltonian Monte Carlo",
    "text": "Hamiltonian Monte Carlo\n\nIdea: Use proposals which steer towards “typical set” without collapsing towards the mode (based on Hamiltonian vector field);\nRequires gradient information: can be obtained through autodifferentiation; challenging for external models;\nCan be very efficient due to potential for anti-correlated samples, but very sensitive to parameterization.\nSame principles for evaluating convergence apply."
  },
  {
    "objectID": "slides/lecture09-2.html#data",
    "href": "slides/lecture09-2.html#data",
    "title": "MCMC: Convergence and Example",
    "section": "Data",
    "text": "Data\n\n\nCode\n# load SF tide gauge data\n# read in data and get annual maxima\nfunction load_data(fname)\n    date_format = DateFormat(\"yyyy-mm-dd HH:MM:SS\")\n    # This uses the DataFramesMeta.jl package, which makes it easy to string together commands to load and process data\n    df = @chain fname begin\n        CSV.read(DataFrame; header=false)\n        rename(\"Column1\" =&gt; \"year\", \"Column2\" =&gt; \"month\", \"Column3\" =&gt; \"day\", \"Column4\" =&gt; \"hour\", \"Column5\" =&gt; \"gauge\")\n        # need to reformat the decimal date in the data file\n        @transform :datetime = DateTime.(:year, :month, :day, :hour)\n        # replace -99999 with missing\n        @transform :gauge = ifelse.(abs.(:gauge) .&gt;= 9999, missing, :gauge)\n        select(:datetime, :gauge)\n    end\n    return df\nend\n\ndat = load_data(\"data/surge/h551.csv\")\n\n# detrend the data to remove the effects of sea-level rise and seasonal dynamics\nma_length = 366\nma_offset = Int(floor(ma_length/2))\nmoving_average(series,n) = [mean(@view series[i-n:i+n]) for i in n+1:length(series)-n]\ndat_ma = DataFrame(datetime=dat.datetime[ma_offset+1:end-ma_offset], residual=dat.gauge[ma_offset+1:end-ma_offset] .- moving_average(dat.gauge, ma_offset))\n\n# group data by year and compute the annual maxima\ndat_ma = dropmissing(dat_ma) # drop missing data\ndat_annmax = combine(dat_ma -&gt; dat_ma[argmax(dat_ma.residual), :], groupby(DataFrames.transform(dat_ma, :datetime =&gt; x-&gt;year.(x)), :datetime_function))\ndelete!(dat_annmax, nrow(dat_annmax)) # delete 2023; haven't seen much of that year yet\nrename!(dat_annmax, :datetime_function =&gt; :Year)\nselect!(dat_annmax, [:Year, :residual])\n\n# make plots\np1 = plot(\n    dat_annmax.Year,\n    dat_annmax.residual;\n    xlabel=\"Year\",\n    ylabel=\"Annual Max Tide Level (mm)\",\n    label=false,\n    marker=:circle,\n    markersize=5,\n    tickfontsize=16,\n    guidefontsize=18\n)\np2 = histogram(\n    dat_annmax.residual,\n    normalize=:pdf,\n    orientation=:horizontal,\n    label=:false,\n    xlabel=\"PDF\",\n    xlims=(0, 0.006),\n    ylabel=\"\",\n    yticks=[],\n    xticks = [],\n    tickfontsize=16,\n    guidefontsize=18\n)\n\nl = @layout [a{0.7w} b{0.3w}]\nplot(p1, p2; layout=l, link=:y, ylims=(1000, 1700), bottom_margin=5mm, left_margin=5mm)\nplot!(size=(1000, 450))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Annual maxima surge data from the San Francisco, CA tide gauge."
  },
  {
    "objectID": "slides/lecture09-2.html#probability-model-annual-maxima",
    "href": "slides/lecture09-2.html#probability-model-annual-maxima",
    "title": "MCMC: Convergence and Example",
    "section": "Probability Model (Annual Maxima)",
    "text": "Probability Model (Annual Maxima)\n\\[\\begin{gather*}\ny_t \\sim \\text{GEV}(\\mu, \\sigma, \\xi) \\\\\n\\mu \\sim \\mathcal{LogNormal}(7, 0.25) \\\\\n\\sigma \\sim \\mathcal{TN}(0, 100; 0, \\infty) \\\\\n\\xi \\sim \\mathcal{N}(0, 0.1)\n\\end{gather*}\\]"
  },
  {
    "objectID": "slides/lecture09-2.html#prior-predictive-check",
    "href": "slides/lecture09-2.html#prior-predictive-check",
    "title": "MCMC: Convergence and Example",
    "section": "Prior Predictive Check",
    "text": "Prior Predictive Check\n\n\nCode\n# sample from priors\nμ = rand(LogNormal(7, 0.25), 1000)\nσ = rand(truncated(Normal(0, 100), lower=0), 1000)\nξ = rand(Normal(0, 0.1), 1000)\n# simulate\n# define return periods and cmopute return levels for parameters\nreturn_periods = 2:100\nreturn_levels = zeros(1_000, length(return_periods))\nfor i in 1:1_000\n    return_levels[i, :] = quantile.(GeneralizedExtremeValue(μ[i], σ[i], ξ[i]), 1 .- (1 ./ return_periods))\nend\n\nplt_prior_1 = plot(; ylabel=\"Return Level (m)\", xlabel=\"Return Period (yrs)\", tickfontsize=16, legendfontsize=18, guidefontsize=18, bottom_margin=10mm, left_margin=10mm, legend=:topleft)\nfor idx in 1:1_000\n    label = idx == 1 ? \"Prior\" : false\n    plot!(plt_prior_1, return_periods, return_levels[idx, :]; color=:black, alpha=0.1, label=label)\nend\nplt_prior_1\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Prior predictive check for surge model."
  },
  {
    "objectID": "slides/lecture09-2.html#probabilistic-programming-languages",
    "href": "slides/lecture09-2.html#probabilistic-programming-languages",
    "title": "MCMC: Convergence and Example",
    "section": "Probabilistic Programming Languages",
    "text": "Probabilistic Programming Languages\n\nRely on more advanced methods (e.g. Hamiltonian Monte Carlo) to draw samples more efficiently.\nUse automatic differentiation to compute gradients.\nSyntax closely resembles statistical model specification.\nExamples:\n\nTuring.jl in Julia\nPyMC in Python\nStan, cross-language"
  },
  {
    "objectID": "slides/lecture09-2.html#turing-model-specification",
    "href": "slides/lecture09-2.html#turing-model-specification",
    "title": "MCMC: Convergence and Example",
    "section": "Turing Model Specification",
    "text": "Turing Model Specification\n\n@model function sf_surge(y)\n    ## pick priors\n    μ ~ LogNormal(7, 0.25) # location\n    σ ~ truncated(Normal(0, 100); lower=0) # scale\n    ξ ~ Normal(0, 0.1) # shape\n\n    ## likelihood\n    y .~ GeneralizedExtremeValue(μ, σ, ξ)\nend\n\nsf_surge (generic function with 2 methods)"
  },
  {
    "objectID": "slides/lecture09-2.html#sampling-with-turing",
    "href": "slides/lecture09-2.html#sampling-with-turing",
    "title": "MCMC: Convergence and Example",
    "section": "Sampling with Turing",
    "text": "Sampling with Turing\n\nsurge_chain = let # variables defined in a let...end block are temporary\n    model = sf_surge(dat_annmax.residual) # initialize model with data\n    sampler = NUTS() # use the No-U-Turn Sampler; there are other options\n    nsamples = 10_000\n    sample(model, sampler, nsamples; drop_warmup=true)\nend\nsummarystats(surge_chain)\n\n\nSummary Statistics\n  parameters        mean       std      mcse    ess_bulk    ess_tail      rhat ⋯\n      Symbol     Float64   Float64   Float64     Float64     Float64   Float64 ⋯\n           μ   1258.8094    5.5677    0.0613   8246.1454   6832.8697    1.0000 ⋯\n           σ     57.4330    4.1522    0.0472   7705.8149   6328.2079    1.0003 ⋯\n           ξ      0.0172    0.0517    0.0006   7025.3590   5849.6953    1.0003 ⋯\n                                                                1 column omitted"
  },
  {
    "objectID": "slides/lecture09-2.html#sampling-visualization",
    "href": "slides/lecture09-2.html#sampling-visualization",
    "title": "MCMC: Convergence and Example",
    "section": "Sampling Visualization",
    "text": "Sampling Visualization\n\n\nCode\nplot(surge_chain, size=(1200, 500), left_margin=5mm, bottom_margin=5mm)\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Sampler visualization for surge chain"
  },
  {
    "objectID": "slides/lecture09-2.html#optimizing-with-turing",
    "href": "slides/lecture09-2.html#optimizing-with-turing",
    "title": "MCMC: Convergence and Example",
    "section": "Optimizing with Turing",
    "text": "Optimizing with Turing\nWe can also use Turing.jl along with Optim.jl to get the MLE and MAP."
  },
  {
    "objectID": "slides/lecture09-2.html#mle",
    "href": "slides/lecture09-2.html#mle",
    "title": "MCMC: Convergence and Example",
    "section": "MLE",
    "text": "MLE\n\nmle_surge = optimize(sf_surge(dat_annmax.residual), MLE())\ncoeftable(mle_surge)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCoef.\nStd. Error\nz\nPr(&gt;\nz\n)\n\n\n\n\nμ\n1258.71\n5.61428\n224.198\n0.0\n1247.71\n1269.71\n\n\nσ\n56.2665\n4.08661\n13.7685\n3.94289e-43\n48.2569\n64.2761\n\n\nξ\n0.0171937\n0.0624531\n0.275306\n0.783081\n-0.105212\n0.139599"
  },
  {
    "objectID": "slides/lecture09-2.html#map",
    "href": "slides/lecture09-2.html#map",
    "title": "MCMC: Convergence and Example",
    "section": "MAP",
    "text": "MAP\n\nmap_surge = optimize(sf_surge(dat_annmax.residual), MAP())\ncoeftable(map_surge)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCoef.\nStd. Error\nz\nPr(&gt;\nz\n)\n\n\n\n\nμ\n1258.73\n5.52041\n228.014\n0.0\n1247.91\n1269.55\n\n\nσ\n56.2336\n4.04126\n13.9149\n5.14484e-44\n48.3129\n64.1543\n\n\nξ\n0.0129045\n0.0523448\n0.246528\n0.805273\n-0.0896894\n0.115498"
  },
  {
    "objectID": "slides/lecture09-2.html#posterior-visualization",
    "href": "slides/lecture09-2.html#posterior-visualization",
    "title": "MCMC: Convergence and Example",
    "section": "Posterior Visualization",
    "text": "Posterior Visualization\n\n\nCode\np1 = histogram(surge_chain[:μ], label=\"Samples\", normalize=:pdf, legend=:topleft, xlabel=L\"μ\", ylabel=L\"p(μ|y)\")\np2 = histogram(surge_chain[:σ], label=\"Samples\", normalize=:pdf, legend=:topleft, xlabel=L\"σ\", ylabel=L\"p(σ|y)\")\np3 = histogram(surge_chain[:ξ], label=\"Samples\", normalize=:pdf, legend=:topleft, xlabel=L\"σ\", ylabel=L\"p(σ|y)\")\np = plot(p1, p2, p3, tickfontsize=16, guidefontsize=18, legendfontsize=18, left_margin=10mm, bottom_margin=10mm, layout = @layout [a b c])\nvline!(p, mean(surge_chain)[:, 2]', color=:purple, linewidth=3, label=\"Posterior Mean\")\nplot!(p, size=(1200, 450))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Posterior visualization for surge chain"
  },
  {
    "objectID": "slides/lecture09-2.html#correlations",
    "href": "slides/lecture09-2.html#correlations",
    "title": "MCMC: Convergence and Example",
    "section": "Correlations",
    "text": "Correlations\n\n\nCode\np1 = histogram2d(surge_chain[:μ], surge_chain[:σ], normalize=:pdf, legend=false, xlabel=L\"μ\", ylabel=L\"σ\")\np2 = histogram2d(surge_chain[:μ], surge_chain[:ξ], normalize=:pdf, legend=false, xlabel=L\"μ\", ylabel=L\"ξ\")\np3 = histogram2d(surge_chain[:σ], surge_chain[:ξ], normalize=:pdf, legend=false, xlabel=L\"σ\", ylabel=L\"ξ\")\np = plot(p1, p2, p3, tickfontsize=16, guidefontsize=18, left_margin=5mm, bottom_margin=5mm, layout = @layout [a b c])\nplot!(p, size=(1200, 450))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Posterior correlations"
  },
  {
    "objectID": "slides/lecture09-2.html#posterior-predictive-checks",
    "href": "slides/lecture09-2.html#posterior-predictive-checks",
    "title": "MCMC: Convergence and Example",
    "section": "Posterior Predictive Checks",
    "text": "Posterior Predictive Checks\n\n\nCode\nplt_rt = plot(; ylabel=\"Return Level (m)\", xlabel=\"Return Period (yrs)\", tickfontsize=16, legendfontsize=18, guidefontsize=18, bottom_margin=10mm, left_margin=10mm, legend=:topleft)\nfor idx in 1:1000\n    μ = surge_chain[:μ][idx]\n    σ = surge_chain[:σ][idx]\n    ξ = surge_chain[:ξ][idx]\n    return_levels[idx, :] = quantile.(GeneralizedExtremeValue(μ, σ, ξ), 1 .- (1 ./ return_periods))\n    label = idx == 1 ? \"Posterior\" : false\n    plot!(plt_rt, return_periods, return_levels[idx, :]; color=:black, alpha=0.05, label=label, linewidth=0.5)\nend\n# plot return level quantiles\nrl_q = mapslices(col -&gt; quantile(col, [0.025, 0.5, 0.975]), return_levels, dims=1)\nplot!(plt_rt, return_periods, rl_q[[1,3], :]', color=:green, linewidth=2, label=\"95% CI\")\nplot!(plt_rt, return_periods, rl_q[2, :], color=:red, linewidth=2, label=\"Posterior Median\")\n# plot data\nscatter!(plt_rt, return_periods, quantile(dat_annmax.residual, 1 .- (1 ./ return_periods)), label=\"Data\", color=:black)\nplot!(plt_rt, size=(1200, 500))\nplt_rt\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: Posterior predictive checks"
  },
  {
    "objectID": "slides/lecture09-2.html#multiple-chains",
    "href": "slides/lecture09-2.html#multiple-chains",
    "title": "MCMC: Convergence and Example",
    "section": "Multiple Chains",
    "text": "Multiple Chains\n\nsurge_chain = let # variables defined in a let...end block are temporary\n    model = sf_surge(dat_annmax.residual) # initialize model with data\n    sampler = NUTS() # use the No-U-Turn Sampler; there are other options\n    nsamples = 10_000\n    nchains = 4\n    sample(model, sampler, MCMCThreads(), nsamples, nchains; drop_warmup=true)\nend\ngelmandiag(surge_chain)\n\n\nGelman, Rubin, and Brooks diagnostic\n  parameters      psrf    psrfci \n      Symbol   Float64   Float64 \n           μ    1.0000    1.0002\n           σ    1.0000    1.0000\n           ξ    1.0001    1.0002"
  },
  {
    "objectID": "slides/lecture09-2.html#plotting-multiple-chains",
    "href": "slides/lecture09-2.html#plotting-multiple-chains",
    "title": "MCMC: Convergence and Example",
    "section": "Plotting Multiple Chains",
    "text": "Plotting Multiple Chains\n\n\nCode\nplot(surge_chain)\nplot!(size=(1200, 500))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8: Sampler visualization for multiple surge chains"
  },
  {
    "objectID": "slides/lecture09-2.html#key-points-convergence",
    "href": "slides/lecture09-2.html#key-points-convergence",
    "title": "MCMC: Convergence and Example",
    "section": "Key Points (Convergence)",
    "text": "Key Points (Convergence)\n\nMust rely on “accumulation of evidence” from heuristics for determination about convergence to stationary distribution.\nTransient portion of chain: Meh. Some people worry about this too much. Discard or run the chain longer.\nParallelizing solves few problems, but running multiple chains can be useful for diagnostics."
  },
  {
    "objectID": "slides/lecture09-2.html#next-classes",
    "href": "slides/lecture09-2.html#next-classes",
    "title": "MCMC: Convergence and Example",
    "section": "Next Classes",
    "text": "Next Classes\nMonday: MCMC Lab (No exercises these weeks)\nNext Wednesday: Literature Presentations (email slides by 9pm Tuesday night)."
  },
  {
    "objectID": "slides/lecture09-2.html#assessments",
    "href": "slides/lecture09-2.html#assessments",
    "title": "MCMC: Convergence and Example",
    "section": "Assessments",
    "text": "Assessments\n\nHomework 3: Due 3/22"
  },
  {
    "objectID": "slides/lecture09-2.html#references-1",
    "href": "slides/lecture09-2.html#references-1",
    "title": "MCMC: Convergence and Example",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n\n\nGelman, A., & Rubin, D. B. (1992). Inference from Iterative Simulation Using Multiple Simulations. Stat. Sci., 7, 457–511. https://doi.org/10.1214/ss/1177011136"
  },
  {
    "objectID": "slides/lecture09-1.html#bayesian-computation",
    "href": "slides/lecture09-1.html#bayesian-computation",
    "title": "Markov Chain Monte Carlo",
    "section": "Bayesian Computation",
    "text": "Bayesian Computation\n\nGoal: Sample from posterior distribution to:\n\nCapture parametric uncertainty;\nCompute MAP estimates.\n\nChallenging because of arbitrary nature of distributions."
  },
  {
    "objectID": "slides/lecture09-1.html#markov-chain-strategy",
    "href": "slides/lecture09-1.html#markov-chain-strategy",
    "title": "Markov Chain Monte Carlo",
    "section": "Markov Chain Strategy",
    "text": "Markov Chain Strategy\n\nGenerate an appropriate Markov chain so that its stationary distribution of the target distribution \\(\\pi\\);\nRun its dynamics long enough to converge to the stationary distribution;\nUse the resulting ensemble of states as Monte Carlo samples from \\(\\pi\\) ."
  },
  {
    "objectID": "slides/lecture09-1.html#markov-chain-convergence",
    "href": "slides/lecture09-1.html#markov-chain-convergence",
    "title": "Markov Chain Monte Carlo",
    "section": "Markov Chain Convergence",
    "text": "Markov Chain Convergence\nGiven a Markov chain \\(\\{X_t\\}_{t=1, \\ldots, T}\\) returned from this procedure, sampling from distribution \\(\\pi\\):\n\n\\(\\mathbb{P}(X_t = y) \\to \\pi(y)\\) as \\(t \\to \\infty\\)\nThis means the chain can be considered a dependent sample approximately distributed from \\(\\pi\\).\nThe first values (the transient portion) of the chain are highly dependent on the initial value."
  },
  {
    "objectID": "slides/lecture09-1.html#the-metropolis-hastings-algorithm",
    "href": "slides/lecture09-1.html#the-metropolis-hastings-algorithm",
    "title": "Markov Chain Monte Carlo",
    "section": "The Metropolis-Hastings Algorithm",
    "text": "The Metropolis-Hastings Algorithm\nThe Metropolis-Hastings algorithm:\n\nThe foundational MCMC algorithm (and was named one of the top ten algorithms of the 20th century).\nBuilds a Markov chain based on transitions by:\n\ngenerating proposals for new samples from a conditional proposal distribution \\(q(y | x)\\);\naccepting or rejecting those proposals."
  },
  {
    "objectID": "slides/lecture09-1.html#the-metropolis-hastings-algorithm-1",
    "href": "slides/lecture09-1.html#the-metropolis-hastings-algorithm-1",
    "title": "Markov Chain Monte Carlo",
    "section": "The Metropolis-Hastings Algorithm",
    "text": "The Metropolis-Hastings Algorithm\nGiven \\(X_t = x_t\\):\n\nGenerate \\(Y_t \\sim q(y | x_t)\\);\nSet \\(X_{t+1} = Y_t\\) with probability \\(\\rho(x_t, Y_t)\\), where \\[\\rho(x, y) = \\min \\left\\{\\frac{\\pi(y)}{\\pi(x)}\\frac{q(x | y)}{q(y | x)}, 1\\right\\},\\] else set \\(X_{t+1} = x_t\\)."
  },
  {
    "objectID": "slides/lecture09-1.html#how-simple-is-that",
    "href": "slides/lecture09-1.html#how-simple-is-that",
    "title": "Markov Chain Monte Carlo",
    "section": "How Simple Is That?",
    "text": "How Simple Is That?\nThe devil is in the details: performance and efficiency are highly dependent on the choice of \\(q\\).\n\nKey: There is a tradeoff between exploration and acceptance.\n\nWide proposal: Can make bigger jumps, may be more likely to reject proposals.\nNarrow proposal: More likely to accept proposals, may not “mix” efficiently."
  },
  {
    "objectID": "slides/lecture09-1.html#proposal-distribution-choice",
    "href": "slides/lecture09-1.html#proposal-distribution-choice",
    "title": "Markov Chain Monte Carlo",
    "section": "Proposal Distribution Choice",
    "text": "Proposal Distribution Choice\nThe original Metropolis et al. (1953) algorithm used symmetric distributions (\\(q(y | x) = q(x | y)\\)).\nThen the acceptance probability reduces to \\[\\rho =  \\min \\left\\{\\frac{\\pi(y)}{\\pi(x)}, 1\\right\\}.\\]\nA common choice: \\(y \\sim \\text{Normal}(X_t, \\sigma)\\) centered around the current point \\(X_t\\)."
  },
  {
    "objectID": "slides/lecture09-1.html#julia-implementation",
    "href": "slides/lecture09-1.html#julia-implementation",
    "title": "Markov Chain Monte Carlo",
    "section": "Julia Implementation",
    "text": "Julia Implementation\n\nfunction mh_transition(x_current, σ)\n    # generate new proposal\n    x_proposal = rand(Normal(x_current, σ))\n    u = rand()\n    ρ = log(target_density(x_proposal)) - log(target_density(x_current)) # transition log-probability\n    if log(u) &lt; min(ρ, 1)\n        y = x_proposal\n    else\n        y = x_current\n    end\n    return y, log(target_density(y))\nend\n\nmh_transition (generic function with 1 method)"
  },
  {
    "objectID": "slides/lecture09-1.html#julia-implementation-1",
    "href": "slides/lecture09-1.html#julia-implementation-1",
    "title": "Markov Chain Monte Carlo",
    "section": "Julia Implementation",
    "text": "Julia Implementation\n\nfunction mh_algorithm(n_iter, σ, x₀)\n    # initialize storage\n    samples = zeros(n_iter) \n    log_target = zeros(n_iter)\n    samples[1] = x₀ # start algorithm\n    log_target[1] = log(target_density(x₀))\n    accept_count = 0\n    for i = 2:length(samples) # iterate\n        samples[i], log_target[i] = mh_transition(samples[i-1], σ)\n        if samples[i] != samples[i-1]\n            accept_count += 1\n        end\n    end\n    accept_rate = accept_count / n_iter # compute acceptance rate\n    return samples, log_target, accept_rate\nend\n\nmh_algorithm (generic function with 1 method)"
  },
  {
    "objectID": "slides/lecture09-1.html#linear-regression-example",
    "href": "slides/lecture09-1.html#linear-regression-example",
    "title": "Markov Chain Monte Carlo",
    "section": "Linear Regression Example",
    "text": "Linear Regression Example\n\n\n\\[\n\\begin{gather}\ny = 5 + 2x + \\varepsilon \\\\\n\\varepsilon \\sim \\text{Normal}(0, 3)\n\\end{gather}\n\\]\n\n\n\nCode\n# create trend for data\nx = rand(Uniform(0, 20), 20)\ny = 5 .+ 2 * x\n# sample and add noise\nε = rand(Normal(0, 3), 20)\ny .+= ε\n\np = scatter(x, y, label=\"Data\", xlabel=L\"$x$\", ylabel=L\"$y$\", markershape=:o, markersize=10, tickfontsize=16, guidefontsize=18, legendfontsize=16, bottom_margin=10mm, left_margin=5mm, right_margin=5mm) \nplot!(p, size=(600, 550))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Regression data plot"
  },
  {
    "objectID": "slides/lecture09-1.html#model-specification",
    "href": "slides/lecture09-1.html#model-specification",
    "title": "Markov Chain Monte Carlo",
    "section": "Model Specification",
    "text": "Model Specification\n\\[\n\\begin{gather}\ny = a + bx + \\varepsilon \\\\\n\\varepsilon \\sim \\text{Normal}(0, \\sigma).\n\\end{gather}\n\\]\nThis makes the likelihood: \\[\ny \\sim \\text{Normal}(a+bx, \\sigma).\n\\]"
  },
  {
    "objectID": "slides/lecture09-1.html#prior-selection",
    "href": "slides/lecture09-1.html#prior-selection",
    "title": "Markov Chain Monte Carlo",
    "section": "Prior Selection",
    "text": "Prior Selection\n\n\n\\[\n\\begin{gather}\na \\sim \\text{Normal(0, 2)} \\\\\nb \\sim \\text{Normal(0, 2)} \\\\\n\\sigma \\sim \\text{Half-Normal}(0, 1)\n\\end{gather}\n\\]\n\n\n\nCode\n# generate data for samples\nfunction gen_data(a, b, σ)\n    x = collect(0:20)\n    y = a .+ b * x\n    # sample and add noise\n    ε = rand(Normal(0, σ), length(x))\n    y .+= ε\n    return y\nend\n\n# sample and plot\nn_samples = 1000\na = rand(Normal(0, 2), n_samples)\nb = rand(Normal(0, 2), n_samples)\nσ = rand(truncated(Normal(0, 1), lower=0), 1000)\ny_prior = [gen_data(a[i], b[i], σ[i]) for i in 1:n_samples]\n# convert y to a Matrix by vcatting each vector\ny_prior = mapreduce(permutedims, vcat, y_prior) \nplt_prior_1 = plot(; ylabel=L\"$y$\", xlabel=L\"$x$\",\n    tickfontsize=16, legendfontsize=16, guidefontsize=18, bottom_margin=5mm, left_margin=5mm, legend=false)\nfor x ∈ [0, 5, 10, 15, 20]\n    boxplot!(plt_prior_1, [x], y_prior[:, x+1], color=:blue)\nend\nplot!(plt_prior_1, size=(600, 550))\nplt_prior_1\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Prior predictive plot for regression example."
  },
  {
    "objectID": "slides/lecture09-1.html#proposal-distribution-and-initial-value",
    "href": "slides/lecture09-1.html#proposal-distribution-and-initial-value",
    "title": "Markov Chain Monte Carlo",
    "section": "Proposal Distribution and Initial Value",
    "text": "Proposal Distribution and Initial Value\nTo illustrate how the M-H algorithm works, let’s use a proposal \\[\\mathcal{N}(x_t, 0.01I_3).\\]\nAnd let’s start at \\[x_0 = \\begin{pmatrix}1 \\\\ 1 \\\\ 1\\end{pmatrix}\\]."
  },
  {
    "objectID": "slides/lecture09-1.html#first-proposal",
    "href": "slides/lecture09-1.html#first-proposal",
    "title": "Markov Chain Monte Carlo",
    "section": "First Proposal",
    "text": "First Proposal\n\n\nCurrent:\n\\[X_0 = \\begin{pmatrix}1 \\\\ 1 \\\\ 1\\end{pmatrix}\\]\n\\[\\text{log-posterior} = -2851\\]\n\nIteration:\n\\[y = \\begin{pmatrix}0.94 \\\\ 1.07 \\\\ 0.82\\end{pmatrix}\\]\n\\[\\text{log-posterior} = -3433\\]\n\n\n\\[\\rho \\approx 0 \\Rightarrow X_1 = X_0\\]"
  },
  {
    "objectID": "slides/lecture09-1.html#another-proposal",
    "href": "slides/lecture09-1.html#another-proposal",
    "title": "Markov Chain Monte Carlo",
    "section": "Another Proposal",
    "text": "Another Proposal\n\n\nCurrent:\n\\[X_1 = \\begin{pmatrix}1 \\\\ 1 \\\\ 1\\end{pmatrix}\\]\n\\[\\text{log-posterior} = -2851\\]\n\nIteration:\n\\[y = \\begin{pmatrix}1.24 \\\\ 1.05 \\\\ 1.04\\end{pmatrix}\\]\n\\[\\text{log-posterior} = -2165\\]\n\n\n\\[\\rho =1 \\Rightarrow X_2 = y\\]"
  },
  {
    "objectID": "slides/lecture09-1.html#iterations",
    "href": "slides/lecture09-1.html#iterations",
    "title": "Markov Chain Monte Carlo",
    "section": "1,000 Iterations",
    "text": "1,000 Iterations\n\n\nCode\nsamples, lpost, α = mh_algorithm(100000, [1; 1; 1], x, y)\n\np = plot(samples[1:1000, :], layout=(1, 3), label=\"Samples\", guidefontsize=18, tickfontsize=16, legendfontsize=16, bottom_margin=15mm, ylabel=\"Value\", xlabel=\"Iteration\", left_margin=10mm)\nhline!(p, [5 2 3], color=:red, linestyle=:dash, label=\"True Value\")\nplot!(p, size=(1300, 400))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: First 1,000 iterations of the MCMC example"
  },
  {
    "objectID": "slides/lecture09-1.html#iterations-1",
    "href": "slides/lecture09-1.html#iterations-1",
    "title": "Markov Chain Monte Carlo",
    "section": "100,000 Iterations",
    "text": "100,000 Iterations\n\n\nCode\np = plot(samples, layout=(1, 3), label=\"Samples\", guidefontsize=18, tickfontsize=16, legendfontsize=16, bottom_margin=15mm, xlabel=\"Iteration\", left_margin=10mm, xticks=0:50000:100000, right_margin=15mm, ylabel = [L\"$a$\" L\"$b$\" L\"$\\sigma$\"], legend=[:false :bottomright :false])\nhline!(p, [5 2 3], color=:red, linestyle=:dash, label=\"True Value\", linewidth=3)\nplot!(p, size=(1300, 400))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: 100,000 iterations of the MCMC example\n\n\n\n\nAcceptance rate: 0.50435"
  },
  {
    "objectID": "slides/lecture09-1.html#marginal-distributions",
    "href": "slides/lecture09-1.html#marginal-distributions",
    "title": "Markov Chain Monte Carlo",
    "section": "Marginal Distributions",
    "text": "Marginal Distributions\n\n\nCode\np = histogram(samples[10001:end, :], layout=(1, 3), label=false, guidefontsize=18, tickfontsize=16, legendfontsize=14, bottom_margin=15mm, ylabel=\"Count\", left_margin=10mm, xlabel = [L\"$a$\" L\"$b$\" L\"$\\sigma$\"], legend=[:false :false :topright], color=:lightblue)\nvline!(p, [5 2 3], color=:red, linestyle=:dash, label=\"True Value\", linewidth=3)\nvline!(p, mapslices(mean, samples[10001:end, :], dims=1), color=:purple, linestyle=:dash, label=\"Posterior Mean\", linewidth=3)\nplot!(p, size=(1300, 450))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: 100,000 iterations of the MCMC example"
  },
  {
    "objectID": "slides/lecture09-1.html#proposal-distribution-example",
    "href": "slides/lecture09-1.html#proposal-distribution-example",
    "title": "Markov Chain Monte Carlo",
    "section": "Proposal Distribution Example",
    "text": "Proposal Distribution Example\n\n\nCode\n# target density: modified Normal(0, 1) PDF\nfunction target_distribution(x) \n    return sin(x)^2 * sin(2x)^2 * pdf(Normal(0, 1), x)\nend\n\n# compute normalizing constant for normalization\nmarg_dens, error = quadgk(x -&gt; target_distribution(x), -Inf, Inf)\n# plot target density\nx = -π:0.01:π\np_base = plot(x, target_distribution.(x) ./ marg_dens, linewidth=3, label=\"Target Density\", tickfontsize=16, legendfontsize=16, guidefontsize=18, bottom_margin=5mm, left_margin=5mm)\nplot!(xlabel=L\"x\", ylabel=\"Density\")\n# pick current value\nx_current = 0.5\nvline!([x_current], color=:black, linewidth=2, label=L\"$x_t$\")\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Example target density for Metropolis-Hastings"
  },
  {
    "objectID": "slides/lecture09-1.html#proposal-distribution-examples",
    "href": "slides/lecture09-1.html#proposal-distribution-examples",
    "title": "Markov Chain Monte Carlo",
    "section": "Proposal Distribution Examples",
    "text": "Proposal Distribution Examples\n\nCode\n# plot proposal distributions\np1 = plot!(deepcopy(p_base), x, pdf.(Normal(x_current, 0.1), x) ./ 10, color=:purple, label=\"Scaled Narrow Proposal\", linewidth=2, linestyle=:dash)\np2 = plot!(deepcopy(p_base), x, pdf.(Normal(x_current, 0.5), x) ./ 2, color=:red, label=\"Scaled Wide Proposal\", linewidth=2, linestyle=:dash)\nplot!(p1, size=(600, 500))\nplot!(p2, size=(600, 500))\ndisplay(p1)\ndisplay(p2)\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Example target density for Metropolis-Hastings\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure 7"
  },
  {
    "objectID": "slides/lecture09-1.html#sampling-efficiency",
    "href": "slides/lecture09-1.html#sampling-efficiency",
    "title": "Markov Chain Monte Carlo",
    "section": "Sampling Efficiency",
    "text": "Sampling Efficiency\nTwo common measures of sampling efficiency:\n\nAcceptance Rate: Rate at which proposals are accepted\n\n“Optimally” 30-45% (depending on number of parameters)\n\nEffective Sample Size (ESS): Accounts for autocorrelation \\(\\rho_t\\) across samples \\[N_\\text{eff} = \\frac{N}{1+2\\sum_{t=1}^\\infty \\rho_t}\\]"
  },
  {
    "objectID": "slides/lecture09-1.html#sampling-efficiency-example",
    "href": "slides/lecture09-1.html#sampling-efficiency-example",
    "title": "Markov Chain Monte Carlo",
    "section": "Sampling Efficiency Example",
    "text": "Sampling Efficiency Example\n\n\n\n\nMCMC Sampling for Various Proposals"
  },
  {
    "objectID": "slides/lecture09-1.html#autocorrelation-of-chains",
    "href": "slides/lecture09-1.html#autocorrelation-of-chains",
    "title": "Markov Chain Monte Carlo",
    "section": "Autocorrelation of Chains",
    "text": "Autocorrelation of Chains\n\n\n\n\nMCMC Sampling for Various Proposals"
  },
  {
    "objectID": "slides/lecture09-1.html#ess-by-proposal-variance-for-example",
    "href": "slides/lecture09-1.html#ess-by-proposal-variance-for-example",
    "title": "Markov Chain Monte Carlo",
    "section": "ESS by Proposal Variance for Example",
    "text": "ESS by Proposal Variance for Example\n\n\n\n\nMCMC Sampling for Various Proposals"
  },
  {
    "objectID": "slides/lecture09-1.html#key-points-metropolis-hastings",
    "href": "slides/lecture09-1.html#key-points-metropolis-hastings",
    "title": "Markov Chain Monte Carlo",
    "section": "Key Points (Metropolis-Hastings)",
    "text": "Key Points (Metropolis-Hastings)\n\nConstruct ergodic and reversible Markov chains with posterior as stationary distribution.\nMetropolis-Hastings: conceptually simple algorithm, but implementation plays a major role.\nProposal distribution plays a large role in acceptance rate and effective sample size."
  },
  {
    "objectID": "slides/lecture09-1.html#next-classes",
    "href": "slides/lecture09-1.html#next-classes",
    "title": "Markov Chain Monte Carlo",
    "section": "Next Classes",
    "text": "Next Classes\nWednesday: MCMC Examples\nMonday: MCMC Lab (No exercises these weeks)\nNext Wednesday: Literature Presentations (email slides by 9pm Tuesday night)."
  },
  {
    "objectID": "slides/lecture09-1.html#assessments",
    "href": "slides/lecture09-1.html#assessments",
    "title": "Markov Chain Monte Carlo",
    "section": "Assessments",
    "text": "Assessments\n\nHomework 3: Due 3/22"
  },
  {
    "objectID": "slides/lecture09-1.html#references",
    "href": "slides/lecture09-1.html#references",
    "title": "Markov Chain Monte Carlo",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n\n\nMetropolis, N., Rosenbluth, A. W., Rosenbluth, M. N., Teller, A. H., & Teller, E. (1953). Equation of State Calculations by Fast Computing Machines. J. Chem. Phys., 21, 1087–1092. https://doi.org/10.1063/1.1699114"
  },
  {
    "objectID": "slides/lecture13-2.html#expected-out-of-sample-predictive-accuracy",
    "href": "slides/lecture13-2.html#expected-out-of-sample-predictive-accuracy",
    "title": "Other Information Criteria",
    "section": "Expected Out-Of-Sample Predictive Accuracy",
    "text": "Expected Out-Of-Sample Predictive Accuracy\nThe out-of-sample predictive fit of a new data point \\(\\tilde{y}_i\\) is\n\\[\n\\begin{align}\n\\log p_\\text{post}(\\tilde{y}_i) &= \\log \\mathbb{E}_\\text{post}\\left[p(\\tilde{y}_i | \\theta)\\right] \\\\\n&= \\log \\int p(\\tilde{y_i} | \\theta) p_\\text{post}(\\theta)\\,d\\theta.\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/lecture13-2.html#expected-out-of-sample-predictive-accuracy-1",
    "href": "slides/lecture13-2.html#expected-out-of-sample-predictive-accuracy-1",
    "title": "Other Information Criteria",
    "section": "Expected Out-Of-Sample Predictive Accuracy",
    "text": "Expected Out-Of-Sample Predictive Accuracy\nHowever, the out-of-sample data \\(\\tilde{y}_i\\) is itself unknown, so we need to compute the expected out-of-sample log-predictive density\n\\[\n\\begin{align}\n\\text{elpd} &= \\text{expected log-predictive density for } \\tilde{y}_i \\\\\n&= \\mathbb{E}_P \\left[\\log p_\\text{post}(\\tilde{y}_i)\\right] \\\\\n&= \\int \\log\\left(p_\\text{post}(\\tilde{y}_i)\\right) P(\\tilde{y}_i)\\,d\\tilde{y}.\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/lecture13-2.html#expected-out-of-sample-predictive-accuracy-2",
    "href": "slides/lecture13-2.html#expected-out-of-sample-predictive-accuracy-2",
    "title": "Other Information Criteria",
    "section": "Expected Out-Of-Sample Predictive Accuracy",
    "text": "Expected Out-Of-Sample Predictive Accuracy\nWhat is the challenge?\n\nWe don’t know \\(P\\) (the distribution of new data)!\nWe need some measure of the error induced by using an approximating distribution \\(Q\\) from some model."
  },
  {
    "objectID": "slides/lecture13-2.html#information-criteria-overview",
    "href": "slides/lecture13-2.html#information-criteria-overview",
    "title": "Other Information Criteria",
    "section": "Information Criteria Overview",
    "text": "Information Criteria Overview\nThere is a common framework for all of these:\nIf we compute the expected log-predictive density for the existing data \\(p(y | \\theta)\\), this will be too good of a fit and will overestimate the predictive skill for new data."
  },
  {
    "objectID": "slides/lecture13-2.html#information-criteria-corrections",
    "href": "slides/lecture13-2.html#information-criteria-corrections",
    "title": "Other Information Criteria",
    "section": "Information Criteria Corrections",
    "text": "Information Criteria Corrections\nWe can adjust for that bias by correcting for the effective number of parameters, which can be thought of as the expected degrees of freedom in a model contributing to overfitting."
  },
  {
    "objectID": "slides/lecture13-2.html#akaike-information-criterion-aic",
    "href": "slides/lecture13-2.html#akaike-information-criterion-aic",
    "title": "Other Information Criteria",
    "section": "Akaike Information Criterion (AIC)",
    "text": "Akaike Information Criterion (AIC)\nThe “first” information criterion that most people see.\nUses a point estimate (the maximum-likelihood estimate \\(\\hat{\\theta}_\\text{MLE}\\)) to compute the log-predictive density for the data, corrected by the number of parameters \\(k\\):\n\\[\\widehat{\\text{elpd}}_\\text{AIC} = \\log p(y | \\hat{\\theta}_\\text{MLE}) - k.\\]"
  },
  {
    "objectID": "slides/lecture13-2.html#aic-formula",
    "href": "slides/lecture13-2.html#aic-formula",
    "title": "Other Information Criteria",
    "section": "AIC Formula",
    "text": "AIC Formula\nThe AIC is defined as \\(-2\\widehat{\\text{elpd}}_\\text{AIC}\\).\nDue to this convention, lower AICs are better (they correspond to a higher predictive skill)."
  },
  {
    "objectID": "slides/lecture13-2.html#aic-correction-term",
    "href": "slides/lecture13-2.html#aic-correction-term",
    "title": "Other Information Criteria",
    "section": "AIC Correction Term",
    "text": "AIC Correction Term\nIn the case of a normal model with independent and identically-distributed data and uniform priors, \\(k\\) is the asymptotically “correct” bias term (there are modified corrections for small sample sizes).\nHowever, with more informative priors and/or hierarchical models, the bias correction \\(k\\) is no longer appropriate, as there is less “freedom” associated with each parameter."
  },
  {
    "objectID": "slides/lecture13-2.html#aic-interpretation",
    "href": "slides/lecture13-2.html#aic-interpretation",
    "title": "Other Information Criteria",
    "section": "AIC Interpretation",
    "text": "AIC Interpretation\nAbsolute AIC values have no meaning, only the differences \\(\\Delta_i = \\text{AIC}_i - \\text{AIC}_\\text{min}\\).\nSome basic rules of thumb (from Burnham & Anderson (2004)):\n\n\\(\\Delta_i &lt; 2\\) means the model has “strong” support across \\(\\mathcal{M}\\);\n\\(4 &lt; \\Delta_i &lt; 7\\) suggests “less” support;\n\\(\\Delta_i &gt; 10\\) suggests “weak” or “no” support."
  },
  {
    "objectID": "slides/lecture13-2.html#aic-and-model-evidence",
    "href": "slides/lecture13-2.html#aic-and-model-evidence",
    "title": "Other Information Criteria",
    "section": "AIC and Model Evidence",
    "text": "AIC and Model Evidence\n\\(\\exp(-\\Delta_i/2)\\) can be thought of as a measure of the likelihood of the model given the data \\(y\\).\nThe ratio \\[\\exp(-\\Delta_i/2) / \\exp(-\\Delta_j/2)\\] can approximate the relative evidence for \\(M_i\\) versus \\(M_j\\)."
  },
  {
    "objectID": "slides/lecture13-2.html#aic-and-model-averaging",
    "href": "slides/lecture13-2.html#aic-and-model-averaging",
    "title": "Other Information Criteria",
    "section": "AIC and Model Averaging",
    "text": "AIC and Model Averaging\nThis gives rise to the idea of Akaike weights: \\[w_i = \\frac{\\exp(-\\Delta_i/2)}{\\sum_{m=1}^M \\exp(-\\Delta_m/2)}.\\]\nModel projections can then be weighted based on \\(w_i\\), which can be interpreted as the probability that \\(M_i\\) is the best (in the sense of approximating the “true” predictive distribution) model in \\(\\mathcal{M}\\)."
  },
  {
    "objectID": "slides/lecture13-2.html#model-averaging-vs.-selection",
    "href": "slides/lecture13-2.html#model-averaging-vs.-selection",
    "title": "Other Information Criteria",
    "section": "Model Averaging vs. Selection",
    "text": "Model Averaging vs. Selection\nModel averaging can sometimes be beneficial vs. model selection.\nModel selection can introduce bias from the selection process (this is particularly acute for stepwise selection due to path-dependence)."
  },
  {
    "objectID": "slides/lecture13-2.html#deviance-information-criterion-dic",
    "href": "slides/lecture13-2.html#deviance-information-criterion-dic",
    "title": "Other Information Criteria",
    "section": "Deviance Information Criterion (DIC)",
    "text": "Deviance Information Criterion (DIC)\nThe Deviance Information Criterion (DIC) is a more Bayesian generalization of AIC which uses the posterior mean \\[\\hat{\\theta}_\\text{Bayes} = \\mathbb{E}\\left[\\theta | y\\right]\\] and a bias correction derived from the data."
  },
  {
    "objectID": "slides/lecture13-2.html#dic",
    "href": "slides/lecture13-2.html#dic",
    "title": "Other Information Criteria",
    "section": "DIC",
    "text": "DIC\n\\[\\widehat{\\text{elpd}}_\\text{DIC} = \\log p(y | \\hat{\\theta}_\\text{Bayes}) - p_{\\text{DIC}},\\] where \\[p_\\text{DIC} = 2\\left(\\log p(y | \\hat{\\theta}_\\text{Bayes}) - \\mathbb{E}_\\text{post}\\left[\\log p(y | \\theta)\\right]\\right).\\]\nThen, as with AIC, \\[\\text{DIC} = -2\\widehat{\\text{elpd}}_\\text{DIC}.\\]"
  },
  {
    "objectID": "slides/lecture13-2.html#dic-effective-number-of-parameters",
    "href": "slides/lecture13-2.html#dic-effective-number-of-parameters",
    "title": "Other Information Criteria",
    "section": "DIC: Effective Number of Parameters",
    "text": "DIC: Effective Number of Parameters\nWhat is the meaning of \\(p_\\text{DIC}\\)?\n\nThe difference between the average log-likelihood (across parameters) and the log-likelihood at a parameter average measures “degrees of freedom”.\nThe DIC adjustment assumes independence of residuals for fixed \\(\\theta\\)."
  },
  {
    "objectID": "slides/lecture13-2.html#aic-vs.-dic",
    "href": "slides/lecture13-2.html#aic-vs.-dic",
    "title": "Other Information Criteria",
    "section": "AIC vs. DIC",
    "text": "AIC vs. DIC\nAIC and DIC often give similar results, but don’t have to.\nThe key difference is the impact of priors on parameter estimation and model degrees of freedom."
  },
  {
    "objectID": "slides/lecture13-2.html#aic-vs.-dic-storm-surge-example",
    "href": "slides/lecture13-2.html#aic-vs.-dic-storm-surge-example",
    "title": "Other Information Criteria",
    "section": "AIC vs. DIC Storm Surge Example",
    "text": "AIC vs. DIC Storm Surge Example\nModels:\n\nStationary (“null”) model, \\(y_t \\sim \\text{GEV}(\\mu, \\sigma, \\xi);\\)\nTime nonstationary (“null-ish”) model, \\(y_t \\sim \\text{GEV}(\\mu_0 + \\mu_1 t, \\sigma, \\xi);\\)\nPDO nonstationary model, \\(y_t \\sim \\text{GEV}(\\mu_0 + \\mu_1 \\text{PDO}_t, \\sigma, \\xi)\\)"
  },
  {
    "objectID": "slides/lecture13-2.html#aic-vs.-dic-1",
    "href": "slides/lecture13-2.html#aic-vs.-dic-1",
    "title": "Other Information Criteria",
    "section": "AIC vs. DIC",
    "text": "AIC vs. DIC\n\n\nCode\n# fit models\nstat_chain = sample(stat_mod, NUTS(), MCMCThreads(), 10_000, 4)\nnonstat_chain = sample(nonstat_mod, NUTS(), MCMCThreads(), 10_000, 4)\npdo_chain = sample(pdo_mod, NUTS(), MCMCThreads(), 10_000, 4)\n\nstat_est = mean(stat_chain)[:, 2]\nnonstat_est = mean(nonstat_chain)[:, 2]\npdo_est = mean(pdo_chain)[:, 2]\n\nstat_bayes = loglikelihood(stat_mod, (μ = stat_est[1], σ = stat_est[2], ξ = stat_est[3]))\nnonstat_bayes = loglikelihood(nonstat_mod, (a = nonstat_est[1], b = nonstat_est[2], σ = nonstat_est[3], ξ = nonstat_est[4]))\npdo_bayes = loglikelihood(pdo_mod, (a = pdo_est[1], b = pdo_est[2], σ = pdo_est[3], ξ = pdo_est[4]))\n\nstat_ll = mean([loglikelihood(stat_mod, row) for row in eachrow(DataFrame(stat_chain))]) \nnonstat_ll = mean([loglikelihood(nonstat_mod, row) for row in eachrow(DataFrame(nonstat_chain))])\npdo_ll = mean([loglikelihood(pdo_mod, row) for row in eachrow(DataFrame(pdo_chain))])\n\nstat_dic = 2 * stat_ll - stat_bayes\nnonstat_dic = 2 * nonstat_ll - nonstat_bayes\npdo_dic = 2 * pdo_ll - pdo_bayes\n\nmodel_dic = DataFrame(Model=[\"Stationary\", \"Time\", \"PDO\"], AIC=trunc.(Int64, round.(-2 * [stat_aic, nonstat_aic, pdo_aic]; digits=0)), DIC=trunc.(Int64, round.(-2 * [stat_dic, nonstat_dic, pdo_dic]; digits=0)))\n\n\n\n3×3 DataFrame\n\n\n\nRow\nModel\nAIC\nDIC\n\n\n\nString\nInt64\nInt64\n\n\n\n\n1\nStationary\n1421\n1421\n\n\n2\nTime\n1413\n1413\n\n\n3\nPDO\n1418\n1419"
  },
  {
    "objectID": "slides/lecture13-2.html#convergence-of-aic-and-dic",
    "href": "slides/lecture13-2.html#convergence-of-aic-and-dic",
    "title": "Other Information Criteria",
    "section": "Convergence of AIC and DIC",
    "text": "Convergence of AIC and DIC\nBoth AIC and DIC converge (as \\(n \\to \\infty\\)) to expected leave-one-out CV.\nThe question is how well they do under limited sample sizes!"
  },
  {
    "objectID": "slides/lecture13-2.html#watanabe-akaike-information-criterion-waic",
    "href": "slides/lecture13-2.html#watanabe-akaike-information-criterion-waic",
    "title": "Other Information Criteria",
    "section": "Watanabe-Akaike Information Criterion (WAIC)",
    "text": "Watanabe-Akaike Information Criterion (WAIC)\n\\[\\widehat{\\text{elpd}}_\\text{WAIC} = \\sum_{i=1}^n \\log \\int p(y_i | \\theta) p_\\text{post}(\\theta)\\,d\\theta - p_{\\text{WAIC}},\\]\nwhere \\[p_\\text{WAIC} = \\sum_{i=1}^n \\text{Var}_\\text{post}\\left(\\log p(y_i | \\theta)\\right).\\]"
  },
  {
    "objectID": "slides/lecture13-2.html#waic-correction-factor",
    "href": "slides/lecture13-2.html#waic-correction-factor",
    "title": "Other Information Criteria",
    "section": "WAIC Correction Factor",
    "text": "WAIC Correction Factor\n\\(p_\\text{WAIC}\\) is an estimate of the number of “unconstrained” parameters in the model.\n\nA parameter counts as 1 if its estimate is “independent” of the prior;\nA parameter counts as 0 if it is fully constrained by the prior.\nA parameter gives a partial value if both the data and prior are informative."
  },
  {
    "objectID": "slides/lecture13-2.html#waic-vs.-aic-and-dic",
    "href": "slides/lecture13-2.html#waic-vs.-aic-and-dic",
    "title": "Other Information Criteria",
    "section": "WAIC vs. AIC and DIC",
    "text": "WAIC vs. AIC and DIC\n\nWAIC can be viewed as an approximation to leave-one-out CV, and averages over the entire posterior, vs. AIC and DIC which use point estimates.\nBut it doesn’t work well with highly structured data; no real alternative to more clever uses of Bayesian cross-validation."
  },
  {
    "objectID": "slides/lecture13-2.html#bayesian-information-criterion-bic",
    "href": "slides/lecture13-2.html#bayesian-information-criterion-bic",
    "title": "Other Information Criteria",
    "section": "“Bayesian” “Information” Criterion (BIC)",
    "text": "“Bayesian” “Information” Criterion (BIC)\n\\[\\text{BIC} = -2\\log p(y | \\hat{\\theta}_\\text{MLE}) + k\\log n.\\]\nBIC:\n\nIs not Bayesian (it relies on the MLE);\nHas no relationship to information theory (unlike AIC/DIC);\nAssumes \\(\\mathcal{M}\\)-closed (e.g. that the true model is under consideration)."
  },
  {
    "objectID": "slides/lecture13-2.html#bic",
    "href": "slides/lecture13-2.html#bic",
    "title": "Other Information Criteria",
    "section": "BIC",
    "text": "BIC\nBIC approximates the prior log-predictive likelihood and leave-\\(k\\)-out cross-validation (hence the extra penalization for additional parameters).\nThis is why it’s odd when model selection consists of examining both AIC and BIC: these are different quantities with different purposes!"
  },
  {
    "objectID": "slides/lecture13-2.html#key-takeaways",
    "href": "slides/lecture13-2.html#key-takeaways",
    "title": "Other Information Criteria",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nInformation Criteria are an approximation to LOO-CV based on “correcting” for model complexity.\nAIC and DIC can be used to approximate LOO-CV.\nBIC is an entirely different measure, approximating the prior predictive distribution (leave-\\(k\\)-out CV)."
  },
  {
    "objectID": "slides/lecture13-2.html#an-important-caveat",
    "href": "slides/lecture13-2.html#an-important-caveat",
    "title": "Other Information Criteria",
    "section": "An Important Caveat",
    "text": "An Important Caveat\nModel selection can result in significant overfitting when separated from hypothesis-driven model development (Freedman, 1983; Smith, 2018)"
  },
  {
    "objectID": "slides/lecture13-2.html#an-important-caveat-1",
    "href": "slides/lecture13-2.html#an-important-caveat-1",
    "title": "Other Information Criteria",
    "section": "An Important Caveat",
    "text": "An Important Caveat\n\nBetter off thinking about the scientific or engineering problem you want to solve and use domain knowledge/checks rather than throwing a large number of possible models into the selection machinery.\nRegularizing priors reduce potential for overfitting.\nModel averaging (Hoeting et al., 2021) and stacking (Yao et al., 2018) can combine multiple models as an alternative to selection."
  },
  {
    "objectID": "slides/lecture13-2.html#next-classes",
    "href": "slides/lecture13-2.html#next-classes",
    "title": "Other Information Criteria",
    "section": "Next Classes",
    "text": "Next Classes\nNext Week: Emulating Complex Models"
  },
  {
    "objectID": "slides/lecture13-2.html#references-1",
    "href": "slides/lecture13-2.html#references-1",
    "title": "Other Information Criteria",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n\n\nBurnham, K. P., & Anderson, D. R. (2004). Multimodel Inference: Understanding AIC and BIC in Model Selection. Sociol. Methods Res., 33, 261–304. https://doi.org/10.1177/0049124104268644\n\n\nFreedman, D. A. (1983). A note on screening regression equations. Am. Stat., 37, 152. https://doi.org/10.2307/2685877\n\n\nHoeting, J. A., Madigan, D., Raftery, A. E., & Volinsky, C. T. (2021). Bayesian model averaging: a tutorial. Stat. Sci., 14, 382–401. https://doi.org/10.1214/ss/1009212519\n\n\nSmith, G. (2018). Step away from stepwise. J. Big Data, 5, 1–12. https://doi.org/10.1186/s40537-018-0143-6\n\n\nYao, Y., Vehtari, A., Simpson, D., & Gelman, A. (2018). Using stacking to average Bayesian predictive distributions. arXiv [Stat.ME]. https://doi.org/10.1214/17-BA1091"
  },
  {
    "objectID": "slides/lecture08-2.html#the-bootstrap",
    "href": "slides/lecture08-2.html#the-bootstrap",
    "title": "Bayesian Computation",
    "section": "The Bootstrap",
    "text": "The Bootstrap\n\nBootstrap Principle: Use the data as a proxy for the population.\nKey: Bootstrap gives idea of sampling error in statistics (including model parameters)\nDistribution of \\(\\tilde{t} - \\hat{t}\\) approximates distribution around estimate \\(\\hat{t} - t_0\\).\nParametric bootstrap introduces model specification error"
  },
  {
    "objectID": "slides/lecture08-2.html#bootstrap-variants",
    "href": "slides/lecture08-2.html#bootstrap-variants",
    "title": "Bayesian Computation",
    "section": "Bootstrap Variants",
    "text": "Bootstrap Variants\n\nResample Cases (Non-Parametric)\nResample Residuals (Semi-Parametric)\nSimulate from Fitted Model (Parametric)"
  },
  {
    "objectID": "slides/lecture08-2.html#which-bootstrap-to-use",
    "href": "slides/lecture08-2.html#which-bootstrap-to-use",
    "title": "Bayesian Computation",
    "section": "Which Bootstrap To Use?",
    "text": "Which Bootstrap To Use?\n\nBias-Variance Tradeoff: Parametric Bootstrap has narrowest intervals, Resampling Cases widest (Exercise 8)\nDepends on trust in model “correctness”:\n\nDo we trust the model parameters to be “correct”?\nDo we trust the shape of the regression model?\nDo we trust the data-generating process?"
  },
  {
    "objectID": "slides/lecture08-2.html#reminder-bayesian-modeling",
    "href": "slides/lecture08-2.html#reminder-bayesian-modeling",
    "title": "Bayesian Computation",
    "section": "Reminder: Bayesian Modeling",
    "text": "Reminder: Bayesian Modeling\nProbability is the degree of belief in a “proposition”.\nThen it makes sense to discuss the probability conditional on observations \\(\\mathbf{y}\\) of\n\nmodel parameters \\(\\mathbf{\\theta}\\)\nunobserved data \\(\\tilde{\\mathbf{y}}\\)\n\n\\[p(\\mathbf{\\theta} | \\mathbf{y}) \\text{ or } p(\\tilde{\\mathbf{y}} | \\mathbf{y})\\]"
  },
  {
    "objectID": "slides/lecture08-2.html#conditioning-on-observations",
    "href": "slides/lecture08-2.html#conditioning-on-observations",
    "title": "Bayesian Computation",
    "section": "Conditioning on Observations",
    "text": "Conditioning on Observations\nThis fundamental conditioning on observations \\(\\mathbf{y}\\) is a distinguishing feature of Bayesian inference.\nCompare: frequentist approaches are based on re-estimated over the distribution of possible \\(\\mathbf{y}\\) conditional on the “true” parameter value."
  },
  {
    "objectID": "slides/lecture08-2.html#bayes-rule",
    "href": "slides/lecture08-2.html#bayes-rule",
    "title": "Bayesian Computation",
    "section": "Bayes’ Rule",
    "text": "Bayes’ Rule\nUpdate priors with Bayes’ Rule:\n\\[\\underbrace{{p(\\theta | y)}}_{\\text{posterior}} = \\frac{\\overbrace{p(y | \\theta)}^{\\text{likelihood}}}{\\underbrace{p(y)}_\\text{normalization}} \\overbrace{p(\\theta)}^\\text{prior}\\]"
  },
  {
    "objectID": "slides/lecture08-2.html#goals-of-bayesian-computation",
    "href": "slides/lecture08-2.html#goals-of-bayesian-computation",
    "title": "Bayesian Computation",
    "section": "Goals of Bayesian Computation",
    "text": "Goals of Bayesian Computation\n\nSampling from the posterior distribution \\[p(\\theta | \\mathbf{y})\\]\nSampling from the posterior predictive distribution \\[p(\\tilde{y} | \\mathbf{y})\\] by generating data."
  },
  {
    "objectID": "slides/lecture08-2.html#bayesian-computation-and-monte-carlo",
    "href": "slides/lecture08-2.html#bayesian-computation-and-monte-carlo",
    "title": "Bayesian Computation",
    "section": "Bayesian Computation and Monte Carlo",
    "text": "Bayesian Computation and Monte Carlo\nBayesian computation involves Monte Carlo simulation from the posterior (predictive) distribution.\nThese samples can then be analyzed to identify estimators, credible intervals, etc."
  },
  {
    "objectID": "slides/lecture08-2.html#posterior-sampling",
    "href": "slides/lecture08-2.html#posterior-sampling",
    "title": "Bayesian Computation",
    "section": "Posterior Sampling",
    "text": "Posterior Sampling\nTrivial for extremely simple problems:\n\nlow-dimensional.\nwith “conjugate” priors (which make the posterior a closed-form distribution).\n\nFor example: normal likelihood, normal prior ⇒ normal posterior"
  },
  {
    "objectID": "slides/lecture08-2.html#a-first-algorithm-rejection-sampling",
    "href": "slides/lecture08-2.html#a-first-algorithm-rejection-sampling",
    "title": "Bayesian Computation",
    "section": "A First Algorithm: Rejection Sampling",
    "text": "A First Algorithm: Rejection Sampling\n\n\nIdea:\n\nGenerate proposed samples from another distribution \\(g(\\theta)\\) which covers the target \\(p(\\theta | \\mathbf{y})\\);\nAccept those proposals based on the ratio of the two distributions.\n\n\n\n\n\nProposal Distribution for Rejection Sampling"
  },
  {
    "objectID": "slides/lecture08-2.html#rejection-sampling-algorithm",
    "href": "slides/lecture08-2.html#rejection-sampling-algorithm",
    "title": "Bayesian Computation",
    "section": "Rejection Sampling Algorithm",
    "text": "Rejection Sampling Algorithm\nSuppose \\(p(\\theta | \\mathbf{y}) \\leq M g(\\theta)\\) for some \\(1 &lt; M &lt; \\infty\\).\n\nSimulate \\(u \\sim \\text{Unif}(0, 1)\\).\nSimulate a proposal \\(\\hat{\\theta} \\sim g(\\theta)\\).\nIf \\(u &lt; \\frac{p(\\hat{\\theta} | \\mathbf{y})}{Mg(\\hat{\\theta})},\\) accept \\(\\hat{\\theta}\\). Otherwise reject."
  },
  {
    "objectID": "slides/lecture08-2.html#rejection-sampling-challenges",
    "href": "slides/lecture08-2.html#rejection-sampling-challenges",
    "title": "Bayesian Computation",
    "section": "Rejection Sampling Challenges",
    "text": "Rejection Sampling Challenges\n\nProbability of accepting a sample is \\(1/M\\), so the “tighter” the proposal distribution coverage the more efficient the sampler.\nNeed to be able to compute \\(M\\).\n\nFinding a good proposal and computing \\(M\\) may not be easy (or possible) for complex posteriors!\nHow can we do better?"
  },
  {
    "objectID": "slides/lecture08-2.html#how-can-we-do-better",
    "href": "slides/lecture08-2.html#how-can-we-do-better",
    "title": "Bayesian Computation",
    "section": "How Can We Do Better?",
    "text": "How Can We Do Better?\nThe fundamental problem with rejection sampling is that we don’t know the properties of the posterior. So we don’t know that we have the appropriate coverage. But…\nWhat if we could construct an proposal/acceptance/rejection scheme that necessarily converged to the target distribution, even without a priori knowledge of its properties?"
  },
  {
    "objectID": "slides/lecture08-2.html#what-is-a-markov-chain",
    "href": "slides/lecture08-2.html#what-is-a-markov-chain",
    "title": "Bayesian Computation",
    "section": "What Is A Markov Chain?",
    "text": "What Is A Markov Chain?\n\n\nConsider a stochastic process \\(\\{X_t\\}_{t \\in \\mathcal{T}}\\), where\n\n\\(X_t \\in \\mathcal{S}\\) is the state at time \\(t\\), and\n\\(\\mathcal{T}\\) is a time-index set (can be discrete or continuous)\n\\(\\mathbb{P}(s_i \\to s_j) = p_{ij}\\).\n\n\n\n\n\nMarkov State Space"
  },
  {
    "objectID": "slides/lecture08-2.html#markovian-property",
    "href": "slides/lecture08-2.html#markovian-property",
    "title": "Bayesian Computation",
    "section": "Markovian Property",
    "text": "Markovian Property\nThis stochastic process is a Markov chain if it satisfies the Markovian (or memoryless) property: \\[\\begin{align*}\n\\mathbb{P}(X_{T+1} = s_i &| X_1=x_1, \\ldots, X_T=x_T) = \\\\ &\\qquad\\mathbb{P}(X_{T+1} = s_i| X_T=x_T)\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/lecture08-2.html#example-drunkards-walk",
    "href": "slides/lecture08-2.html#example-drunkards-walk",
    "title": "Bayesian Computation",
    "section": "Example: “Drunkard’s Walk”",
    "text": "Example: “Drunkard’s Walk”\n\n\n\n\n:img Random Walk, 80%\n\n\n\n\n\nHow can we model the unconditional probability \\(\\mathbb{P}(X_T = s_i)\\)?\nHow about the conditional probability \\(\\mathbb{P}(X_T = s_i | X_{T-1} = x_{T-1})\\)?"
  },
  {
    "objectID": "slides/lecture08-2.html#example-weather",
    "href": "slides/lecture08-2.html#example-weather",
    "title": "Bayesian Computation",
    "section": "Example: Weather",
    "text": "Example: Weather\nLet’s look at a more interesting example. Suppose the weather can be foggy, sunny, or rainy.\nBased on past experience, we know that:\n\nThere are never two sunny days in a row;\nEven chance of two foggy or two rainy days in a row;\nA sunny day occurs 1/4 of the time after a foggy or rainy day."
  },
  {
    "objectID": "slides/lecture08-2.html#aside-higher-order-markov-chains",
    "href": "slides/lecture08-2.html#aside-higher-order-markov-chains",
    "title": "Bayesian Computation",
    "section": "Aside: Higher Order Markov Chains",
    "text": "Aside: Higher Order Markov Chains\nSuppose that today’s weather depends on the prior two days.\n\n\nCan we write this as a Markov chain?\nWhat are the states?"
  },
  {
    "objectID": "slides/lecture08-2.html#weather-transition-matrix",
    "href": "slides/lecture08-2.html#weather-transition-matrix",
    "title": "Bayesian Computation",
    "section": "Weather Transition Matrix",
    "text": "Weather Transition Matrix\nWe can summarize these probabilities in a transition matrix \\(P\\): \\[\nP =\n\\begin{array}{cc}\n\\begin{array}{ccc}\n\\phantom{i}\\color{red}{F}\\phantom{i} & \\phantom{i}\\color{red}{S}\\phantom{i} & \\phantom{i}\\color{red}{R}\\phantom{i}\n\\end{array}\n\\\\\n\\begin{pmatrix}\n      1/2 & 1/4 & 1/4 \\\\\n      1/2 & 0 & 1/2 \\\\\n      1/4 & 1/4 & 1/2\n      \\end{pmatrix}\n&\n\\begin{array}{ccc}\n\\color{red}F  \\\\ \\color{red}S  \\\\ \\color{red}R\n\\end{array}   \n\\end{array}\n\\]\nRows are the current state, columns are the next step, so \\(\\sum_i p_{ij} = 1\\)."
  },
  {
    "objectID": "slides/lecture08-2.html#weather-example-state-probabilities",
    "href": "slides/lecture08-2.html#weather-example-state-probabilities",
    "title": "Bayesian Computation",
    "section": "Weather Example: State Probabilities",
    "text": "Weather Example: State Probabilities\nDenote by \\(\\lambda^t\\) a probability distribution over the states at time \\(t\\).\nThen \\(\\lambda^t = \\lambda^{t-1}P\\):\n\\[\\begin{pmatrix}\\lambda^t_F & \\lambda^t_S & \\lambda^t_R \\end{pmatrix} =  \n\\begin{pmatrix}\\lambda^{t-1}_F & \\lambda^{t-1}_S & \\lambda^{t-1}_R \\end{pmatrix}\n      \\begin{pmatrix}\n      1/2 & 1/4 & 1/4 \\\\\n      1/2 & 0 & 1/2 \\\\\n      1/4 & 1/4 & 1/2\n      \\end{pmatrix}\n\\]"
  },
  {
    "objectID": "slides/lecture08-2.html#multi-transition-probabilities",
    "href": "slides/lecture08-2.html#multi-transition-probabilities",
    "title": "Bayesian Computation",
    "section": "Multi-Transition Probabilities",
    "text": "Multi-Transition Probabilities\nNotice that \\[\\lambda^{t+i} = \\lambda^t P^i,\\] so multiple transition probabilities are \\(P\\)-exponentials.\n\\[P^3 =\n\\begin{array}{cc}\n\\begin{array}{ccc}\n\\phantom{iii}\\color{red}{F}\\phantom{ii} & \\phantom{iii}\\color{red}{S}\\phantom{iii} & \\phantom{ii}\\color{red}{R}\\phantom{iii}\n\\end{array}\n\\\\\n\\begin{pmatrix}\n      26/64 & 13/64 & 25/64 \\\\\n      26/64 & 12/64 & 26/64 \\\\\n      26/64 & 13/64 & 26/64\n      \\end{pmatrix}\n&\n\\begin{array}{ccc}\n\\color{red}F  \\\\ \\color{red}S  \\\\ \\color{red}R\n\\end{array}   \n\\end{array}\n\\]"
  },
  {
    "objectID": "slides/lecture08-2.html#long-run-probabilities",
    "href": "slides/lecture08-2.html#long-run-probabilities",
    "title": "Bayesian Computation",
    "section": "Long Run Probabilities",
    "text": "Long Run Probabilities\nWhat happens if we let the system run for a while starting from an initial sunny day?\n\n\nCode\ncurrent = [1.0, 0.0, 0.0]\nP = [1/2 1/4 1/4\n    1/2 0 1/2\n    1/4 1/4 1/2]   \n\nT = 21\n\nstate_probs = zeros(T, 3)\nstate_probs[1,:] = current\nfor t=1:T-1\n    state_probs[t+1, :] = state_probs[t:t, :] * P\nend\n\n\np = plot(0:T-1, state_probs, label=[\"Foggy\" \"Sunny\" \"Rainy\"], palette=:mk_8, linewidth=3, tickfontsize=16, guidefontsize=18, legendfontsize=16, left_margin=5mm, bottom_margin=10mm)\nxlabel!(\"Time\")\nylabel!(\"State Probability\")\nplot!(p, size=(1000, 350))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: State probabilities for the weather examples."
  },
  {
    "objectID": "slides/lecture08-2.html#stationary-distributions",
    "href": "slides/lecture08-2.html#stationary-distributions",
    "title": "Bayesian Computation",
    "section": "Stationary Distributions",
    "text": "Stationary Distributions\nThis stabilization always occurs when the probability distribution is an eigenvector of \\(P\\) with eigenvalue 1:\n\\[\\pi = \\pi P.\\]\nThis is called an invariant or a stationary distribution."
  },
  {
    "objectID": "slides/lecture08-2.html#what-markov-chains-have-stationary-distributions",
    "href": "slides/lecture08-2.html#what-markov-chains-have-stationary-distributions",
    "title": "Bayesian Computation",
    "section": "What Markov Chains Have Stationary Distributions?",
    "text": "What Markov Chains Have Stationary Distributions?\nNot necessarily! The key is two properties:\n\nIrreducible\nAperiodicity"
  },
  {
    "objectID": "slides/lecture08-2.html#irreducibility",
    "href": "slides/lecture08-2.html#irreducibility",
    "title": "Bayesian Computation",
    "section": "Irreducibility",
    "text": "Irreducibility\nA Markov chain is irreducible if every state is accessible from every other state, e.g. for every pair of states \\(s_i\\) and \\(s_j\\) there is some \\(k &gt; 0\\) such that \\(P_{ij}^k &gt; 0.\\)\n\nReducible Markov Chain"
  },
  {
    "objectID": "slides/lecture08-2.html#aperiodicity",
    "href": "slides/lecture08-2.html#aperiodicity",
    "title": "Bayesian Computation",
    "section": "Aperiodicity",
    "text": "Aperiodicity\nThe period of a state \\(s_i\\) is the greatest common divisor \\(k\\) of all \\(t\\) such that \\(P^t_{ii} &gt; 0\\).\nIn other words, if a state \\(s_i\\) has period \\(k\\), all returns must occur after time steps which are multiples of \\(k\\).\n\n\nA Markov chain is aperiodic if all states have period 1.\n\n\n\n\nPeriodic Markov Chain"
  },
  {
    "objectID": "slides/lecture08-2.html#ergodicity",
    "href": "slides/lecture08-2.html#ergodicity",
    "title": "Bayesian Computation",
    "section": "Ergodicity",
    "text": "Ergodicity\nA Markov chain is ergodic if it is aperiodic and irreducible.\nErgodic Markov chains have a limiting distribution which is the limit of the time-evolution of the chain dynamics, e.g. \\[\\pi_j = \\lim_{t \\to \\infty} \\mathbb{P}(X_t = s_j).\\]\nKey: The limiting distribution limit is independent of the initial state probability."
  },
  {
    "objectID": "slides/lecture08-2.html#ergodicity-1",
    "href": "slides/lecture08-2.html#ergodicity-1",
    "title": "Bayesian Computation",
    "section": "Ergodicity",
    "text": "Ergodicity\n\\[\\pi_j = \\lim_{t \\to \\infty} \\mathbb{P}(X_t = s_j).\\]\nIntuition: Ergodicity means we can exchange thinking about time-averages and ensemble-averages."
  },
  {
    "objectID": "slides/lecture08-2.html#limiting-distributions-are-stationary",
    "href": "slides/lecture08-2.html#limiting-distributions-are-stationary",
    "title": "Bayesian Computation",
    "section": "Limiting Distributions are Stationary",
    "text": "Limiting Distributions are Stationary\nFor an ergodic chain, the limiting distribution is the unique stationary distribution (we won’t prove uniqueness):\n\\[\\begin{align}\n\\pi_j &= \\lim_{t \\to \\infty} \\mathbb{P}(X_t = s_j | X_0 = s_i) \\\\\n&= \\lim_{t \\to \\infty} (P^{t+1})_{ij} = \\lim_{t \\to \\infty} (P^tP)_{ij} \\\\\n&= \\lim_{t \\to \\infty} \\sum_d (P^t)_{id} P_{dj} \\\\\n&= \\sum_d \\pi_d P_{dj}\n\\end{align}\\]"
  },
  {
    "objectID": "slides/lecture08-2.html#transient-portion-of-the-chain",
    "href": "slides/lecture08-2.html#transient-portion-of-the-chain",
    "title": "Bayesian Computation",
    "section": "Transient Portion of the Chain",
    "text": "Transient Portion of the Chain\nThe portion of the chain prior to convergence to the stationary distribution is called the transient portion.\n\n\nCode\nvspan!(p, [0, 4], color=:red, alpha=0.3, label=\"Transient Portion\")\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Transient portion of the weather Markov chain."
  },
  {
    "objectID": "slides/lecture08-2.html#detailed-balance",
    "href": "slides/lecture08-2.html#detailed-balance",
    "title": "Bayesian Computation",
    "section": "Detailed Balance",
    "text": "Detailed Balance\nThe last important concept is detailed balance.\nLet \\(\\{X_t\\}\\) be a Markov chain and let \\(\\pi\\) be a probability distribution over the states. Then the chain is in detailed balance with respect to \\(\\pi\\) if \\[\\pi_i P_{ij} = \\pi_j P_{ji}.\\]\n\nDetailed balance implies reversibility: the chain’s dynamics are the same when viewed forwards or backwards in time."
  },
  {
    "objectID": "slides/lecture08-2.html#detailed-balance-intuition",
    "href": "slides/lecture08-2.html#detailed-balance-intuition",
    "title": "Bayesian Computation",
    "section": "Detailed Balance Intuition",
    "text": "Detailed Balance Intuition\nA nice analogy (from Miranda Holmes-Cerfon) is traffic flow.\n\n\nConsider NYC and its surroundings: each borough/region can be thought of as a node, and population transitions occur across bridges/tunnels.\n\n\n\n\nNew York City Graph"
  },
  {
    "objectID": "slides/lecture08-2.html#detailed-balance-stationary-distributions",
    "href": "slides/lecture08-2.html#detailed-balance-stationary-distributions",
    "title": "Bayesian Computation",
    "section": "Detailed Balance: Stationary Distributions",
    "text": "Detailed Balance: Stationary Distributions\nDetailed balance is a sufficient but not necessary condition for the existence of a stationary distribution (namely \\(\\pi\\)):\n\\[\\begin{align*}\n(\\pi P)_i &= \\sum_j \\pi_j P_{ji} \\\\\n&= \\sum_j \\pi_i P_{ij} \\\\\n&= \\pi_i \\sum_j P_{ij} = \\pi_i\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/lecture08-2.html#idea-of-sampling-algorithm",
    "href": "slides/lecture08-2.html#idea-of-sampling-algorithm",
    "title": "Bayesian Computation",
    "section": "Idea of Sampling Algorithm",
    "text": "Idea of Sampling Algorithm\nThe idea of our sampling algorithm (which we will discuss next time) is to construct an ergodic Markov chain from the detailed balance equation for the target distribution.\n\nDetailed balance implies that the target distribution is the stationary distribution.\nErgodicity implies that this distribution is unique and can be obtained as the limiting distribution of the chain’s dynamics."
  },
  {
    "objectID": "slides/lecture08-2.html#idea-of-sampling-algorithm-1",
    "href": "slides/lecture08-2.html#idea-of-sampling-algorithm-1",
    "title": "Bayesian Computation",
    "section": "Idea of Sampling Algorithm",
    "text": "Idea of Sampling Algorithm\nIn other words:\n\nGenerate an appropriate Markov chain so that its stationary distribution of the target distribution \\(\\pi\\);\nRun its dynamics long enough to converge to the stationary distribution;\nUse the resulting ensemble of states as Monte Carlo samples from \\(\\pi\\) ."
  },
  {
    "objectID": "slides/lecture08-2.html#sampling-algorithm",
    "href": "slides/lecture08-2.html#sampling-algorithm",
    "title": "Bayesian Computation",
    "section": "Sampling Algorithm",
    "text": "Sampling Algorithm\nAny algorithm which follows this procedure is a Markov chain Monte Carlo algorithm.\nGood news: These algorithms are designed to work quite generally, without (usually) having to worry about technical details like detailed balance and ergodicity.\nBad news: They can involve quite a bit of tuning for computational efficiency. Some algorithms or implementations are faster/adaptive to reduce this need."
  },
  {
    "objectID": "slides/lecture08-2.html#sampling-algorithm-1",
    "href": "slides/lecture08-2.html#sampling-algorithm-1",
    "title": "Bayesian Computation",
    "section": "Sampling Algorithm",
    "text": "Sampling Algorithm\nAnnoying news:\n\nConvergence to the stationary distribution is only guaranteed asymptotically; evaluating if the chain has been run long enough requires lots of heuristics.\nDue to Markovian property, samples are dependent, so smaller “effective sample size”."
  },
  {
    "objectID": "slides/lecture08-2.html#key-points",
    "href": "slides/lecture08-2.html#key-points",
    "title": "Bayesian Computation",
    "section": "Key Points",
    "text": "Key Points\n\nBayesian computation is difficult because we need to sample from effectively arbitrary distributions.\nMarkov chains provide a path forward if we can construct a chain satisfying detailed balance whose stationary distribution is the target distribution.\nThen a post-convergence chain of samples is the same as a dependent Monte Carlo set of samples."
  },
  {
    "objectID": "slides/lecture08-2.html#next-classes",
    "href": "slides/lecture08-2.html#next-classes",
    "title": "Bayesian Computation",
    "section": "Next Classes",
    "text": "Next Classes\nNext Week: Markov chain Monte Carlo"
  },
  {
    "objectID": "slides/lecture08-2.html#assessments",
    "href": "slides/lecture08-2.html#assessments",
    "title": "Bayesian Computation",
    "section": "Assessments",
    "text": "Assessments\n\nExercise 8: Due Friday\nHomework 3: Due 3/22"
  },
  {
    "objectID": "slides/lecture08-2.html#references-1",
    "href": "slides/lecture08-2.html#references-1",
    "title": "Bayesian Computation",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "policies/homework.html",
    "href": "policies/homework.html",
    "title": "Homework Policies",
    "section": "",
    "text": "This page includes some information on the homework assignments for BEE 4850/5850, including policies and logistics. The goal is to help you get as many points on your homework as possible and to return them as soon as possible.\nThis document is long, but please do read the whole thing. Hopefully most of this is obvious, but some of it may not be.",
    "crumbs": [
      "Course Information",
      "Homework Policies"
    ]
  },
  {
    "objectID": "policies/homework.html#homework-logistics",
    "href": "policies/homework.html#homework-logistics",
    "title": "Homework Policies",
    "section": "Homework Logistics",
    "text": "Homework Logistics\nHomework assignments will be assigned on Monday and are generally due two Fridays hence (so you have two “school weeks” to work on the assignment). Solutions should be submitted as PDFs to Gradescope by 9pm on the due date. You do not need to include code unless you want to, but you should always include some description of the logic of the modeling or problem-solving process that your code implements.",
    "crumbs": [
      "Course Information",
      "Homework Policies"
    ]
  },
  {
    "objectID": "policies/homework.html#how-to-write-assignments",
    "href": "policies/homework.html#how-to-write-assignments",
    "title": "Homework Policies",
    "section": "How To Write Assignments",
    "text": "How To Write Assignments\nHere are some tips for how to make the grader (the TA or Prof. Srikrishnan) understand what you mean in the time they’re looking at your problem: as noted in the rubrics, if your solution isn’t clear enough for us to follow, you will not receive the points even if your answer is technically correct, because part of demonstrating understanding is the ability to organize and communicate.\n\nBe Honest\n\nWrite everything in your own words. It’s perfectly ok to work with others; in fact, we encourage it! But you should write up and implement everything yourself; it’s very easy to convince yourself that you understand something when you’re mimicking someone else’s solution, and often we realize we don’t actually understand something when we try to write it ourselves.\nCite every outside source you use. You are allowed, in fact encouraged, to use any outside source1 Getting an idea from an outside source is not a problem, and will not lower your grade; if you’re critically evaluating the idea and implementing and writing your solution yourself (see above), then you’re demonstrating understanding even if the idea originated with someone else. But you must give appropriate credit to that source. Taking credit for someone else’s ideas and failing to properly cite an outside source, including your classmates, is plagiarism, and will be treated accordingly.\nThe only sources you do not have to cite are official class materials. If you use the lectures, lecture notes, website materials, homework solutions, etc, you do not have to cite these.\nList everyone that you worked with. Give your classmates proper credit for their assistance. If you get an idea from Ed Discussion, credit the poster. If you’re not sure if you should list someone as a collaborator, err on the side of including them. For discussions in class or in office hours, you don’t have to list everyone who participated in the discussion (though you should if you worked one on one with them), but mention that the class discussion or the office hour was useful.\n\n1 Yes, including ChatGPT, though you should ask and describe how you used it.\n\nBe Clear\n\nWrite legibly. This doesn’t refer to handwriting (since you’ll be submitting PDFs of Jupyter notebooks), but the text itself should be clearly written and well organized, so that it’s easy for the grader to follow your reasoning or explanation. Structuring your solution, and not writing in a stream of consciousness, helps you think more clearly. To reiterate: You will be given no points if the grader cannot easily follow your answer, and the graders have complete discretion to determine this.\nWrite clearly. Use proper spelling, grammar, logic, etc. We will try not to penalize people for not having complete mastery of English, but again, we need to be able to follow your reasoning.\nWrite carefully and completely. We can only grade what you write; nobody can read your mind, and we will not try. The solution that you submit must stand on its own. If your answer is ambiguous, the TA has been instructed to interpret it the wrong way. Regrade requests also cannot be used to add more information to a solution.\nDon’t submit your first draft. For most people, first drafts are terrible. They are often poorly organized, unclear, and contain gaps or jumps in reasoning. You will likely need to revise, possibly several times, for your answer to be clear, careful, and complete. This is another reason to start the assignment early — if you start late, you may be stuck with your first draft, and your grade is likely to suffer for it.\nState your assumptions. If you think a problem statement is ambiguous and your solution depends on a particular interpretation, or you need to make some assumptions to solve the problem, make it explicit (though do also ask for clarification in class or on Ed Discussion).\nDon’t rely on your code. The TA will not try to scrutinize your code, which is a waste of time (again, the code is a means to an end — it works or it doesn’t). If you make it clear what your code is supposed to do, then the TA can tell if this logic is correct and any mistake must be something minor in the code. If you just provide code (even if commented), the TA can’t do this without running your code and debugging, which is not a valuable use of time. We want to focus on your ideas.\n\n\n\nBe Concise\n\nKeep solutions short. Organized answers should not be long. There’s a fine balance between conciseness and completeness: find it!\nDon’t regurgitate. You can reference concepts, models, etc from class without repeating them. Just make it clear what you’re modifying and how you’re using those concepts for that particular problem.\nDon’t bullshit. You will get no points for word salad, even if you accidentally hit on the right answer.",
    "crumbs": [
      "Course Information",
      "Homework Policies"
    ]
  },
  {
    "objectID": "solutions/hw02/hw02.html",
    "href": "solutions/hw02/hw02.html",
    "title": "Homework 2 Solutions",
    "section": "",
    "text": "Due Date\n\n\n\nFriday, 2/23/24, 9:00pm"
  },
  {
    "objectID": "solutions/hw02/hw02.html#overview",
    "href": "solutions/hw02/hw02.html#overview",
    "title": "Homework 2 Solutions",
    "section": "Overview",
    "text": "Overview\n\nInstructions\nThe goal of this homework assignment is to practice developing and working with probability models for data.\n\nProblem 1 asks you to fit a sea-level rise model using normal residuals and to assess the validity of that assumption.\nProblem 2 asks you to model the time series of hourly weather-related variability at a tide gauge.\nProblem 3 asks you to model the occurrences of Cayuga Lake freezing, and is only slightly adapted from Example 4.1 in Statistical Methods in the Atmospheric Sciences by Daniel Wilks.\nProblem 4 (graded only for graduate students) asks you to revisit the sea-level model in Problem 1 by including a model-data discrepancy term in the model calibration.\n\n\n\nLearning Outcomes\nAfter completing this assignments, students will be able to:\n\ndevelop probability models for data and model residuals under a variety of statistical assumptions;\nevaluate the appropriateness of those assumptions through the use of qualitative and quantitative evaluations of goodness-of-fit;\nfit a basic Bayesian model to data.\n\n\n\nLoad Environment\nThe following code loads the environment and makes sure all needed packages are installed. This should be at the start of most Julia scripts.\n\n\nimport Pkg\nPkg.activate(@__DIR__)\nPkg.instantiate()\n\n\nThe following packages are included in the environment (to help you find other similar packages in other languages). The code below loads these packages for use in the subsequent notebook (the desired functionality for each package is commented next to the package).\n\n\nusing Random # random number generation and seed-setting\nusing DataFrames # tabular data structure\nusing CSVFiles # reads/writes .csv files\nusing Distributions # interface to work with probability distributions\nusing Plots # plotting library\nusing StatsBase # statistical quantities like mean, median, etc\nusing StatsPlots # some additional statistical plotting tools\nusing Optim # optimization tools\n\nRandom.seed!(1)"
  },
  {
    "objectID": "solutions/hw02/hw02.html#problems-total-30-points-for-4850-40-for-5850",
    "href": "solutions/hw02/hw02.html#problems-total-30-points-for-4850-40-for-5850",
    "title": "Homework 2 Solutions",
    "section": "Problems (Total: 30 Points for 4850; 40 for 5850)",
    "text": "Problems (Total: 30 Points for 4850; 40 for 5850)\n\nProblem 1\n\nConsider the following sea-level rise model from Rahmstorf (2007):\n\\[\\frac{dH(t)}{dt} = \\alpha (T(t) - T_0),\\] where \\(T_0\\) is the temperature (in \\(^\\circ C\\)) where sea-level is in equilibrium (\\(dH/dt = 0\\)), and \\(\\alpha\\) is the sea-level rise sensitivity to temperature. Discretizing this equation using the Euler method and using an annual timestep (\\(\\delta t = 1\\)), we get \\[H(t+1) = H(t) + \\alpha (T(t) - T_0).\\]\nIn this problem:\n\nLoad the data from the data/ folder\n\nGlobal mean temperature data from the HadCRUT 5.0.2.0 dataset (https://hadobs.metoffice.gov.uk/hadcrut5/data/HadCRUT.5.0.2.0/download.html) can be found in data/HadCRUT.5.0.2.0.analysis.summary_series.global.annual.csv. This data is averaged over the Northern and Southern Hemispheres and over the whole year.\nGlobal mean sea level anomalies (relative to the 1990 mean global sea level) are in data/CSIRO_Recons_gmsl_yr_2015.csv, courtesy of CSIRO (https://www.cmar.csiro.au/sealevel/sl_data_cmar.html).\n\nFit the model under the assumption of normal i.i.d. residuals by maximizing the likelihood and report the parameter estimates. Note that you will need another parameter \\(H_0\\) for the initial sea level. What can you conclude about the relationship between global mean temperature increases and global mean sea level rise?\nHow appropriate was the normal i.i.d. probability model for the residuals? Use any needed quantitative or qualitative assessments of goodness of fit to justify your answer. If this was not an appropriate probability model, what would you change?\n\n\nSolution:\nFirst, let’s load the data and implement the model with a function.\n\n# load data files\nslr_data = DataFrame(load(\"data/CSIRO_Recons_gmsl_yr_2015.csv\"))\ngmt_data = DataFrame(load(\"data/HadCRUT.5.0.2.0.analysis.summary_series.global.annual.csv\"))\nslr_data[:, :Time] = slr_data[:, :Time] .- 0.5; # remove 0.5 from Times\ndat = leftjoin(slr_data, gmt_data, on=\"Time\") # join data frames on time\nselect!(dat, [1, 2, 3, 4])  # drop columns we don't need\nfirst(dat, 6)\n\n\n6×4 DataFrame\n\n\n\nRow\nTime\nGMSL (mm)\nGMSL uncertainty (mm)\nAnomaly (deg C)\n\n\n\nFloat64\nFloat64\nFloat64\nFloat64?\n\n\n\n\n1\n1880.0\n-158.7\n24.2\n-0.315832\n\n\n2\n1881.0\n-153.1\n24.2\n-0.232246\n\n\n3\n1882.0\n-169.9\n23.0\n-0.29553\n\n\n4\n1883.0\n-164.6\n22.8\n-0.346474\n\n\n5\n1884.0\n-143.7\n22.2\n-0.49232\n\n\n6\n1885.0\n-145.2\n21.9\n-0.471124\n\n\n\n\n\n\n\n\n# slr_model: function to simulate sea-level rise from global mean temperature based on the Rahmstorf (2007) model\n\nfunction slr_model(α, T₀, H₀, temp_data)\n    temp_effect = α .* (temp_data .- T₀)\n    slr_predict = cumsum(temp_effect) .+ H₀\n    return slr_predict\nend\n\nslr_model (generic function with 1 method)\n\n\nNow, let’s fit the model under the normal residual assumption.\n\n# split data structure into individual pieces\nyears = dat[:, 1]\nsealevels = dat[:, 2]\ntemp = dat[:, 4]\n\n# write function to calculate likelihood of residuals for given parameters\n# parameters are a vector [α, T₀, H₀, σ]\nfunction llik_normal(params, temp_data, slr_data)\n    slr_out = slr_model(params[1], params[2], params[3], temp_data)\n    resids = slr_out - slr_data\n    return sum(logpdf.(Normal(0, params[4]), resids))\nend\n\n# set up lower and upper bounds for the parameters for the optimization\nlbds = [0.0, -50.0, -200.0, 0.0]\nubds = [10.0, 1.0, 0.0, 20.0]\np0 = [5.0, -1.0, -100.0, 5.0]\np_mle = Optim.optimize(p -&gt; -llik_normal(p, temp, sealevels), lbds, ubds, p0).minimizer\n\n4-element Vector{Float64}:\n    1.863932768247625\n   -0.9709731494908135\n -157.3346053534705\n    5.911511282104595\n\n\nAs p_mle[1] has a positive value of 1.86, this suggests a positive correlation between global mean temperature and global mean sea levels. Of course, we can’t conclude anything about causality, as it could be that both are independently increasing simultaneously, or that there is a mutual cause of both increases. However, mechanistically it makes sense that temperature increases would increase sea levels, due to melting ice and thermal expansion of ocean waters.\nTo look at the appropriateness of the normal residual assumption, let’s compute the residuals and look at a Q-Q plot and their autocorrelation.\n\n# compute the residuals\nresids = slr_model(p_mle[1], p_mle[2], p_mle[3], temp) - sealevels\nhistogram(resids, xlabel=\"Residual\", ylabel=\"Count\", legend=false)\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Histogram of the residuals from the maximum likelihood estimation.\n\n\n\n\nThe residual histogram in Figure 1 looks a bit skewed. Let’s look at a Q-Q plot.\n\nqqplot(Normal, resids, xlabel=\"Normal Theoretical Quantile\", ylabel=\"Sample Residual Quantile\", legend=false)\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Q-Q plot of the residuals from the maximum likelihood estimation.\n\n\n\n\nThe Q-Q plot (Figure 2) looks good; there are some discrepancies near the tail, but that is not surprising.\n\nresid_acf = pacf(resids, 1:5)\nplot(resid_acf , marker=:circle, line=:stem, linewidth=3, markersize=5, legend=false, ylabel=\"Partial Autocorrelation\", xlabel=\"Time Lag\")\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Autocorrelation plot of the residuals from the maximum likelihood estimation.\n\n\n\n\nThe bigger problem is that the lag-1 autocorrelation coefficient is 0.54, which is not consistent with the normally-distributed assumption. The next thing to try would be an AR(1) model for the residuals.\n\n\nProblem 2\n\nTide gauge data is complicated to analyze because it is influenced by different harmonic processes (such as the linear cycle). In this problem, we will develop a model for this data using NOAA data from the Sewell’s Point tide gauge outside of Norfolk, VA from data/norfolk-hourly-surge-2015.csv. This is hourly data (in m) from 2015 and includes both the observed data (Verified (m)) and the tide level predicted by NOAA’s sinusoidal model for periodic variability, such as tides and other seasonal cycles (Predicted (m)).\nIn this problem:\n\nLoad the data file. Take the difference between the observations and the sinusoidal predictions to obtain the tide level which could be attributed to weather-related variability (since for one year sea-level rise and other factors are unlikely to matter). Plot this data.\nDevelop an autoregressive model for the weather-related variability in the Norfolk tide gauge. Make sure to include your logic or exploratory analysis used in determining the model specification.\nUse your model to simulate 1,000 realizations of hourly tide gauge observations. What is the distribution of the maximum tide level? How does this compare to the observed value?\n\n\nSolution:\nFirst, load the data, find the weather-related residuals, and plot. To load and process the data, we will use the DataFramesMeta.jl package, which lets us string together commands in a convenient way. This is completely optional, however.\n\ntide_dat = DataFrame(load(\"data/norfolk-hourly-surge-2015.csv\"))\nsurge_resids = tide_dat[:, 5] - tide_dat[:, 3]\nplot(surge_resids; ylabel=\"Gauge Measurement (m)\", label=\"Weather-Related Residual\", legend=:topleft, xlabel=\"Hour Number\")\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Plot of the weather-related variability from the 2015 Sewell’s Point hourly tide gauge data.\n\n\n\n\nLet’s look at the autocorrelation function of the residuals to determine what the autoregressive lag should be.\n\nresid_acf = pacf(surge_resids, 1:5)\nplot(resid_acf , marker=:circle, line=:stem, linewidth=3, markersize=5, legend=false, ylabel=\"Partial Autocorrelation\", xlabel=\"Time Lag\")\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Autocorrelation plot of the tide gauge residuals.\n\n\n\n\nWe can see that there is a very strong autocorrelation at lag 1, but not much additional autocorrelation introduced at lag 2, so we will use an AR(1) model (though you might be justified in using an AR(2) model, as the lag-2 partial autocorrelation is not completely negligible).\nThis AR(1) model is \\[y_t = \\rho y_{t+1} + \\varepsilon, \\quad \\varepsilon \\sim N(0, \\sigma).\\] Let’s implement this model and fit it. We’ll specify the likelihood based on whitening the residuals and computing the individual normal likelihoods of the resulting terms.\n\nfunction llik_ar(ρ, σ, y)\n    ll = 0 # initialize log-likelihood counter\n    ll += logpdf(Normal(0, σ/sqrt(1-ρ^2)), y[1])\n    for i = 1:length(y)-1\n        residuals_whitened = y[i+1] - ρ * y[i]\n        ll += logpdf(Normal(0, σ), residuals_whitened)\n    end\n    return ll\nend\n\n# set up lower and upper bounds for the parameters for the optimization\nlbds = [0.0, 0.0]\nubds = [1.0, 5.0]\np0 = [0.5, 2.0]\np_tide_mle = Optim.optimize(p -&gt; -llik_ar(p[1], p[2], surge_resids), lbds, ubds, p0).minimizer\n\n2-element Vector{Float64}:\n 0.9956392098280269\n 0.019663589611082367\n\n\nNow we can simulate these residuals and add them back to the NOAA model predictions using the mapslices() function. The resulting histogram of the maximum values is shown in Figure 6.\n\n# T is the length of the simulated series, N the number of simulations\nfunction simulate_ar1(ρ, σ, T, N)\n    y = zeros(T, N) # initialize storage\n    y[1, :] = rand(Normal(0, σ/sqrt(1-ρ^2)), N) # sample initial value\n    # simulate the remaining residuals\n    for t = 2:T\n        y[t, :] = ρ * y[t-1, :] + rand(Normal(0, σ), N)\n    end\n    return y\nend\n\n# simulate new observations\nsimulated_resids = simulate_ar1(p_tide_mle[1], p_tide_mle[2], length(surge_resids), 1000)\nsimulated_obs = mapslices(col -&gt; col + tide_dat[:, 3], simulated_resids, dims=1)\n# plot histogram\nmax_obs = [maximum(simulated_obs[:, n]) for n in 1:size(simulated_obs, 2)]\nhistogram(max_obs, xlabel=\"Maximum Simulated Gauge Level (m)\", ylabel=\"Count\", legend=false)\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Histogram of the maximum simulated tide gauge measurements.\n\n\n\n\nThis distribution fails to capture the observed maximum 1.98, which could be due to a model structural problem (we might want to allow for more extreme AR(1) innovations than the normal distribution of \\(\\varepsilon_t\\)) or just the unlikeliness of that one observation.\n\n\nProblem 3\n\nAs of 2010, Cayuga Lake has frozen in the following years: 1796, 1816, 1856, 1875, 1884, 1904, 1912, 1934, 1961, and 1979. Based on this data, we would like to project whether Cayuga Lake is likely to freeze again in the next 25 years.\nIn this problem:\n\nAssuming that observations began in 1780, write down a Bayesian model for whether Cayuga Lake will freeze in a given year, using a Bernoulli distribution. How did you select what prior to use?\nFind the maximum a posteriori estimate using your model.\nGenerate 1,000 realizations of Cayuga Lake freezing occurrences from 1780 to 2010 and check the simulations against the occurrance data.\nUsing your model, calculate the probability of Cayuga Lake freezing at least once in the next 10 years.\nWhat do you think about the validity of your model, both in terms of its ability to reproduce historical data and its use to make future projections? Why might you believe or discount it? What changes might you make (include thoughts about the prior)?\n\n\nSolution:\nFirst, let’s enter the data.\n\nfz_yrs = [1796, 1816, 1856, 1875, 1884, 1904, 1912, 1934, 1961, 1979]\ny = DataFrame(year=1780:2010, freeze=zeros(Bool, length(1780:2010)))\n# indexin(a, b) finds indices of a which are in b and returns nothing if not found\n# so we use !isnothing to only get the indices which occur in fz_yrs, and (.!) is needed\n# to broadcast\ny[.!(isnothing.(indexin(y.year, fz_yrs))), :freeze] .= true\n\n10-element view(::Vector{Bool}, [17, 37, 77, 96, 105, 125, 133, 155, 182, 200]) with eltype Bool:\n 1\n 1\n 1\n 1\n 1\n 1\n 1\n 1\n 1\n 1\n\n\nUsing a Bernoulli distribution means that we need \\(y_t \\sim Bernoulli(p)\\), where \\(p\\) is the probability of freezing in year \\(t\\). The maximum-likelihood estimate of \\(p\\) is easy: it’s just the empirical frequency of true. But we want to develop a Bayesian model, as this empirically-observed frequency might not actually be the most likely outcome. To do this, we need to specify a prior for \\(p\\), which is usually given as \\(p \\sim Beta(\\alpha, \\beta)\\). We will use prior predictive simulations to determine appropriate Beta parameters.\n\nfunction sim_freeze(α, β, nyrs, nsamples)\n    p = rand(Beta(α, β), nsamples)\n    fz = sum.(rand.(Bernoulli.(p), nyrs))\n    return fz\nend\n\nsim_freeze (generic function with 1 method)\n\n\nLet’s see what the impact of a \\(Beta(1, 10)\\) prior and a \\(Beta(1, 20)\\) prior look like.\n\nfz_10 = sim_freeze(1, 10, nrow(y), 1000)\np1 = histogram(fz_10, xlabel=\"Number of Freezes\", ylabel=\"Count\", legend=false)\nplot!(p1, size=(300, 250))\n\nfz_20 = sim_freeze(1, 20, nrow(y), 1000)\np2 = histogram(fz_20, xlabel=\"Number of Freezes\", ylabel=\"Count\", legend=false)\nplot!(p2, size=(300, 250))\n\ndisplay(p1)\ndisplay(p2)\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) \\(Beta(1,10)\\) prior\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) \\(Beta(1,20)\\) prior\n\n\n\n\n\n\n\nFigure 7: Prior predictive distribution results.\n\n\n\n\nFigure 7 (b) looks reasonable, since we might suspect that freezing shouldn’t occur too frequently. As a result, we’ll work with the \\(p \\sim Beta(1, 20)\\) prior for this model. So the full model specification is:\n\\[\\begin{align*}\ny_t &\\sim Bernoulli(p) \\\\\np &\\sim Beta(1, 20)\n\\end{align*}\\]\nNow we can compute the MAP estimate.\n\n# compute the log-posterior\nfunction lpost(p, dat)\n    lpri = logpdf(Beta(1, 20), p)\n    llik = sum(logpdf.(Bernoulli(p), dat))\n    return lpri + llik\nend\n\nmap = Optim.optimize(p -&gt; -lpost(p, y[:, :freeze]), 0, 1).minimizer\n\n0.039999999097374374\n\n\nThat gives us a MAP estimate of 0.039999999097374374. Note that your answer might differ depending on the prior you selected. Now we can do simulations and compare the results to the observations. We will do this using histograms to show the number of occurrences, as the model does not account for any temporal changes in frequency.\n\nfunction sim_freeze_map(p, nyrs, nsamples)\n    fz = mapslices(sum, rand(Bernoulli(p), (nyrs, nsamples)), dims=1)'\n    return fz\nend\n\np = histogram(sim_freeze_map(map, nrow(y), 1000), xlabel=\"Number of Freezing Occurrences\", ylabel=\"Count\", label=false)\nvline!(p, [sum(y[:, :freeze])], label=\"Data\")\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8: Histogram of number of freezing occurrences from model simulations.\n\n\n\n\nThe probability of Cayuga Lake freezing in the next 10 years is calculated with:\n\n100 * sum(sim_freeze_map(map, 10, 1000)) / (10*1000)\n\n4.22\n\n\nBased on Figure 8, the model seems to replicate the data well (with a slight bias towards fewer freezing events due to the prior). However, the model might be improved with time or temperature dependence, as the original data appears to have more freezing events earlier in the record and fewer after 1950.\n\n\nProblem 4\nGRADED FOR 5850 STUDENTS ONLY\nFor the sea-level model in Problem 1, model the model-data discrepancy using an AR(1) process, with observation error modeled as normally distributed with standard deviation given by the uncertainty column in the data file.\nIn this problem:\n\nFind the maximum likelihood estimate of the parameters with this discrepancy structure. How does the parameter inference change from the normal i.i.d. estimate in Problem 1?\nGenerate 1,000 traces, plot a comparison of the hindcasts to those from Problem 1, and compare the surprise indices.\nDetermine whether you have accounted for autocorrelation in the residuals appropriately (hint: generate realizations of just the discrepancy series, compute the resulting residuals from the model fit + discrepancy, and look at the distribution of autocorrelation values).\nWhich model specification would you prefer and why?\n\n:::\nFrom the lecture, we can implement the discrepancy structure likelihood as:\n\nfunction ar_covariance_mat(σ, ρ, y_err)\n    H = abs.((1:length(y_err)) .- (1:(length(y_err)))') # compute the outer product to get the correlation lags\n    ζ_var = σ^2 / (1-ρ^2)\n    Σ = ρ.^H * ζ_var\n    for i in 1:length(y_err)\n        Σ[i, i] += y_err[i]^2\n    end\n    return Σ\nend\n\nfunction ar_discrep_log_likelihood(p, σ, ρ, y, m, y_err)\n    y_pred = m(p)\n    residuals = y_pred .- y\n    Σ = ar_covariance_mat(σ, ρ, y_err)\n    ll = logpdf(MvNormal(zeros(length(y)), Σ), residuals)\n    return ll\nend\n\n# params vector is [α, T₀, H₀]\nslr_wrap(params) = slr_model(params[1], params[2], params[3], temp)\n\n# maximize log-likelihood within some range\n# important to make everything a Float instead of an Int \nlower = [0.0, -5.0, -200.0, 0.0, -1.0]\nupper = [5.0, 1.0, 0.0, 20.0, 1.0]\np0 = [2.0, 0.0, -100.0, 5.0, 0.5]\nresult = Optim.optimize(params -&gt; -ar_discrep_log_likelihood(params[1:end-2], params[end-1], params[end], sealevels, slr_wrap, dat[:, 3]), lower, upper, p0)\np_ar_mle = result.minimizer\n\n5-element Vector{Float64}:\n    1.998198372838903\n   -0.9076726257161041\n -154.93253804174117\n    1.3977277644676105\n    0.8557998637644137\n\n\nThe AR discrepancy model has a higher sea-level sensitivity to temperature and a slightly higher equilibrium temperature and initial condition. Simulating a hindcast:\n\nnsamples = 1000\nΣ = ar_covariance_mat(p_ar_mle[4], p_ar_mle[5], dat[:, 3])\nresiduals_discrep = rand(MvNormal(zeros(nrow(dat)), Σ), nsamples)\nmodel_discrep = slr_wrap(p_ar_mle[1:end-2])\nmodel_sim_discrep = (residuals_discrep .+ model_discrep)'\nmedian_discrep = mapslices(col -&gt; quantile(col, 0.5), model_sim_discrep; dims=1)' # compute median\nq90_discrep = mapslices(col -&gt; quantile(col, [0.05, 0.95]), model_sim_discrep; dims=1) # compute 90% prediction interval\nplot(years, median_discrep, linewidth=2, label=\"Discrepancy Model\", ribbon=(median_discrep .- q90_discrep[1, :], q90_discrep[2, :] .- median_discrep), fillalpha=0.2, xlabel=\"Year\", ylabel=\"Sea Level Anomaly (mm)\")\nscatter!(years, sealevels, label=\"Observations\")\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 9: Hindcast generated from 1,000 simulations from the SLR model with AR discrepancy.\n\n\n\n\nCalculating the surprise index (\\(1 - \\frac{1}{n} \\sum_{i=1}^n \\mathbb{I}_{\\mathcal{I}_\\alpha}(y_i)\\)) of both 90% intervals:\n\n# need to compute the hindcast projection intervals for the iid model\nslr_iid_out = slr_model(p_mle[1], p_mle[2], p_mle[3], temp)\niid_residuals = rand(Normal(0, p_mle[4]), (length(years), nsamples))\nmodel_sim_iid = (iid_residuals .+ slr_iid_out)'\nq90_iid = mapslices(col -&gt; quantile(col, [0.05, 0.95]), model_sim_iid; dims=1) \n\n2×134 Matrix{Float64}:\n -165.159  -164.663  -161.948  …  46.7122  48.602   52.3031  55.1233\n -146.702  -145.918  -144.36      65.5852  69.0316  70.9158  74.4833\n\n\n\nsurprise_discrep = 0 # initialize surprise counter\nsurprise_iid = 0\n# go through the data and check which points are outside of the 90% interval\nfor i = 1:length(sealevels)\n    ## The || operator is an OR, so returns true if either of the terms are true\n    if (sealevels[i] &lt; q90_discrep[1, i]) || (q90_discrep[2, i] &lt; sealevels[i])\n        surprise_discrep += 1\n    end\n    if (sealevels[i] &lt; q90_iid[1, i]) || (q90_iid[2, i] &lt; sealevels[i])\n        surprise_iid += 1\n    end\n\nend\nsi_iid = surprise_iid / length(sealevels) * 100\nsi_discrep = surprise_discrep / length(sealevels) * 100\n[si_iid; si_discrep]\n\n2-element Vector{Float64}:\n 8.955223880597014\n 1.4925373134328357\n\n\nThe AR(1) discrepancy model here actually looks underconfident, which might suggest that the variance for the discrepancy process is too large due to the large historical SLR observation errors (which could be accounted for with a tighter prior in a Bayesian setup). But we should also check if we did account for the residual autocorrelation that we saw in Problem 1, which we can do by generating simulations without observation error.\n\nnsamples = 1000\nΣ = ar_covariance_mat(p_ar_mle[4], p_ar_mle[5], zeros(nrow(dat)))\nresiduals_discrep = rand(MvNormal(zeros(nrow(dat)), Σ), nsamples)\nmodel_discrep = slr_wrap(p_ar_mle[1:end-2])\nmodel_sim_discrep = (residuals_discrep .+ model_discrep)'\nobs_error = model_sim_discrep .- sealevels'\nobs_error_acf = mapslices(col -&gt; pacf(col, 1:5), obs_error, dims=2)\nboxplot(obs_error_acf, legend=false, xlabel=\"Autocorrelation Lag\", ylabel=\"Partial Autocorrelation\")\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 10: Autocorrelation of residuals generated from 1,000 simulations from the SLR model with AR discrepancy.\n\n\n\n\nFigure 10 suggests that the lag-1 autocorrelation has not actually been accounted for properly, which might suggest that there is a problem with the model structure or that a more sophisticated error model is needed (for example, linking the residuals to the global mean temperature or accounting for clustering in the residual variance). Between the two models, since neither model properly accounted for the lag-1 residual autocorrelation, we might prefer the simpler i.i.d. normal model which had a more appropriate surprise index."
  },
  {
    "objectID": "solutions/hw01/hw01.html",
    "href": "solutions/hw01/hw01.html",
    "title": "Homework 1 Solutions",
    "section": "",
    "text": "The goal of this homework assignment is to introduce you to simulation-based data analysis.\n\nProblem 1 asks you to explore whether a difference between data collected from two groups might be statistically meaningful or the result of noise. This problem repeats the analysis from Statistics Without The Agonizing Pain by John Rauser (which is a neat watch!).\nProblem 2 asks you to evaluate an interview method for finding the level of cheating on a test to determine whether cheating was relatively high or low. This problem was adapted from Bayesian Methods for Hackers.\n\n\n\n\nThe following code loads the environment and makes sure all needed packages are installed. This should be at the start of most Julia scripts.\n\nimport Pkg\nPkg.activate(@__DIR__)\nPkg.instantiate()\n\nThe following packages are included in the environment (to help you find other similar packages in other languages). The code below loads these packages for use in the subsequent notebook (the desired functionality for each package is commented next to the package).\n\nusing Random # random number generation and seed-setting\nusing DataFrames # tabular data structure\nusing CSVFiles # reads/writes .csv files\nusing Distributions # interface to work with probability distributions\nusing Plots # plotting library\nusing StatsBase # adds some basic statistical functionality (means, medians, empirical cumulative densities, etc)\nusing StatsPlots # for qqplots"
  },
  {
    "objectID": "solutions/hw01/hw01.html#overview",
    "href": "solutions/hw01/hw01.html#overview",
    "title": "Homework 1 Solutions",
    "section": "",
    "text": "The goal of this homework assignment is to introduce you to simulation-based data analysis.\n\nProblem 1 asks you to explore whether a difference between data collected from two groups might be statistically meaningful or the result of noise. This problem repeats the analysis from Statistics Without The Agonizing Pain by John Rauser (which is a neat watch!).\nProblem 2 asks you to evaluate an interview method for finding the level of cheating on a test to determine whether cheating was relatively high or low. This problem was adapted from Bayesian Methods for Hackers.\n\n\n\n\nThe following code loads the environment and makes sure all needed packages are installed. This should be at the start of most Julia scripts.\n\nimport Pkg\nPkg.activate(@__DIR__)\nPkg.instantiate()\n\nThe following packages are included in the environment (to help you find other similar packages in other languages). The code below loads these packages for use in the subsequent notebook (the desired functionality for each package is commented next to the package).\n\nusing Random # random number generation and seed-setting\nusing DataFrames # tabular data structure\nusing CSVFiles # reads/writes .csv files\nusing Distributions # interface to work with probability distributions\nusing Plots # plotting library\nusing StatsBase # adds some basic statistical functionality (means, medians, empirical cumulative densities, etc)\nusing StatsPlots # for qqplots"
  },
  {
    "objectID": "solutions/hw01/hw01.html#problems",
    "href": "solutions/hw01/hw01.html#problems",
    "title": "Homework 1 Solutions",
    "section": "Problems",
    "text": "Problems\n\nProblem 1\n\ndata = DataFrame(load(\"data/bites.csv\")) # load data into DataFrame\n\n# print data variable (semi-colon suppresses echoed output in Julia, which in this case would duplicate the output)\n@show data; \n\ndata = 43×2 DataFrame\n Row │ group   bites\n     │ String  Int64\n─────┼───────────────\n   1 │ beer       27\n   2 │ beer       20\n   3 │ beer       21\n   4 │ beer       26\n   5 │ beer       27\n   6 │ beer       31\n   7 │ beer       24\n   8 │ beer       21\n   9 │ beer       20\n  10 │ beer       19\n  11 │ beer       23\n  12 │ beer       24\n  13 │ beer       28\n  14 │ beer       19\n  15 │ beer       24\n  16 │ beer       29\n  17 │ beer       18\n  18 │ beer       20\n  19 │ beer       17\n  20 │ beer       31\n  21 │ beer       20\n  22 │ beer       25\n  23 │ beer       28\n  24 │ beer       21\n  25 │ beer       27\n  26 │ water      21\n  27 │ water      22\n  28 │ water      15\n  29 │ water      12\n  30 │ water      21\n  31 │ water      16\n  32 │ water      19\n  33 │ water      15\n  34 │ water      22\n  35 │ water      24\n  36 │ water      19\n  37 │ water      23\n  38 │ water      13\n  39 │ water      22\n  40 │ water      20\n  41 │ water      24\n  42 │ water      18\n  43 │ water      20\n\n\n\n# split data into vectors of bites for each group\nbeer = data[data.group .== \"beer\", :bites]\nwater = data[data.group .== \"water\", :bites]\n\nobserved_difference = mean(beer) - mean(water)\n@show observed_difference;\n\nobserved_difference = 4.37777777777778\n\n\nIn this problem:\n\nConduct the above procedure to generate 50,000 simulated datasets under the skeptic’s hypothesis.\nPlot a histogram of the results and add a dashed vertical line to show the experimental difference (if you are using Julia, feel free to look at the Making Plots with Julia tutorial on the class website).\nDraw conclusions about the plausibility of the skeptic’s hypothesis that there is no difference? Feel free to use any quantitative or qualitative assessments of your simulations and the observed difference.\n\nSolution:\nFirst, we write a function (simulate_differences()) to generate a new data set and compute the group differences under the skeptic’s hypothesis by shuffling the data across the two groups (this is called the non-parametric bootstrap, which we will talk about more later):\n\n# simulate_differences: function which simulates a new group difference based on the skeptic's hypothesis of no \"real\" difference between groups by shuffling the input data across groups\n# inputs:\n#   y₁: vector of bite counts for the beer-drinking group\n#   y₂: vector of bite counts for the water-drinking group\n# output:\n#   a simulated difference between shuffled group averages\nfunction simulate_differences(y₁, y₂)\n    # concatenate both vectors into a single vector\n    y = vcat(y₁, y₂)\n\n    # create new experimental groups consistent with skeptic's hypothesis\n    y_shuffle = shuffle(y)   # shuffle the combined data\n    n₁ = length(y₁)\n    x₁ = y_shuffle[1:n₁]\n    x₂ = y_shuffle[(n₁+1):end]\n\n    # compute difference between new group means\n    diff = mean(x₁) - mean(x₂)\n    return diff\nend\n\nsimulate_differences (generic function with 1 method)\n\n\nNext, we evaluate this function 10,000 times and plot the resulting histogram of differences.\nIn Julia (and in Python), it is convenient to use a comprehension to automatically allocate the output of the for loop to a vector. The syntax for a comprehension is [some_function(input) for input in some_range]. In this case, the index input doesn’t appear in the comprehension as we’re just repeating the exact same calculation every time:\n\nshuffled_differences = [simulate_differences(beer, water) for i in 1:50_000]\n\n50000-element Vector{Float64}:\n -0.87777777777778\n -1.4511111111111106\n -0.5911111111111111\n  0.173333333333332\n  0.8422222222222224\n  0.07777777777777928\n  0.6511111111111099\n  1.0333333333333314\n -0.017777777777777004\n -2.120000000000001\n -2.024444444444441\n -0.4955555555555584\n  0.6511111111111099\n  ⋮\n  1.3200000000000003\n -0.3044444444444423\n  1.0333333333333314\n -0.3999999999999986\n  2.0844444444444434\n -0.4955555555555584\n -0.11333333333333329\n -0.3999999999999986\n -0.017777777777777004\n  1.0333333333333314\n -1.1644444444444417\n -0.3044444444444423\n\n\nWithout a comprehension, this loop would look something like:\nshuffled_diffs = zeros(10_000)\nfor i in 1:length(shuffled_diffs)\n    shuffled_differences[i] = simulate_differences(beer, water)\nend\nBack to the problem. Now let’s plot the histogram. When you see a ! after a function name in Julia, it means that this is a mutating function, which changes the object that it acts on, rather than returning a new object and preserving the old one. In this case, the plot object is changed to add new elements to the original histogram.\n\nhist = histogram(shuffled_differences, label=\"Simulated Differences\") # plot the basic histogram\n# MAKE SURE TO ADD AXIS LABELS!\nxlabel!(hist, \"Increased Number of Average Bites for Beer Group\")\nylabel!(hist, \"Count\")\n\n# now add a vertical line for the experimentally-observed difference\nvline!(hist, [observed_difference], color=:red, linestyle=:dash, label=\"Observed Difference\")\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Histogram of simulated differences between the average bites for the beer group and the average bites for the water group under the assumption that differences are due only to random chance. A positive value indicates that the beer group is bitten more often. The red line is the group difference from the experimental data.\n\n\n\n\nWhat do we see from Figure 1?\n\nThe simulated differences follow roughly a normal distribution centered at a value of zero. This is not surprising given the hypothesis that there is no “true” difference in bite frequency between the two groups.\nThe observed data are extremely unlikely if the skeptic’s hypothesis is true. We can calculate the probability of seeing data that extreme given this hypothesis (also called the p-value) by finding the empirical cumulative density function of the simulated data vector:\n\nempirical_cdf = ecdf(shuffled_differences)\n1 - empirical_cdf(observed_difference)\n\n0.00038000000000004697\n\n\nThis shows that we would only expect, given the skeptic’s hypothesis, to see data at least this extreme by chance in 0.04% of experiments. If we don’t think that our experiment is likely to be an outlier, this suggests that the skeptic’s hypothesis is quite unlikely. However, this does not mean our mechanistic theory for the group difference is correct: this would require more work and maybe a more targeted experiment!\n\n\n\nProblem 2\nIn this problem:\n\nDerive and code a simulation model for the above interview procedure given the “true” probability of cheating \\(p\\).\nSimulate your model (for a class of 100 students) 50,000 times under the your hypothesis and the TA’s hypothesis, and plot the two resulting datasets.\nIf you received 31 “Yes, I cheated” responses while interviewing your class, what could you conclude? Feel free to use any qualitative or quantiative assessments to justify your conclusions.\nHow useful do you think the interview procedure is to identify systemic teaching? What changes to the design might you make?\n\nSolution:\nLet’s start by sketching out the simulation model. For every student, we first need to simulate the outcome of the first coin flip, which has a 50% probability of heads. If this coin flip comes up as heads, then the student answers honestly, and admits to cheating with probability \\(p\\). If the coin flip comes up tails, the student flips another coin and answers that they cheated with probability 50%. After looping over this procedure for each student in the class, we add up the “true” values.\nWe might code this model as follows.\n\n# cheating_model: function which simulates the outcome of the interview procedure described in this problem and returns the number of confessions obtained.\n# inputs:\n#   p: base probability of cheating under a given hypothesis\n#   n: vector of bite counts for the water-drinking group\n# output:\n#   a simulated number of confessions for one realization of the process.\nfunction cheating_model(p, n)\n    # initialize the storage vector for whether students admit to cheating\n    # we do this with a boolean vector, which is a little faster, but storing integers is basically the same thing\n    cheat = zeros(Bool, n)\n    # loop over every student to simulate the interview process\n    for i in 1:n\n        # initial coin flip\n        # rand() simulates a uniform random number between 0 and 1\n        if rand() &gt;= 0.5\n            # if this came up heads, simulate whether the student cheated based on the cheating probability\n            if rand() &lt; p\n                cheat[i] = true\n            else\n                cheat[i] = false\n            end\n        else\n            # otherwise, simulate another coin flip\n            if rand() &gt;= 0.5\n                cheat[i] = true\n            else\n                cheat[i] = false\n            end\n        end\n    end\n    # return the total number of cheating admissions\n    return sum(cheat)\nend\n\ncheating_model (generic function with 1 method)\n\n\nNow we simulate under our assumption of low cheating and the TA’s assumption of widespread cheating and plot the results.\n\n# conduct the simulations\nlow_cheat_data = [cheating_model(0.05, 100) for i in 1:50_000]\nhigh_cheat_data = [cheating_model(0.30, 100) for i in 1:50_000]\n\n# plot the histograms with axis labels and a vertical line for the \"real\" outcome of the procedure\nhistogram(low_cheat_data, color=:blue, label=\"Cheating Rate 5%\", alpha=0.4)\nhistogram!(high_cheat_data, color=:red, label=\"Cheating Rate 30%\", alpha=0.4)\nxlabel!(\"Number of Students who Confess to Cheating\")\nylabel!(\"Count\")\nvline!([31], linestyle=:dash, color=:green, linewidth=3, label=\"Observed Outcome\")\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Histograms of the number of simulated confessions obtained under the hypothesis of a 5% cheating rate (blue) and a 30% cheating rate (red). The green line is the observed number of cheating confessions.\n\n\n\n\nFrom Figure 2, we can see:\n\nThe distributions look roughly normal (not surprising given the Central Limit Theorem!).\nThere is some overlap around the 25-42 confession count rate. Lower than 25 students confessing would strongly suggest that the TA is overstating the rate of cheating, while more than 40 would strongly suggest that we are underestimating the cheating rate.\nNote that neither of these is “confirmation” of either theory, but evidence about the relative proportion of cheating being more or less consistent with one of our hypotheses.\nThe outcome of 31 confessions is in the range where the evidence isn’t completely clear either way. However, it appears more likely that this outcome is explained by a lower rate of cheating than a higher rate. For example, under our hypothesis, the “high-tail” p-value of the outcome is:\n\necdf_low = ecdf(low_cheat_data)\n1 - ecdf_low(31)\n\n0.18247999999999998\n\n\nwhile under the TA’s hypothesis, the “low-tail” p-value is\n\necdf_high = ecdf(high_cheat_data)\necdf_high(31)\n\n0.0392\n\n\nSo there is a 18% probability of seeing 31 or more confessions under our assumed 5% cheating rate, while there is only a 4% probability of seeing 31 or fewer confessions under the TA’s assumed 30% cheating rate, which seems to suggest a lower rate is more likely.\nWe can’t draw any strong conclusions from this, but we will talk more later this semester about how to quantify consistency of models with data based on simulations!\nThis interview process is noisy, but appears to work to separate very large differences in cheating rates. On the other hand, it might not work so well if we cared about the difference between 5% cheating and 10% cheating rates, as the difference in the “true” confessions would be swamped by the coin flips. If we wanted to tease out those differences, we could use a weighted coin (to increase the number of “honest” confessions).\n\nLater in the semester, we will look at methods for how we might quantify what base cheating rates are consistent with a given outcome (though we won’t apply them to this example!)."
  },
  {
    "objectID": "solutions/hw01/hw01.html#problem-3",
    "href": "solutions/hw01/hw01.html#problem-3",
    "title": "Homework 1 Solutions",
    "section": "Problem 3",
    "text": "Problem 3\nIn this problem:\n\nWrite a function galton_sim which simulates n Galton board trials (assume the board has 8 board rows, as in the image above) and returns a vector with the number of balls which fall into each bin. You can assume (for now) that the board is fair, e.g. that the probability of a left or right bounce is 0.5; you may want to make this probability a function parameter so you can change it later.\nRun your simulation for a sample of 50 balls. Create a histogram of the results, with each bar corresponding to one bin. Make sure you use a random seed for reproducibility, and label your axes!\nEach Galton board trial can be represented as a realization from a binomial distribution. But as we noted above, by the Central Limit Theorem, the distribution of a large enough number of trials should be approximately normal. Use a quantile-quantile (Q-Q) plot to compare a fitted normal distribution with your simulation results. How well does a normal distribution fit the data?\nRepeat your simulation experiment with 250 trials and compare to a normal distribution. Does it describe the empirical distribution better?\nIf the probability of a left bounce is 70%, what does this do to the fit of a normal distribution? What other distribution might you use if not a normal and why?\n\nSolution:\nFirst, we write the galton_sim function.\n\n# p is the probability of a left bounce\nfunction galton_sim(n_balls, n_rows; p=0.5)\n    n_bins = n_rows + 1 # number of bins balls can fall into\n    # this subfunction simulates the trajectory of a single ball; this is not entirely necessary but cleans things up a bit\n    function simulate_ball(n_rows, p)\n        bin = 0 # start in the center\n        # for every row, see if the ball bounces to the left (0) or to the right (+1)\n        # the accounting here is a little different than might be intuitive: it's easier to calculate the bin if we start from 0 (all left bounces) and just count the number of right increments. We could also have set this up to start in the middle and bounce to the left/right with -/+ 1.\n        for row in 1:n_rows\n            bin = bin + sample([0, 1], ProbabilityWeights([p, 1-p]))  # prob\n        end\n        return bin\n    end\n    # simulate outcome for every ball\n    # the use of a comprehension lets us avoid pre-allocation and looping\n    bins = [simulate_ball(n_rows, p) for _ in 1:n_balls] # the _ just avoids introducing a new counter variable we don't use\n\n    return bins # return simulated outcomes\nend\n\nNext, we run our simulation for 50 balls and create a histogram.\n\nRandom.seed!(1)\n\ngalton_simulation = galton_sim(50, 8)\nhistogram(galton_simulation, legend=:false) # plot histogram\nxlabel!(\"Galton Board Bin\")\nylabel!(\"Ball Count\")\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n50 Ball Galton Board Histogram\n\n\nHow well does a normal distribution fit this data? We use StatsPlots.qqplot() to check.\n\nqqplot(Normal, galton_simulation)\nxlabel!(\"Theoretical Normal Quantiles\")\nylabel!(\"Empirical Quantiles\")\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: 50 Ball Galton Board QQ Plot\n\n\n\n\nFigure 3 might be slightly tricky to interpret as the data is not continuous. We can see how the repeated samples form horizontal lines across the 1-1 line. But in general, the fit is pretty good; the values seem to follow those predicted by a normal once you account for the discretization.\nLet’s repeat this with 250 samples:\n\ngalton_simulation = galton_sim(250, 8)\nhistogram(galton_simulation, legend=:false) # plot histogram\nxlabel!(\"Galton Board Bin\")\nylabel!(\"Ball Count\")\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n250 Ball Galton Board Histogram\n\n\n\nqqplot(Normal, galton_simulation)\nxlabel!(\"Theoretical Normal Quantiles\")\nylabel!(\"Empirical Quantiles\")\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: 250 Ball Galton Board QQ Plot\n\n\n\n\nFigure 4 is similar to the last 50-ball simulation, which is not entirely surprising given that the Central Limit Theorem should only work better with larger samples.\nFinally, let’s test with a 70% probability of a left bounce.\n\ngalton_simulation = galton_sim(250, 8, p=0.7)\nhistogram(galton_simulation, legend=:false) # plot histogram\nxlabel!(\"Galton Board Bin\")\nylabel!(\"Ball Count\")\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Histogram for Galton Board with 70% left bounce.\n\n\n\n\n\nqqplot(Normal, galton_simulation)\nxlabel!(\"Theoretical Normal Quantiles\")\nylabel!(\"Empirical Quantiles\")\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: QQ plot for Galton Board with 70% left bounce.\n\n\n\n\nWe can see a bit more of a skew in Figure 5, though Figure 6 still looks reasonable. We could think about using a SkewNormal distribution to capture that skew."
  },
  {
    "objectID": "solutions/hw01/hw01.html#problem-4",
    "href": "solutions/hw01/hw01.html#problem-4",
    "title": "Homework 1 Solutions",
    "section": "Problem 4",
    "text": "Problem 4\nIn this problem:\n\nWrite down a model which encodes the Showcase rules as a function of the showcase value and your bid. You can assume that your wagering is independent of your opponent.\nSelect and fit a distribution to the above statistics (you have some freedom to pick a distribution, but make sure you justify it).\nUsing 1,000 samples from your price distribution in your model, plot the expected winnings for bids from $20,000 through $72,000.\nFind the bid which maximizes your expected winnings. If you were playing The Price Is Right, is this the strategy you would adopt, or are there other considerations you would take into account which were not included in this model?\n\nSolution:\nHere’s a function for the outcome of a bid:\n\nfunction showcase_model(value, bid)\n    if bid &gt; value\n        winnings = 0\n    elseif value - bid &lt; 250\n        winnings = 2 * value\n    else\n        winnings = value\n    end\n    return winnings\nend\n\nshowcase_model (generic function with 1 method)\n\n\nWe’ll use a truncated normal distribution to reflect an assumption that moderate showcase values (near the median) are more typical than the extremes (this is an assumption, not part of the problem; other choices are fine!).\n\nshowcase_dist = truncated(Normal(33_048, 5_000); lower=20_432, upper=72_409)\nhistogram(rand(showcase_dist, 100_000))\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow let’s write a function to calculate the expected winnings for a given bid, which we will then evaluate through the bidding range.\n\nfunction showcase_winnings(bid, value_dist)\n    # sample 100,000 values of showcases\n    showcase_values = rand(value_dist, 100_000)\n    winnings = [showcase_model(val, bid) for val in showcase_values]\n    return mean(winnings)\nend\n\nbids = 20_000:1_000:72_000\nexp_winnings = [showcase_winnings(bid, showcase_dist) for bid in bids]\nplot(bids, exp_winnings)\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7\n\n\n\n\n\nbids[argmax(exp_winnings)]\n\n20000\n\n\nSo with this assumed distribution, the lowest bid maximizes the expected winnings because there is a large skew to the showcase values, meaning that higher bids pose a correspondingly higher probability of overbidding. This would likely not translate well to the actual show given that the lowest bid also increases the probability of being outbid by the opponent. We also might have actual clues from the showcase itself about the relative value within the distribution, rather than assuming that it’s random."
  },
  {
    "objectID": "weeks/week03.html",
    "href": "weeks/week03.html",
    "title": "Week 3",
    "section": "",
    "text": "Exercise 3 due 2/9/24.",
    "crumbs": [
      "Weeks",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week03.html#announcements",
    "href": "weeks/week03.html#announcements",
    "title": "Week 3",
    "section": "",
    "text": "Exercise 3 due 2/9/24.",
    "crumbs": [
      "Weeks",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week03.html#exercises",
    "href": "weeks/week03.html#exercises",
    "title": "Week 3",
    "section": "Exercises",
    "text": "Exercises\n\nSpurious Correlations",
    "crumbs": [
      "Weeks",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week03.html#homework",
    "href": "weeks/week03.html#homework",
    "title": "Week 3",
    "section": "Homework",
    "text": "Homework\n\nHomework 2: Probability Models",
    "crumbs": [
      "Weeks",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week03.html#lectures",
    "href": "weeks/week03.html#lectures",
    "title": "Week 3",
    "section": "Lectures",
    "text": "Lectures\n\nLecture 03-1: Exploratory Data Analysis\nLecture 03-2: Discrepancy and Bayesian Statistics",
    "crumbs": [
      "Weeks",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week14.html",
    "href": "weeks/week14.html",
    "title": "Week 14",
    "section": "",
    "text": "Lecture 14-1: Model Complexity and Emulation\nLecture 14-2: Emulation Methods",
    "crumbs": [
      "Weeks",
      "Week 14"
    ]
  },
  {
    "objectID": "weeks/week14.html#lectures",
    "href": "weeks/week14.html#lectures",
    "title": "Week 14",
    "section": "",
    "text": "Lecture 14-1: Model Complexity and Emulation\nLecture 14-2: Emulation Methods",
    "crumbs": [
      "Weeks",
      "Week 14"
    ]
  },
  {
    "objectID": "weeks/week04.html",
    "href": "weeks/week04.html",
    "title": "Week 4",
    "section": "",
    "text": "Exercise 4 due 2/16/24.\nNo class Wednesday, 2/14!",
    "crumbs": [
      "Weeks",
      "Week 4"
    ]
  },
  {
    "objectID": "weeks/week04.html#announcements",
    "href": "weeks/week04.html#announcements",
    "title": "Week 4",
    "section": "",
    "text": "Exercise 4 due 2/16/24.\nNo class Wednesday, 2/14!",
    "crumbs": [
      "Weeks",
      "Week 4"
    ]
  },
  {
    "objectID": "weeks/week04.html#exercises",
    "href": "weeks/week04.html#exercises",
    "title": "Week 4",
    "section": "Exercises",
    "text": "Exercises\n\nDistributions and Extreme Values",
    "crumbs": [
      "Weeks",
      "Week 4"
    ]
  },
  {
    "objectID": "weeks/week04.html#lectures",
    "href": "weeks/week04.html#lectures",
    "title": "Week 4",
    "section": "Lectures",
    "text": "Lectures\n\nLecture 04-1: More Bayes and Extreme Values",
    "crumbs": [
      "Weeks",
      "Week 4"
    ]
  },
  {
    "objectID": "weeks/week11.html",
    "href": "weeks/week11.html",
    "title": "Week 11",
    "section": "",
    "text": "Lecture 11-1: Hypothesis Testing",
    "crumbs": [
      "Weeks",
      "Week 11"
    ]
  },
  {
    "objectID": "weeks/week11.html#lectures",
    "href": "weeks/week11.html#lectures",
    "title": "Week 11",
    "section": "",
    "text": "Lecture 11-1: Hypothesis Testing",
    "crumbs": [
      "Weeks",
      "Week 11"
    ]
  },
  {
    "objectID": "weeks/week13.html",
    "href": "weeks/week13.html",
    "title": "Week 13",
    "section": "",
    "text": "Lecture 13-1: Information Criteria\nLecture 13-2: Other Information Criteria",
    "crumbs": [
      "Weeks",
      "Week 13"
    ]
  },
  {
    "objectID": "weeks/week13.html#lectures",
    "href": "weeks/week13.html#lectures",
    "title": "Week 13",
    "section": "",
    "text": "Lecture 13-1: Information Criteria\nLecture 13-2: Other Information Criteria",
    "crumbs": [
      "Weeks",
      "Week 13"
    ]
  },
  {
    "objectID": "weeks/week06.html",
    "href": "weeks/week06.html",
    "title": "Week 6",
    "section": "",
    "text": "No office hours on 3/4.",
    "crumbs": [
      "Weeks",
      "Week 6"
    ]
  },
  {
    "objectID": "weeks/week06.html#announcements",
    "href": "weeks/week06.html#announcements",
    "title": "Week 6",
    "section": "",
    "text": "No office hours on 3/4.",
    "crumbs": [
      "Weeks",
      "Week 6"
    ]
  },
  {
    "objectID": "weeks/week06.html#lectures",
    "href": "weeks/week06.html#lectures",
    "title": "Week 6",
    "section": "Lectures",
    "text": "Lectures\n\nLecture 06-2: Figure Discussion",
    "crumbs": [
      "Weeks",
      "Week 6"
    ]
  },
  {
    "objectID": "weeks/week09.html",
    "href": "weeks/week09.html",
    "title": "Week 9",
    "section": "",
    "text": "Office hour modifications; see Ed.",
    "crumbs": [
      "Weeks",
      "Week 9"
    ]
  },
  {
    "objectID": "weeks/week09.html#announcements",
    "href": "weeks/week09.html#announcements",
    "title": "Week 9",
    "section": "",
    "text": "Office hour modifications; see Ed.",
    "crumbs": [
      "Weeks",
      "Week 9"
    ]
  },
  {
    "objectID": "weeks/week09.html#readings",
    "href": "weeks/week09.html#readings",
    "title": "Week 9",
    "section": "Readings",
    "text": "Readings\n\nRuckert, K. L., Guan, Y., Bakker, A. M. R., Forest, C. E. & Keller, K. The effects of time-varying observation errors on semi-empirical sea-level projections. Clim. Change 140, 349–360 (2017), http://dx.doi.org/10.1007/s10584-016-1858-z",
    "crumbs": [
      "Weeks",
      "Week 9"
    ]
  },
  {
    "objectID": "weeks/week09.html#lectures",
    "href": "weeks/week09.html#lectures",
    "title": "Week 9",
    "section": "Lectures",
    "text": "Lectures\n\nLecture 09-1: Markov Chain Monte Carlo\nLecture 09-2: MCMC Convergence and Example",
    "crumbs": [
      "Weeks",
      "Week 9"
    ]
  },
  {
    "objectID": "assignments/hw03/hw03.html",
    "href": "assignments/hw03/hw03.html",
    "title": "Homework 3: Bayesian and Extreme Value Statistics",
    "section": "",
    "text": "Due Date\n\n\n\nFriday, 3/22/24, 9:00pm",
    "crumbs": [
      "Homework",
      "Homework 3"
    ]
  },
  {
    "objectID": "assignments/hw03/hw03.html#overview",
    "href": "assignments/hw03/hw03.html#overview",
    "title": "Homework 3: Bayesian and Extreme Value Statistics",
    "section": "Overview",
    "text": "Overview\n\nInstructions\nThe goal of this homework assignment is to practice developing and working with probability models for data.\n\n\nLearning Outcomes\nAfter completing this assignments, students will be able to:\n\ndevelop probability models for data and model residuals under a variety of statistical assumptions;\nevaluate the appropriateness of those assumptions through the use of qualitative and quantitative evaluations of goodness-of-fit;\nfit a basic Bayesian model to data.\n\n\n\nLoad Environment\nThe following code loads the environment and makes sure all needed packages are installed. This should be at the start of most Julia scripts.\n\n\nimport Pkg\nPkg.activate(@__DIR__)\nPkg.instantiate()\n\n\nThe following packages are included in the environment (to help you find other similar packages in other languages). The code below loads these packages for use in the subsequent notebook (the desired functionality for each package is commented next to the package).\n\n\nusing Random # random number generation and seed-setting\nusing DataFrames # tabular data structure\nusing DataFramesMeta # API which can simplify chains of DataFrames transformations\nusing CSVFiles # reads/writes .csv files\nusing Distributions # interface to work with probability distributions\nusing Plots # plotting library\nusing StatsBase # statistical quantities like mean, median, etc\nusing StatsPlots # some additional statistical plotting tools\nusing Optim # optimization tools",
    "crumbs": [
      "Homework",
      "Homework 3"
    ]
  },
  {
    "objectID": "assignments/hw03/hw03.html#problems-total-30-points-for-4850-40-for-5850",
    "href": "assignments/hw03/hw03.html#problems-total-30-points-for-4850-40-for-5850",
    "title": "Homework 3: Bayesian and Extreme Value Statistics",
    "section": "Problems (Total: 30 Points for 4850; 40 for 5850)",
    "text": "Problems (Total: 30 Points for 4850; 40 for 5850)\n\nProblem 1\n\nConsider the Rahmstorf (2007) sea-level rise model from Homework 2:\n\\[\\frac{dH(t)}{dt} = \\alpha (T(t) - T_0),\\]\nwhere \\(T_0\\) is the temperature (in \\(^\\circ C\\)) where sea-level is in equilibrium (\\(dH/dt = 0\\)), and \\(\\alpha\\) is the sea-level rise sensitivity to temperature. Discretizing this equation using the Euler method and using an annual timestep (\\(\\delta t = 1\\)), we get \\[H(t+1) = H(t) + \\alpha (T(t) - T_0).\\]\nSuppose that we wanted to develop a Bayesian probability model for this problem, assuming independent normal residuals:\n\\[\\begin{gather*}\ny(t) = F(t) + \\varepsilon_t \\\\\n\\varepsilon_t \\sim \\mathcal{N}(0, \\sigma^2)\n\\end{gather*}\\]\nWe might specify the following priors (assuming independence across parameters):\n\n\\(T_0 \\sim \\mathcal{N}(-0.5, 0.1)\\);\n\\(\\alpha \\sim \\mathcal{TN}(0, 5; 0, \\infty)\\) (truncated normal between 0 and infinity);\n\\(H_0 \\sim \\mathcal{N}(-150, 25)\\);\n\\(\\sigma \\sim \\mathcal{TN}(0, 5; 0, \\infty)\\)\n\nIn this problem:\n\n\nHistorical and RCP 8.5 global mean temperatures from NOAA can be found in data/NOAA_IPCC_RCPtempsscenarios.csv (use the fourth column for the temperature series).\nGlobal mean sea level anomalies (relative to the 1990 mean global sea level) are in data/CSIRO_Recons_gmsl_yr_2015.csv, courtesy of CSIRO (https://www.cmar.csiro.au/sealevel/sl_data_cmar.html).\nSimulate from the prior predictive distribution. What do you think about the priors?\nWould you propose new priors? If so, what might they be and why?\n\n\n\n\nProblem 2\nFollowing from Problem 1, compare the maximum likelihood and maximum a posteriori estimates for the model.\nIn this problem:\n\nFind the MLE and MAP parameter values using the prior distributions given in Problem 1.\nPlot the median and 95% credible intervals for the hindcasts and the projections until 2100 under RCP 8.5 (using data/NOAA_IPCC_RCPtempsscenarios.csv; make sure \\(T_0\\) and \\(H_0\\) have the same meaning as in Problem 1!).\nWhat differences do you observe? What do you attribute these differences to? What conclusions can you draw about the Bayesian model?\n\n\n\nProblem 3\nLet’s look at how (modeled) daily maximum temperatures have (or have not) increased in Ithaca from 1850–2014. Model output from NOAA’s GFDL-ESM4 climate model (one of the models used in the latest Climate Model Intercomparison Project, CMIP6) is available in data/gfdl-esm4-tempmax-ithaca.csv. While this model output has not been bias-corrected, we won’t worry about that for the purposes of this assignment.\nIn this problem:\n\nLoad and plot the temperature maxima data from data/gfdl-esm4-tempmax-ithaca.csv.\nSuppose that we were interested in looking at temperature exceedances over 28°C. Decluster these occurrences and plot the number of exceedances by year. Have they increased over time?\nFit a stationary GPD model for the exceedances. What does this distribution look like?\n\n\n\nProblem 4\nGRADED FOR 5850 STUDENTS ONLY\nIn class, we modeled the annual maxima of the San Francisco tide gauge data using a stationary GEV distribution. We could also hypothesize that the tide extremes are influenced by the Pacific Decadal Oscillation (PDO), which is a climate pattern related to the sea-surface temperatures in the Pacific Ocean (similar to the El Niño-Southern Oscillation), in the following fashion (where only the GEV location is variable):\n\\[y_t \\sim \\text{GEV}(\\mu_0 + \\mu_1 p_t, \\sigma, \\xi)\\]\nIn this problem:\n\nLoad the San Francisco tide gauge data (data/h551.csv) and the PDO index dataset (data/errst.v5.pdo.dat; this file is a space-delimited file, versus the comma-delimited .csv files, which can be loaded in Julia with CSV.read(data/errst.v5.pdo.dat, DataFrame; delim=\" \", header=2, ignorerepeated=true)). The PDO data is given as monthly values; convert these to yearly indices by taking the mean. You should also drop 2023 due to the incomplete record. You can use the function at the bottom of these instructions to load the data, or adapt accordingly to a different language.\nFind the MLE of the non-stationary GEV model and for a stationary GEV (constant \\(\\mu\\); we did this in class).\nDiscuss the difference(s) between the two fitted models based on the coefficient values (you can also bring to bear the range(s) of PDO values from the data), the 100- and 500-year return periods in 2022, and plotted hindcasts.\n\n\n## load the data from the file and return a DataFrame of DateTime values and gauge measurements\n\nfunction load_pdo(fname)\n    # This uses the DataFramesMeta.jl package, which makes it easy to string together commands to load and process data\n    df = DataFrame(CSVFiles.load(File(format\"CSV\", fname), spacedelim=true, skiplines_begin=1))\n    # take yearly average\n    @transform!(df, :PDO = mean(AsTable(names(df)[2:13])))\n    @select!(df, $[:Year, :PDO])\n    @rsubset!(df, :Year != 2023)\n    return df\nend\n\npdo = load_pdo(\"data/ersst.v5.pdo.dat\")\n# subset for years that match the tide gauge data\nyears = pdo[!, :Year]\n@rsubset!(pdo, :Year in years)\n\n\n169×2 DataFrame144 rows omitted\n\n\n\nRow\nYear\nPDO\n\n\n\nInt64\nFloat64\n\n\n\n\n1\n1854\n-0.818333\n\n\n2\n1855\n-0.743333\n\n\n3\n1856\n-0.2075\n\n\n4\n1857\n-0.0266667\n\n\n5\n1858\n0.196667\n\n\n6\n1859\n-1.74333\n\n\n7\n1860\n-0.954167\n\n\n8\n1861\n-0.955\n\n\n9\n1862\n-0.226667\n\n\n10\n1863\n-0.095\n\n\n11\n1864\n-0.281667\n\n\n12\n1865\n-0.553333\n\n\n13\n1866\n-0.9425\n\n\n⋮\n⋮\n⋮\n\n\n158\n2011\n-1.81167\n\n\n159\n2012\n-1.73333\n\n\n160\n2013\n-1.16583\n\n\n161\n2014\n0.554167\n\n\n162\n2015\n0.920833\n\n\n163\n2016\n0.673333\n\n\n164\n2017\n-0.0933333\n\n\n165\n2018\n-0.365\n\n\n166\n2019\n-0.154167\n\n\n167\n2020\n-1.145\n\n\n168\n2021\n-1.87417\n\n\n169\n2022\n-2.115",
    "crumbs": [
      "Homework",
      "Homework 3"
    ]
  },
  {
    "objectID": "assignments/hw01/hw01.html",
    "href": "assignments/hw01/hw01.html",
    "title": "Homework 1: Simulation-Based Data Analysis",
    "section": "",
    "text": "Due Date\n\n\n\nFriday, 2/2/24, 9:00pm",
    "crumbs": [
      "Homework",
      "Homework 1"
    ]
  },
  {
    "objectID": "assignments/hw01/hw01.html#overview",
    "href": "assignments/hw01/hw01.html#overview",
    "title": "Homework 1: Simulation-Based Data Analysis",
    "section": "Overview",
    "text": "Overview\n\nInstructions\nThe goal of this homework assignment is to introduce you to simulation-based data analysis.\n\nProblem 1 asks you to explore whether a difference between data collected from two groups might be statistically meaningful or the result of noise. This problem repeats the analysis from Statistics Without The Agonizing Pain by John Rauser (which is a neat watch!).\nProblem 2 asks you to evaluate an interview method for finding the level of cheating on a test to determine whether cheating was relatively high or low. This problem was adapted from Bayesian Methods for Hackers.\nProblem 3 asks you to assess the quality of fit of a normal distribution to realizations from a Galton Board simulation.\nProblem 4 (graded only for graduate students) asks you to simulate outcomes from the Price Is Right Showcase game to identify a bidding strategy.\n\n\n\nLoad Environment\nThe following code loads the environment and makes sure all needed packages are installed. This should be at the start of most Julia scripts.\n\n\nimport Pkg\nPkg.activate(@__DIR__)\nPkg.instantiate()\n\n\nThe following packages are included in the environment (to help you find other similar packages in other languages). The code below loads these packages for use in the subsequent notebook (the desired functionality for each package is commented next to the package).\n\n\nusing Random # random number generation and seed-setting\nusing DataFrames # tabular data structure\nusing CSVFiles # reads/writes .csv files\nusing Distributions # interface to work with probability distributions\nusing Plots # plotting library\nusing StatsBase # statistical quantities like mean, median, etc\nusing StatsPlots # some additional statistical plotting tools",
    "crumbs": [
      "Homework",
      "Homework 1"
    ]
  },
  {
    "objectID": "assignments/hw01/hw01.html#problems-total-20-points",
    "href": "assignments/hw01/hw01.html#problems-total-20-points",
    "title": "Homework 1: Simulation-Based Data Analysis",
    "section": "Problems (Total: 20 Points)",
    "text": "Problems (Total: 20 Points)\n\nProblem 1 (10 points)\nThe underlying question we would like to address is: what is the influence of drinking beer on the likelihood of being bitten by mosquitoes? There is a mechanistic reason why this might occur: mosquitoes are attracted by changes in body temperature and released CO2, and it might be that drinking beer induces these changes. We’ll analyze this question using (synthetic) data which separates an experimental population into two groups, one which drank beer and the other which drank only water.\nFirst, we’ll load data for the number of bites reported by the participants who drank beer. This is in a comma-delimited file, data/bites.csv (which is grossly overkill for this assignment). Each row contains two columns: the group (beer and water) the person belonged to and the number of times that person was bitten.\nIn Julia, we can do this using CSVFiles.jl, which will read in the .csv file into a DataFrame, which is a typical data structure for tabular data (and equivalent to a Pandas DataFrame in Python or a dataframe in R).\n\n\ndata = DataFrame(load(\"data/bites.csv\")) # load data into DataFrame\n\n# print data variable (semi-colon suppresses echoed output in Julia, which in this case would duplicate the output)\n@show data; \n\n\nHow can we tell if there’s a meaningful difference between the two groups? Naively, we might just look at the differences in group means.\n\n\n\n\n\n\n\nBroadcasting\n\n\n\nThe subsetting operations in the below code use .==, which “broadcasts” the element-wise comparison operator == across every element. The decimal in front of == indicates that this should be used element-wise (every pair of elements compared for equality, returning a vector of true or false values); otherwise Julia would try to just check for vector equality (returning a single true or false value).\nBroadcasting is a very specific feature of Julia, so this syntax would look different in a different programming language.\n\n\n\n# split data into vectors of bites for each group\nbeer = data[data.group .== \"beer\", :bites]\nwater = data[data.group .== \"water\", :bites]\n\nobserved_difference = mean(beer) - mean(water)\n@show observed_difference;\n\nobserved_difference = 4.37777777777778\n\n\n\nThis tells us that, on average, the participants in the experiment who drank beer were bitten approximately 4.4 more times than the participants who drank water! Does that seem like a meaningful difference, or could it be the result of random chance?\nWe will use a simulation approach to address this question, as follows.\n\nSuppose someone is skeptical of the idea that drinking beer could result in a higher attraction to mosquitoes, and therefore more bites. To this skeptic, the two datasets are really just different samples from the same underlying population of people getting bitten by mosquitoes, rather than two different populations with different propensities for being bitten. This is the skeptic’s hypothesis, versus our hypothesis that drinking beer changes body temperature and CO2 release sufficiently to attract mosquitoes.\nIf the skeptic’s hypothesis is true, then we can “shuffle” all of the measurements between the two datasets and re-compute the differences in the means. After repeating this procedure a large number of times, we would obtain a distribution of the differences in means under the assumption that the skeptic’s hypothesis is true.\nComparing our experimentally-observed difference to this distribution, we can then evaluate the consistency of the skeptic’s hypothesis with the experimental results.\n\n\n\n\n\n\n\n\nWhy Do We Call This A Simulation-Based Approach?\n\n\n\nThis is a simulation-based approach because the “shuffling” is a non-parametric way of generating new samples from the underlying distribution (more on this later in the semester).\nThe alternative to this approach is to use a statistical test, such as a t-test, which may have other assumptions which may not be appropriate for this setting, particularly given the seemingly small sample sizes.\n\n\n\nIn this problem:\n\nConduct the above procedure to generate 50,000 simulated datasets under the skeptic’s hypothesis.\nPlot a histogram of the results and add a dashed vertical line to show the experimental difference (if you are using Julia, feel free to look at the Making Plots with Julia tutorial on the class website).\nDraw conclusions about the plausibility of the skeptic’s hypothesis that there is no difference between groups. Feel free to use any quantitative or qualitative assessments of your simulations and the observed difference.\n\n\n\nProblem 2 (10 points)\nYou are trying to detect if how prevalent cheating was on an exam. You are skeptical of the efficacy of just asking the students if they cheated. You are also concerned about privacy — your goal is not to punish individual students, but to see if there are systemic problems that need to be addressed. Someone proposes the following interview procedure, which the class agrees to participate in:\n\nEach student flips a fair coin, with the results hidden from the interviewer. The student answers honestly if the coin comes up heads. Otherwise, if the coin comes up tails, the student flips the coin again, and answers “I did cheat” if heads, and “I did not cheat”, if tails.\n\nWe have a hypothesis that cheating was not prevalent, and the proportion of cheaters was no more than 5% of the class; in other words, we expect 5 “true” cheaters out of a class of 100 students. Our TA is more jaded and thinks that cheating was more rampant, and that 30% of the class cheated. The proposed interview procedure is noisy: the interviewer does not know if an admission means that the student cheated, or the result of a heads. However, it gives us a data-generating process that we can model and analyze for consistency with our hypothesis and that of the TA.\nIn this problem:\n\nDerive and code a simulation model for the above interview procedure given the “true” probability of cheating \\(p\\).\nSimulate your model (for a class of 100 students) 50,000 times under the your hypothesis and the TA’s hypothesis, and plot the two resulting datasets.\nIf you received 31 “Yes, I cheated” responses while interviewing your class, what could you conclude? Feel free to use any qualitative or quantiative assessments to justify your conclusions.\nHow useful do you think the interview procedure is to identify systemic teaching? What changes to the design might you make?\n\n\n\nProblem 3 (10 points)\nSuppose that we ask a class of students to walk individually from one end of a field to the other and count the number of steps that they take in order to cover the distance. More than likely, each student will get a somewhat different answer. After each student has paced off the field, we then make a histogram of the number of steps taken by the individual students. For a sufficiently large number of students, this histogram would resemble a normal distribution, due to the Central Limit Theorem.\nA Galton board (named after Sir Francis Galton, a pioneering statistician1, shown below, is a physical device that demonstrates the Central Limit Theorem. A Galton board has rows of pins arranged in a triangular shape, with each row of pins offset horizontally relative to the rows above and below it. As illustrated in Figure 1, the top row has one pin, the second row has two pins, and so forth. If you’ve watched The Price is Right, you’ve seen this as “Plinko”.\n1 And, notably, a leading eugenicist, which is unfortunately a recurring theme with leading early statisticians…)\n\n\n\n\n\n\nFigure 1: Galton Board simulation. The top shows a schematic of how the Galton board works and the bottom shows a histogram of the landing positions after 200 simulations.\n\n\n\n\nIf a ball is dropped into the Galton board, it falls either to the right or the left when it bounces off the top pin. The ball then falls all the way to the bottom of the board, moving slightly to the right or left as it passes through each row of pins. Bins at the bottom of the board capture the ball and record its final position. If this experiment is repeated with many balls, the number of balls in each bin resembles a normal distribution, which is expected due to the Central Limit Theorem.\nYour goal in this exercise is to explore how well a normal distribution fits the outcomes of repeated Galton Board trials as the number increases using quantile-quantile plots. Assessing the appropriateness of a probability model for a data set is a key part of exploratory analysis; we will return to this theme repeatedly. Or, to put it another way, it’s time for you to play Plinko!\nIn this problem:\n\nWrite a function galton_sim which simulates n Galton board trials (assume the board has 8 board rows, as in the image above) and returns a vector with the number of balls which fall into each bin. You can assume (for now) that the board is fair, e.g. that the probability of a left or right bounce is 0.5; you may want to make this probability a function parameter so you can change it later.\nRun your simulation for a sample of 50 balls. Create a histogram of the results, with each bar corresponding to one bin. Make sure you use a random seed for reproducibility, and label your axes!\nEach Galton board trial can be represented as a realization from a binomial distribution. But as we noted above, by the Central Limit Theorem, the distribution of a large enough number of trials should be approximately normal. Use a quantile-quantile (Q-Q) plot to compare a fitted normal distribution with your simulation results. How well does a normal distribution fit the data?\nRepeat your simulation experiment with 250 trials and compare to a normal distribution. Does it describe the empirical distribution better?\nIf the probability of a left bounce is 70%, what does this do to the fit of a normal distribution? What other distribution might you use if not a normal and why?\n\n\n\nProblem 4 (10 points)\nGRADED FOR 5850 STUDENTS ONLY\nYour mastery of the Central Limit Theorem has led you to win your game of Plinko, and it’s time for the Showcases. This is the final round of an episode of The Price is Right, matching the two big winners from the episode. Each contestant is shown a “showcase” of prizes, which are usually some combination of a trip, a motor vehicle, some furniture, and maybe some other stuff. They then each have to make a bid on the retail price of the showcase. The rules are:\n\nan overbid is an automatic loss;\nthe contest who gets closest to the retail price wins their showcase;\nif a contestant gets within \\(\\$250\\) of the retail price and is closer than their opponent, they win both showcases.\n\nThanks to exhaustive statistics kept by game show fans, in Season 51 of The Price is Right showcase values had the following properties:\n\nmedian: $33,0481;\nlowest-value: $20,432\nhighest-value: $72,409\n\nIn this problem:\n\nWrite down a model which encodes the Showcase rules as a function of the showcase value and your bid. You can assume that your wagering is independent of your opponent.\nSelect and fit a distribution to the above statistics (you have some freedom to pick a distribution, but make sure you justify it).\nUsing 1,000 samples from your price distribution in your model, plot the expected winnings for bids from $20,000 through $72,000.\nFind the bid which maximizes your expected winnings. If you were playing The Price Is Right, is this the strategy you would adopt, or are there other considerations you would take into account which were not included in this model?",
    "crumbs": [
      "Homework",
      "Homework 1"
    ]
  },
  {
    "objectID": "assignments/hw04/hw04.html",
    "href": "assignments/hw04/hw04.html",
    "title": "Homework 4: Probabilistic Calibration and Model Selection",
    "section": "",
    "text": "Due Date\n\n\n\nFriday, 5/03/24, 9:00pm",
    "crumbs": [
      "Homework",
      "Homework 4"
    ]
  },
  {
    "objectID": "assignments/hw04/hw04.html#overview",
    "href": "assignments/hw04/hw04.html#overview",
    "title": "Homework 4: Probabilistic Calibration and Model Selection",
    "section": "Overview",
    "text": "Overview\n\n\nInstructions\nThe goal of this homework assignment is to practice probabilistic model calibration and using the resulting distributions for model evaluation and selection.\n\n\nLearning Outcomes\nAfter completing this assignments, students will be able to:\n\nquantify uncertainty in model parameters with probabilistic programming.\nuse information criteria to assess the relative evidence for models.\n\n\n\nLoad Environment\nThe following code loads the environment and makes sure all needed packages are installed. This should be at the start of most Julia scripts.\n\n\n\nimport Pkg\nPkg.activate(@__DIR__)\nPkg.instantiate()\n\nThe following packages are included in the environment (to help you find other similar packages in other languages). The code below loads these packages for use in the subsequent notebook (the desired functionality for each package is commented next to the package).\n\nusing Random # random number generation and seed-setting\nusing DataFrames # tabular data structure\nusing DataFramesMeta # API which can simplify chains of DataFrames transformations\nusing CSVFiles # reads/writes .csv files\nusing Distributions # interface to work with probability distributions\nusing Plots # plotting library\nusing StatsBase # statistical quantities like mean, median, etc\nusing StatsPlots # some additional statistical plotting tools",
    "crumbs": [
      "Homework",
      "Homework 4"
    ]
  },
  {
    "objectID": "assignments/hw04/hw04.html#problems-total-30-points",
    "href": "assignments/hw04/hw04.html#problems-total-30-points",
    "title": "Homework 4: Probabilistic Calibration and Model Selection",
    "section": "Problems (Total: 30 Points)",
    "text": "Problems (Total: 30 Points)\n\nProblem 1\nThis problem is the same as the MCMC Lab from earlier in this semester.\nYou will use a probabilistic programming language to fit a linear model for monthly mean tide gauge data from the Sewell’s Point, VA tide gauge from 1927 through 2022, obtained from the Permanent Service for Mean Sea Level. The data (in data/norfolk-monthly-tide-data.txt) has been slightly cleaned by setting dates to the yyyy-mm format. We’ve left missing values as -99999; make sure to fix those as appropriate for your programming language.\n\ntide_dat = CSV.read(\"data/norfolk-monthly-tide-data.txt\", DataFrame)\n# replace -99999 with missing\ntide_dat.gauge = ifelse.(tide_dat.gauge .== -99999, missing, tide_dat.gauge)\n\nLoadError: UndefVarError: `CSV` not defined\n\n\nNow let’s plot the data.\n\np = scatter(tide_dat.datetime, tide_dat.gauge, xlabel=\"Month\", ylabel=\"Monthly Mean Sea Level (mm)\", legend=false)\ndisplay(p)\n\nLoadError: UndefVarError: `tide_dat` not defined\n\n\nWe would like to quantify the uncertainty in the time-trend of this local sea level increase (which includes global mean sea level rise but also more local effects, such as subsidence). The plot in looks roughly linear, so let’s use the following model (assuming the errors are independent and identically-distributed for simplicity):\n\\[\\begin{equation*}\n\\begin{aligned}\ny(t) &= \\alpha + \\beta t + \\varepsilon \\\\\n\\varepsilon &\\sim \\mathcal{N}(0, \\sigma).\n\\end{aligned}\n\\label{eq-model}\n\\end{equation*}\\]\nIn this problem:\n\nWrite a model for the linear regression above in the probabilistic programming language of your choice. You’ll need to pick some priors for \\(\\alpha\\), \\(\\beta\\), and \\(\\sigma\\).\nSample from the posterior with four chains (for convergence diagnostics).\nEvaluate convergence. How many iterations did you use? What is the effective sample size?\nPlot the posterior distributions. In particular, we are interested in uncertainty in the \\(\\beta\\) coefficient, which reflects the mean increase in sea-level rise over time in mm/months.\nGenerate hindcasts by sampling from the posterior distribution and simulating data. If you plot the 95% posterior predictive distribution and the data, how does it look?\n\n\n\nProblem 2\nBuilding on Problem 1, suppose we wanted to examine if the increase in sea level at Norfolk was quadratic instead of linear.\nIn this problem:\n\nFit a quadratic model (with the same error structure) to the same data. What are the estimates?\nPlot the linear and quadratic fits along with the data. Do you visually see any differences?\nCompute the log-posterior, AIC, and DIC for each model. Based on these metrics, what would you conclude about the relative evidence for each model? What are your conclusions about whether the sea level trend is linear or quadratic?",
    "crumbs": [
      "Homework",
      "Homework 4"
    ]
  },
  {
    "objectID": "assignments/hw02/hw02.html",
    "href": "assignments/hw02/hw02.html",
    "title": "Homework 2: Probability Models",
    "section": "",
    "text": "Due Date\n\n\n\nFriday, 2/23/24, 9:00pm",
    "crumbs": [
      "Homework",
      "Homework 2"
    ]
  },
  {
    "objectID": "assignments/hw02/hw02.html#overview",
    "href": "assignments/hw02/hw02.html#overview",
    "title": "Homework 2: Probability Models",
    "section": "Overview",
    "text": "Overview\n\nInstructions\nThe goal of this homework assignment is to practice developing and working with probability models for data.\n\nProblem 1 asks you to fit a sea-level rise model using normal residuals and to assess the validity of that assumption.\nProblem 2 asks you to model the time series of hourly weather-related variability at a tide gauge.\nProblem 3 asks you to model the occurrences of Cayuga Lake freezing, and is only slightly adapted from Example 4.1 in Statistical Methods in the Atmospheric Sciences by Daniel Wilks.\nProblem 4 (graded only for graduate students) asks you to revisit the sea-level model in Problem 1 by including a model-data discrepancy term in the model calibration.\n\n\n\nLearning Outcomes\nAfter completing this assignments, students will be able to:\n\ndevelop probability models for data and model residuals under a variety of statistical assumptions;\nevaluate the appropriateness of those assumptions through the use of qualitative and quantitative evaluations of goodness-of-fit;\nfit a basic Bayesian model to data.\n\n\n\nLoad Environment\nThe following code loads the environment and makes sure all needed packages are installed. This should be at the start of most Julia scripts.\n\n\nimport Pkg\nPkg.activate(@__DIR__)\nPkg.instantiate()\n\n\nThe following packages are included in the environment (to help you find other similar packages in other languages). The code below loads these packages for use in the subsequent notebook (the desired functionality for each package is commented next to the package).\n\n\nusing Random # random number generation and seed-setting\nusing DataFrames # tabular data structure\nusing CSVFiles # reads/writes .csv files\nusing Distributions # interface to work with probability distributions\nusing Plots # plotting library\nusing StatsBase # statistical quantities like mean, median, etc\nusing StatsPlots # some additional statistical plotting tools\nusing Optim # optimization tools",
    "crumbs": [
      "Homework",
      "Homework 2"
    ]
  },
  {
    "objectID": "assignments/hw02/hw02.html#problems-total-30-points-for-4850-40-for-5850",
    "href": "assignments/hw02/hw02.html#problems-total-30-points-for-4850-40-for-5850",
    "title": "Homework 2: Probability Models",
    "section": "Problems (Total: 30 Points for 4850; 40 for 5850)",
    "text": "Problems (Total: 30 Points for 4850; 40 for 5850)\n\nProblem 1\n\nConsider the following sea-level rise model from Rahmstorf (2007):\n\\[\\frac{dH(t)}{dt} = \\alpha (T(t) - T_0),\\] where \\(T_0\\) is the temperature (in \\(^\\circ C\\)) where sea-level is in equilibrium (\\(dH/dt = 0\\)), and \\(\\alpha\\) is the sea-level rise sensitivity to temperature. Discretizing this equation using the Euler method and using an annual timestep (\\(\\delta t = 1\\)), we get \\[H(t+1) = H(t) + \\alpha (T(t) - T_0).\\]\nIn this problem:\n\nLoad the data from the data/ folder\n\nGlobal mean temperature data from the HadCRUT 5.0.2.0 dataset (https://hadobs.metoffice.gov.uk/hadcrut5/data/HadCRUT.5.0.2.0/download.html) can be found in data/HadCRUT.5.0.2.0.analysis.summary_series.global.annual.csv. This data is averaged over the Northern and Southern Hemispheres and over the whole year.\nGlobal mean sea level anomalies (relative to the 1990 mean global sea level) are in data/CSIRO_Recons_gmsl_yr_2015.csv, courtesy of CSIRO (https://www.cmar.csiro.au/sealevel/sl_data_cmar.html).\n\nFit the model under the assumption of normal i.i.d. residuals by maximizing the likelihood and report the parameter estimates. Note that you will need another parameter \\(H_0\\) for the initial sea level. What can you conclude about the relationship between global mean temperature increases and global mean sea level rise?\nHow appropriate was the normal i.i.d. probability model for the residuals? Use any needed quantitative or qualitative assessments of goodness of fit to justify your answer. If this was not an appropriate probability model, what would you change?\n\n\n\n\nProblem 2\n\nTide gauge data is complicated to analyze because it is influenced by different harmonic processes (such as the linear cycle). In this problem, we will develop a model for this data using NOAA data from the Sewell’s Point tide gauge outside of Norfolk, VA from data/norfolk-hourly-surge-2015.csv. This is hourly data (in m) from 2015 and includes both the observed data (Verified (m)) and the tide level predicted by NOAA’s sinusoidal model for periodic variability, such as tides and other seasonal cycles (Predicted (m)).\nIn this problem: * Load the data file. Take the difference between the observations and the sinusoidal predictions to obtain the tide level which could be attributed to weather-related variability (since for one year sea-level rise and other factors are unlikely to matter). Plot this data. * Develop an autoregressive model for the weather-related variability in the Norfolk tide gauge. Make sure to include your logic or exploratory analysis used in determining the model specification. * Use your model to simulate 1,000 realizations of hourly tide gauge observations. What is the distribution of the maximum tide level? How does this compare to the observed value?",
    "crumbs": [
      "Homework",
      "Homework 2"
    ]
  },
  {
    "objectID": "assignments/hw02/hw02.html#problem-3",
    "href": "assignments/hw02/hw02.html#problem-3",
    "title": "Homework 2: Probability Models",
    "section": "Problem 3",
    "text": "Problem 3\n\nAs of 2010, Cayuga Lake has frozen in the following years: 1796, 1816, 1856, 1875, 1884, 1904, 1912, 1934, 1961, and 1979. Based on this data, we would like to project whether Cayuga Lake is likely to freeze again in the next 25 years.\nIn this problem:\n\nAssuming that observations began in 1780, write down a Bayesian model for whether Cayuga Lake will freeze in a given year, using a Bernoulli distribution. How did you select what prior to use?\nFind the maximum a posteriori estimate using your model.\nGenerate 1,000 realizations of Cayuga Lake freezing occurrences from 1780 to 2010 and check the simulations against the occurrance data.\nUsing your model, calculate the probability of Cayuga Lake freezing at least once in the next 10 years.\nWhat do you think about the validity of your model, both in terms of its ability to reproduce historical data and its use to make future projections? Why might you believe or discount it? What changes might you make (include thoughts about the prior)?\n\n\n\nProblem 4\nGRADED FOR 5850 STUDENTS ONLY\nFor the sea-level model in Problem 1, model the model-data discrepancy using an AR(1) process, with observation error modeled as normally distributed with standard deviation given by the uncertainty column in the data file.\nIn this problem:\n\nFind the maximum likelihood estimate of the parameters with this discrepancy structure. How does the parameter inference change from the normal i.i.d. estimate in Problem 1?\nGenerate 1,000 traces, plot a comparison of the hindcasts to those from Problem 1, and compare the surprise indices.\nDetermine whether you have accounted for autocorrelation in the residuals appropriately (hint: generate realizations of just the discrepancy series, compute the resulting residuals from the model fit + discrepancy, and look at the distribution of autocorrelation values).\nWhich model specification would you prefer and why?\n\n:::",
    "crumbs": [
      "Homework",
      "Homework 2"
    ]
  },
  {
    "objectID": "weeks/week08.html",
    "href": "weeks/week08.html",
    "title": "Week 8",
    "section": "",
    "text": "No office hours on 3/18.",
    "crumbs": [
      "Weeks",
      "Week 8"
    ]
  },
  {
    "objectID": "weeks/week08.html#announcements",
    "href": "weeks/week08.html#announcements",
    "title": "Week 8",
    "section": "",
    "text": "No office hours on 3/18.",
    "crumbs": [
      "Weeks",
      "Week 8"
    ]
  },
  {
    "objectID": "weeks/week08.html#readings",
    "href": "weeks/week08.html#readings",
    "title": "Week 8",
    "section": "Readings",
    "text": "Readings\n\nRahmstorf, S., & Coumou, D. (2011). Increase of extreme events in a warming world. Proceedings of the National Academy of Sciences of the United States of America, 108(44), 17905–17909. https://doi.org/10.1073/pnas.1101766108",
    "crumbs": [
      "Weeks",
      "Week 8"
    ]
  },
  {
    "objectID": "weeks/week08.html#exercises",
    "href": "weeks/week08.html#exercises",
    "title": "Week 8",
    "section": "Exercises",
    "text": "Exercises\n\nNon-Parametric vs. Parametric Bootstrap",
    "crumbs": [
      "Weeks",
      "Week 8"
    ]
  },
  {
    "objectID": "weeks/week08.html#lectures",
    "href": "weeks/week08.html#lectures",
    "title": "Week 8",
    "section": "Lectures",
    "text": "Lectures\n\nLecture 08-1: The Parametric Bootstrap\nLecture 08-2: Bayesian Computation and Markov Chains",
    "crumbs": [
      "Weeks",
      "Week 8"
    ]
  },
  {
    "objectID": "weeks/week12.html",
    "href": "weeks/week12.html",
    "title": "Week 12",
    "section": "",
    "text": "Homework 4: Probabilistic Calibration and Model Assessment",
    "crumbs": [
      "Weeks",
      "Week 12"
    ]
  },
  {
    "objectID": "weeks/week12.html#homework",
    "href": "weeks/week12.html#homework",
    "title": "Week 12",
    "section": "",
    "text": "Homework 4: Probabilistic Calibration and Model Assessment",
    "crumbs": [
      "Weeks",
      "Week 12"
    ]
  },
  {
    "objectID": "weeks/week12.html#lectures",
    "href": "weeks/week12.html#lectures",
    "title": "Week 12",
    "section": "Lectures",
    "text": "Lectures\n\nLecture 12-1: Model Assessment\nLecture 12-2: Cross-Validation",
    "crumbs": [
      "Weeks",
      "Week 12"
    ]
  },
  {
    "objectID": "weeks/week07.html",
    "href": "weeks/week07.html",
    "title": "Week 7",
    "section": "",
    "text": "No office hours on 3/4.",
    "crumbs": [
      "Weeks",
      "Week 7"
    ]
  },
  {
    "objectID": "weeks/week07.html#announcements",
    "href": "weeks/week07.html#announcements",
    "title": "Week 7",
    "section": "",
    "text": "No office hours on 3/4.",
    "crumbs": [
      "Weeks",
      "Week 7"
    ]
  },
  {
    "objectID": "weeks/week07.html#readings",
    "href": "weeks/week07.html#readings",
    "title": "Week 7",
    "section": "Readings",
    "text": "Readings\n\nKale, A., Kay, M., & Hullman, J. (2021). Visual reasoning strategies for effect size judgments and decisions. IEEE Transactions on Visualization and Computer Graphics, 27(2), 272–282. https://doi.org/10.1109/TVCG.2020.3030335",
    "crumbs": [
      "Weeks",
      "Week 7"
    ]
  },
  {
    "objectID": "weeks/week07.html#exercises",
    "href": "weeks/week07.html#exercises",
    "title": "Week 7",
    "section": "Exercises",
    "text": "Exercises\n\nMonte Carlo and Distributional Assumptions",
    "crumbs": [
      "Weeks",
      "Week 7"
    ]
  },
  {
    "objectID": "weeks/week07.html#lectures",
    "href": "weeks/week07.html#lectures",
    "title": "Week 7",
    "section": "Lectures",
    "text": "Lectures\n\nLecture 07-1: Monte Carlo Simulation\nLecture 07-2: The Bootstrap",
    "crumbs": [
      "Weeks",
      "Week 7"
    ]
  },
  {
    "objectID": "weeks/week05.html",
    "href": "weeks/week05.html",
    "title": "Week 5",
    "section": "",
    "text": "Exercise 5 due 2/23/24.",
    "crumbs": [
      "Weeks",
      "Week 5"
    ]
  },
  {
    "objectID": "weeks/week05.html#announcements",
    "href": "weeks/week05.html#announcements",
    "title": "Week 5",
    "section": "",
    "text": "Exercise 5 due 2/23/24.",
    "crumbs": [
      "Weeks",
      "Week 5"
    ]
  },
  {
    "objectID": "weeks/week05.html#exercises",
    "href": "weeks/week05.html#exercises",
    "title": "Week 5",
    "section": "Exercises",
    "text": "Exercises\n\nGood and Bad Visualizations",
    "crumbs": [
      "Weeks",
      "Week 5"
    ]
  },
  {
    "objectID": "weeks/week05.html#lectures",
    "href": "weeks/week05.html#lectures",
    "title": "Week 5",
    "section": "Lectures",
    "text": "Lectures\n\nLecture 05-1: Extreme Value Theory and Models\nLecture 05-2: Data Visualization",
    "crumbs": [
      "Weeks",
      "Week 5"
    ]
  },
  {
    "objectID": "weeks/week10.html",
    "href": "weeks/week10.html",
    "title": "Week 10",
    "section": "",
    "text": "Have a great spring break!",
    "crumbs": [
      "Weeks",
      "Week 10"
    ]
  },
  {
    "objectID": "weeks/week10.html#announcements",
    "href": "weeks/week10.html#announcements",
    "title": "Week 10",
    "section": "",
    "text": "Have a great spring break!",
    "crumbs": [
      "Weeks",
      "Week 10"
    ]
  },
  {
    "objectID": "weeks/week01.html",
    "href": "weeks/week01.html",
    "title": "Week 1",
    "section": "",
    "text": "Welcome to BEE 4850/5850! Make sure to read the syllabus and make sure you’re enrolled in Gradescope and Ed Discussion.\nHomework 1 assigned, due 2/2/24.",
    "crumbs": [
      "Weeks",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/week01.html#announcements",
    "href": "weeks/week01.html#announcements",
    "title": "Week 1",
    "section": "",
    "text": "Welcome to BEE 4850/5850! Make sure to read the syllabus and make sure you’re enrolled in Gradescope and Ed Discussion.\nHomework 1 assigned, due 2/2/24.",
    "crumbs": [
      "Weeks",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/week01.html#readings",
    "href": "weeks/week01.html#readings",
    "title": "Week 1",
    "section": "Readings",
    "text": "Readings\nAs you’re reading along, please use the collaborative annotation in Canvas to make notes and read and respond to others’ thoughts and questions. Post a summary of key points, takeaways, and thoughts on Ed Discussion and be prepared to engage in the discussion on-line. We will not dedicate class time to this reading but relevant concepts will come up over the first few weeks of class.\n\nRockmore, D. (2024, January 15). How Much of the World Is It Possible to Model? The New Yorker. Retrieved from https://www.newyorker.com/culture/annals-of-inquiry/how-much-of-the-world-is-it-possible-to-model",
    "crumbs": [
      "Weeks",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/week01.html#exercises",
    "href": "weeks/week01.html#exercises",
    "title": "Week 1",
    "section": "Exercises",
    "text": "Exercises\n\nLoading, Plotting, and Reasoning About Data",
    "crumbs": [
      "Weeks",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/week01.html#lectures",
    "href": "weeks/week01.html#lectures",
    "title": "Week 1",
    "section": "Lectures",
    "text": "Lectures\n\nLecture 01-1: Class Overview\nLecture 01-2: Uncertainty and Probability Review",
    "crumbs": [
      "Weeks",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/week02.html",
    "href": "weeks/week02.html",
    "title": "Week 2",
    "section": "",
    "text": "Homework 1 and Exercise 1 due 2/2/24.",
    "crumbs": [
      "Weeks",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/week02.html#announcements",
    "href": "weeks/week02.html#announcements",
    "title": "Week 2",
    "section": "",
    "text": "Homework 1 and Exercise 1 due 2/2/24.",
    "crumbs": [
      "Weeks",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/week02.html#readings",
    "href": "weeks/week02.html#readings",
    "title": "Week 2",
    "section": "Readings",
    "text": "Readings\nAs you’re reading along, please use the collaborative annotation in Canvas to make notes and read and respond to others’ thoughts and questions. Post a summary of key points, takeaways, and thoughts on Ed Discussion and be prepared to engage in the discussion on-line. We will dedicate class time to this reading on Wednesday, so make sure you’ve read it, though you’ll have more time to finalize your thoughts for the online.\nNote: The link here will require additional access through the Cornell library; use the class schedule for a direct link or access through Canvas.\n\nStein, M. L. (2020). Some statistical issues in climate science. Statistical Science: A Review Journal of the Institute of Mathematical Statistics, 35(1), 31–41. https://doi.org/10.1214/19-sts730",
    "crumbs": [
      "Weeks",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/week02.html#exercises",
    "href": "weeks/week02.html#exercises",
    "title": "Week 2",
    "section": "Exercises",
    "text": "Exercises\n\nLoading, Plotting, and Reasoning About Data",
    "crumbs": [
      "Weeks",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/week02.html#lectures",
    "href": "weeks/week02.html#lectures",
    "title": "Week 2",
    "section": "Lectures",
    "text": "Lectures\n\nLecture 02-1: Probability Models and Model Residuals\nLecture 02-2: Probability Models II",
    "crumbs": [
      "Weeks",
      "Week 2"
    ]
  },
  {
    "objectID": "homework.html",
    "href": "homework.html",
    "title": "Homework Assignments",
    "section": "",
    "text": "This page contains information about and a schedule of the homework assignments for the semester.",
    "crumbs": [
      "Homework"
    ]
  },
  {
    "objectID": "homework.html#general-information",
    "href": "homework.html#general-information",
    "title": "Homework Assignments",
    "section": "General Information",
    "text": "General Information\n\nWhile the instructions for each assignment are available through the linked pages for quick and public access, if you are in the class you must use the link provided in Ed Discussion to accept the assignment. This will ensure that:\n\nYou have compatible versions of all relevant packages provided in the environment;\nYou have a GitHub repository that you can use to share your code.\n\nSubmit assignments by 9:00pm Eastern Time on the due date on Gradescope.\nSubmissions must be PDFs. Make sure that you tag the pages corresponding to each question; points will be deducted otherwise.",
    "crumbs": [
      "Homework"
    ]
  },
  {
    "objectID": "homework.html#rubric",
    "href": "homework.html#rubric",
    "title": "Homework Assignments",
    "section": "Rubric",
    "text": "Rubric\nThe standard rubric for homework problems is available here.",
    "crumbs": [
      "Homework"
    ]
  },
  {
    "objectID": "homework.html#schedule",
    "href": "homework.html#schedule",
    "title": "Homework Assignments",
    "section": "Schedule",
    "text": "Schedule\n\n\n\n\n\n\n\n\n\nAssignment\nInstructions\nDue Date\nSolutions\n\n\n\n\nHW1\n\nFeb 02, 2024\n\n\n\nHW2\n\nFeb 23, 2024\n\n\n\nHW3\n\nMar 22, 2024\n\n\n\nHW4\n\nMay 04, 2024",
    "crumbs": [
      "Homework"
    ]
  },
  {
    "objectID": "solutions/hw03/hw03.html",
    "href": "solutions/hw03/hw03.html",
    "title": "Homework 3 Solutions",
    "section": "",
    "text": "Due Date\n\n\n\nFriday, 3/22/24, 9:00pm"
  },
  {
    "objectID": "solutions/hw03/hw03.html#overview",
    "href": "solutions/hw03/hw03.html#overview",
    "title": "Homework 3 Solutions",
    "section": "Overview",
    "text": "Overview\n\nInstructions\nThe goal of this homework assignment is to practice developing and working with probability models for data.\n\n\nLearning Outcomes\nAfter completing this assignments, students will be able to:\n\ndevelop probability models for data and model residuals under a variety of statistical assumptions;\nevaluate the appropriateness of those assumptions through the use of qualitative and quantitative evaluations of goodness-of-fit;\nfit a basic Bayesian model to data.\n\n\n\nLoad Environment\nThe following code loads the environment and makes sure all needed packages are installed. This should be at the start of most Julia scripts.\n\n\nimport Pkg\nPkg.activate(@__DIR__)\nPkg.instantiate()\n\n\nThe following packages are included in the environment (to help you find other similar packages in other languages). The code below loads these packages for use in the subsequent notebook (the desired functionality for each package is commented next to the package).\n\n\nusing Random # random number generation and seed-setting\nusing DataFrames # tabular data structure\nusing DataFramesMeta # API which can simplify chains of DataFrames transformations\nusing CSVFiles # reads/writes .csv files\nusing Distributions # interface to work with probability distributions\nusing Plots # plotting library\nusing StatsBase # statistical quantities like mean, median, etc\nusing StatsPlots # some additional statistical plotting tools\nusing Optim # optimization tools\nusing Dates # DateTime structures and interface"
  },
  {
    "objectID": "solutions/hw03/hw03.html#problems-total-30-points-for-4850-40-for-5850",
    "href": "solutions/hw03/hw03.html#problems-total-30-points-for-4850-40-for-5850",
    "title": "Homework 3 Solutions",
    "section": "Problems (Total: 30 Points for 4850; 40 for 5850)",
    "text": "Problems (Total: 30 Points for 4850; 40 for 5850)\n\nProblem 1\n\nConsider the Rahmstorf (2007) sea-level rise model from Homework 2:\n\\[\\frac{dH(t)}{dt} = \\alpha (T(t) - T_0),\\]\nwhere \\(T_0\\) is the temperature (in \\(^\\circ C\\)) where sea-level is in equilibrium (\\(dH/dt = 0\\)), and \\(\\alpha\\) is the sea-level rise sensitivity to temperature. Discretizing this equation using the Euler method and using an annual timestep (\\(\\delta t = 1\\)), we get \\[H(t+1) = H(t) + \\alpha (T(t) - T_0).\\]\nSuppose that we wanted to develop a Bayesian probability model for this problem, assuming independent normal residuals:\n\\[\\begin{gather*}\ny(t) = F(t) + \\varepsilon_t \\\\\n\\varepsilon_t \\sim \\mathcal{N}(0, \\sigma^2)\n\\end{gather*}\\]\nWe might specify the following priors (assuming independence across parameters):\n\n\\(T_0 \\sim \\mathcal{N}(-0.5, 0.1)\\);\n\\(\\alpha \\sim \\mathcal{TN}(0, 5; 0, \\infty)\\) (truncated normal between 0 and infinity);\n\\(H_0 \\sim \\mathcal{N}(-150, 25)\\);\n\\(\\sigma \\sim \\mathcal{TN}(0, 5; 0, \\infty)\\)\n\nIn this problem:\n\n\nHistorical and RCP 8.5 global mean temperatures from NOAA can be found in data/NOAA_IPCC_RCPtempsscenarios.csv (use the fourth column for the temperature series).\nGlobal mean sea level anomalies (relative to the 1990 mean global sea level) are in data/CSIRO_Recons_gmsl_yr_2015.csv, courtesy of CSIRO (https://www.cmar.csiro.au/sealevel/sl_data_cmar.html).\nSimulate from the prior predictive distribution. What do you think about the priors?\nWould you propose new priors? If so, what might they be and why?\n\n\nSolution:\nFirst, let’s load the data.\n\n# load data files\nslr_data = DataFrame(load(\"data/CSIRO_Recons_gmsl_yr_2015.csv\"))\ngmt_data = DataFrame(load(\"data/NOAA_IPCC_RCPtempsscenarios.csv\"))\nslr_data[:, :Time] = slr_data[:, :Time] .- 0.5; # remove 0.5 from Times\ndat = leftjoin(slr_data, gmt_data, on=\"Time\") # join data frames on time\nselect!(dat, [1, 2, 6])  # drop columns we don't need\nfirst(dat, 6)\n\n┌ Warning: In data/NOAA_IPCC_RCPtempsscenarios.csv line 426 has 0 fields but 13 fields are expected. Skipping row.\n└ @ TextParse ~/.julia/packages/TextParse/gNKVx/src/csv.jl:382\n\n\n\n6×3 DataFrame\n\n\n\nRow\nTime\nGMSL (mm)\nHistorical NOAA temp & CNRM RCP 8.5 with respect to 20th century\n\n\n\nFloat64\nFloat64\nFloat64?\n\n\n\n\n1\n1880.0\n-158.7\n-0.16\n\n\n2\n1881.0\n-153.1\n-0.1\n\n\n3\n1882.0\n-169.9\n-0.12\n\n\n4\n1883.0\n-164.6\n-0.17\n\n\n5\n1884.0\n-143.7\n-0.23\n\n\n6\n1885.0\n-145.2\n-0.2\n\n\n\n\n\n\n\nCreating the model:\n\n# slr_model: function to simulate sea-level rise from global mean temperature based on the Rahmstorf (2007) model\n\nfunction slr_model(α, T₀, H₀, temp_data)\n    temp_effect = α .* (temp_data .- T₀)\n    slr_predict = cumsum(temp_effect) .+ H₀\n    return slr_predict\nend\n\nslr_model (generic function with 1 method)\n\n\nNow, we can sample from the priors and simulate the prior predictive distributions.\n\n\\(T_0 \\sim \\mathcal{N}(-0.5, 0.1)\\);\n\\(\\alpha \\sim \\mathcal{TN}(0, 5; 0, \\infty)\\) (truncated normal between 0 and infinity);\n\\(H_0 \\sim \\mathcal{N}(-150, 25)\\);\n\\(\\sigma \\sim \\mathcal{TN}(0, 5; 0, \\infty)\\)\n\n\n# set up prior distributions\nT₀_prior = Normal(-0.5, 0.1)\nα_prior = truncated(Normal(0, 5), lower=0)\nH₀_prior = Normal(-150, 25)\nσ_prior = truncated(Normal(0, 5), lower=0)\n\n# sample and simulate\nn_samples = 10000\nT₀ = rand(T₀_prior, n_samples)\nα = rand(α_prior, n_samples)\nH₀ = rand(H₀_prior, n_samples)\nσ = rand(σ_prior, n_samples)\nslr_prior = zeros(n_samples, nrow(dat))\nfor i = 1:n_samples\n    slr_out = slr_model(α[i], T₀[i], H₀[i], dat[:, 3])\n    slr_prior[i, :] = slr_out + rand(Normal(0, σ[i]), length(slr_out))\nend\n\nFinally, let’s plot the prior predictive median and 95% quantiles to evaluate how reasonable the priors are.\n\n# compute quantiles\nslr_quantile = mapslices(col -&gt; quantile(col, [0.025, 0.5, 0.975]), slr_prior; dims=1)\n\n# make plot\nplot(dat[:, 1], slr_quantile[2, :], ribbon=(slr_quantile[2, :] - slr_quantile[1, :], slr_quantile[3, :] - slr_quantile[2, :]), fillalpha=0.3, label=\"Prior Predictive Distribution\")\nxlabel!(\"Year\")\nylabel!(\"Sea Level Anomaly (mm)\")\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Prior predictive distribution for the SLR model.\n\n\n\n\nWe can see that later in the period, there is a long upper tail (based on the difference between the 95% quantile and the predictive median). This is actually quite large compared to reasonable ranges of the data and we might want to reduce that prior range. We have a few possible options:\n\nCalibrate the model with these priors and check if the posterior predictive distribution is under-confident;\nReduce the prior range of \\(\\alpha\\), as the level of sensitivity might be too high;\nReduce the prior range of \\(\\sigma\\) as the level of noise might be too large.\n\n\n\nProblem 2\nFollowing from Problem 1, compare the maximum likelihood and maximum a posteriori estimates for the model.\nIn this problem:\n\nFind the MLE and MAP parameter values using the prior distributions given in Problem 1.\nPlot the median and 95% credible intervals for the hindcasts and the projections until 2100 under RCP 8.5 (using data/NOAA_IPCC_RCPtempsscenarios.csv; make sure \\(T_0\\) and \\(H_0\\) have the same meaning as in Problem 1!).\nWhat differences do you observe? What do you attribute these differences to? What conclusions can you draw about the Bayesian model?\n\nSolution:\nWe’ll write the likelihood and posterior functions to use with the optimization.\n\nfunction slr_loglik(p, slr_dat, temp_dat)\n    α, T₀, H₀, σ = p # unpack parameter vector\n    slr_out = slr_model(α, T₀, H₀, temp_dat)\n    # compute log-likelihood of residuals\n    ll = sum(logpdf.(Normal(0, σ), slr_out - slr_dat))\n    return ll\nend\n\nfunction slr_logpri(p, priors)\n    lp = sum(logpdf.(priors, p))\n    return lp\nend\n\nfunction slr_logposterior(p, priors, slr_dat, temp_dat)\n    # calculate priors\n    lp = slr_logpri(p, priors)\n    # if the log-prior is infinite, no need to calculate \n    # likelihood\n    if !isinf(lp)\n        lp += slr_loglik(p, slr_dat, temp_dat)\n    end\n    return lp\nend\n\nslr_logposterior (generic function with 1 method)\n\n\nNow let’s optimize to find the MAP and MLE.\n\n# set parameter ranges: α, T₀, H₀, σ\nlb = [0.0, -5.0, -200.0, 0.0]\nub = [10.0, 0.0, 0.0, 10.0]\np0 = [5.0, -2.0, -100.0, 5.0]\n\n# find the MLE\np_mle = Optim.optimize(p -&gt; -slr_loglik(p, dat[:, 2], dat[:, 3]), lb, ub, p0).minimizer\n@show p_mle;\n\n# find the MAP\npriors = [α_prior, T₀_prior, H₀_prior, σ_prior]\np_map = Optim.optimize(p -&gt; -slr_logposterior(p, priors, dat[:, 2], dat[:, 3]), lb, ub, p0).minimizer\n@show p_map;\n\np_mle = [2.0100774149505423, -0.802522918446612, -159.1081730249685, 5.894320405843044]\np_map = [2.1289464794526687, -0.7571088562951854, -158.07993761294216, 5.888145510737759]\n\n\nComparing the MLE and the MAP, we can see that they are similar, but the SLR sensitivity \\(\\alpha\\) is slightly higher for the MAP, while the equilibrium temperature \\(T_0\\) and sea-level \\(H_0\\) are slightly lower to compensate. Broadly speaking, these are similar, though, which suggests that the prior has relatively limited influence.\nNow let’s plot the two projected distributions to see what we get.\n\ntemp_idx = indexin(1880:2100, gmt_data.Time) # get the right year indices\n\nmle_out = zeros(n_samples, length(temp_idx))\nmap_out = zeros(n_samples, length(temp_idx))\n\n# simulate the model\nmle_sim = slr_model(p_mle[1], p_mle[2], p_mle[3], gmt_data[temp_idx, 4])\nmap_sim = slr_model(p_map[1], p_map[2], p_map[3], gmt_data[temp_idx, 4])\n# add back in the residuals\nmle_residuals = rand(Normal(0, p_mle[4]), (n_samples, length(temp_idx)))\nmap_residuals = rand(Normal(0, p_map[4]), (n_samples, length(temp_idx)))\n\nmle_out = mapslices(row -&gt; row + mle_sim, mle_residuals; dims=2)\nmap_out = mapslices(row -&gt; row + map_sim, map_residuals; dims=2)\n\n# compute quantiles\nmle_q = mapslices(col -&gt; quantile(col, [0.025, 0.5, 0.975]), mle_out; dims=1)\nmap_q = mapslices(col -&gt; quantile(col, [0.025, 0.5, 0.975]), map_out; dims=1)\n\n# plot quantiles\nplot(gmt_data.Time[temp_idx], mle_q[2, :], ribbon=(mle_q[2, :] - mle_q[1, :], mle_q[3, :] - mle_q[2, :]), color=:blue, fillalpha=0.3, label=\"MLE\")\nplot!(gmt_data.Time[temp_idx], map_q[2, :], ribbon=(map_q[2, :] - map_q[1, :], map_q[3, :] - map_q[2, :]), color=:green, fillalpha=0.3, label=\"MAP\")\n# add data\nscatter!(dat[:, 1], dat[:, 2], color=:orange, label=\"Observations\")\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Predictive distributions from the MLE and MAP parameters.\n\n\n\n\nThe MAP estimate is a little higher, due to the increased \\(\\alpha\\) parameter, which resulted from the prior. A more constrained prior might reduce this estimate, which would have the corresponding impact on the projections (which may or may not be appropriate!). As a result, the Bayesian model will be more sensitive to uncertainties in projected temperatures than the maximum-likelihood model.\n\n\nProblem 3\nLet’s look at how (modeled) daily maximum temperatures have (or have not) increased in Ithaca from 1850–2014. Model output from NOAA’s GFDL-ESM4 climate model (one of the models used in the latest Climate Model Intercomparison Project, CMIP6) is available in data/gfdl-esm4-tempmax-ithaca.csv. While this model output has not been bias-corrected, we won’t worry about that for the purposes of this assignment.\nIn this problem:\n\nLoad and plot the temperature maxima data from data/gfdl-esm4-tempmax-ithaca.csv.\nSuppose that we were interested in looking at temperature exceedances over 28°C. Decluster these occurrences and plot the number of exceedances by year. Have they increased over time?\nFit a stationary GPD model for the exceedances. What does this distribution look like?\n\nSolution:\nLet’s load the data and filter for daily maxima above 28°C.\n\ntemp_data = DataFrame(load(\"data/gfdl-esm4-tempmax-ithaca.csv\"))\ntemp_high = temp_data[temp_data.TempMax .&gt; 28, :]\n\n\n231×2 DataFrame206 rows omitted\n\n\n\nRow\nDay\nTempMax\n\n\n\nDate\nFloat64\n\n\n\n\n1\n1853-08-13\n28.734\n\n\n2\n1854-07-12\n29.5106\n\n\n3\n1854-07-18\n28.131\n\n\n4\n1854-07-19\n28.9641\n\n\n5\n1854-07-31\n28.1494\n\n\n6\n1854-08-01\n28.2747\n\n\n7\n1854-08-02\n28.6134\n\n\n8\n1854-08-03\n28.7721\n\n\n9\n1857-07-07\n28.0677\n\n\n10\n1858-07-29\n28.0767\n\n\n11\n1860-07-01\n28.5655\n\n\n12\n1860-07-29\n29.743\n\n\n13\n1860-08-08\n28.4803\n\n\n⋮\n⋮\n⋮\n\n\n220\n2013-07-08\n28.2372\n\n\n221\n2013-07-10\n29.2171\n\n\n222\n2013-07-11\n29.7322\n\n\n223\n2013-07-13\n29.0477\n\n\n224\n2014-05-23\n28.3297\n\n\n225\n2014-05-31\n28.6978\n\n\n226\n2014-06-17\n28.5005\n\n\n227\n2014-06-18\n29.4382\n\n\n228\n2014-06-19\n30.1999\n\n\n229\n2014-06-20\n30.032\n\n\n230\n2014-06-21\n28.7772\n\n\n231\n2014-07-09\n28.6819\n\n\n\n\n\n\n\nTo decluster, let’s calculate the extremal index using the estimator \\[\\hat{\\theta}(u) = \\frac{2\\left(\\sum_{i-1}^{N-1} T_i\\right)^2}{(N-1)\\sum_{i=1}^{N-1}T_i^2}.\\]\n\nS = findall(temp_data.TempMax .&gt; 28)\nN = length(S)\nT = diff(S)\nθ = 2 * sum(T)^2 / ((N-1) * sum(T.^2))\n\n0.47832378879687915\n\n\nThis means that the declustering time is 2.0 days. Now we want to assign data that is within that period to appropriate clusters. Let’s write a function to do that.\n\n# cluster data points which occur within period\nfunction assign_cluster(dat, period)\n    cluster_index = 1\n    clusters = zeros(Int, length(dat))\n    for i in 1:length(dat)\n        if clusters[i] == 0\n            clusters[findall(abs.(dat .- dat[i]) .&lt;= period)] .= cluster_index\n            cluster_index += 1\n        end\n    end\n    return clusters\nend\n\n# cluster exceedances that occur within a two-day window\n# @transform is a macro from DataFramesMeta.jl which adds a new column based on a data transformation\ntemp_high = @transform temp_high :cluster = assign_cluster(:Day, Dates.Day(2))\n# find maximum value within cluster\ntemp_decluster = combine(temp_high -&gt; temp_high[argmax(temp_high.TempMax), :], groupby(temp_high, :cluster))\n\n\n164×3 DataFrame139 rows omitted\n\n\n\nRow\ncluster\nDay\nTempMax\n\n\n\nInt64\nDate\nFloat64\n\n\n\n\n1\n1\n1853-08-13\n28.734\n\n\n2\n2\n1854-07-12\n29.5106\n\n\n3\n3\n1854-07-19\n28.9641\n\n\n4\n4\n1854-07-31\n28.1494\n\n\n5\n5\n1854-08-03\n28.7721\n\n\n6\n6\n1857-07-07\n28.0677\n\n\n7\n7\n1858-07-29\n28.0767\n\n\n8\n8\n1860-07-01\n28.5655\n\n\n9\n9\n1860-07-29\n29.743\n\n\n10\n10\n1860-08-08\n28.4803\n\n\n11\n11\n1861-06-30\n28.3882\n\n\n12\n12\n1861-07-22\n29.5198\n\n\n13\n13\n1861-08-10\n28.1314\n\n\n⋮\n⋮\n⋮\n⋮\n\n\n153\n153\n2011-06-20\n29.1638\n\n\n154\n154\n2013-05-30\n29.2171\n\n\n155\n155\n2013-06-06\n28.669\n\n\n156\n156\n2013-06-07\n30.2114\n\n\n157\n157\n2013-06-17\n28.2765\n\n\n158\n158\n2013-07-08\n28.2372\n\n\n159\n159\n2013-07-11\n29.7322\n\n\n160\n160\n2014-05-23\n28.3297\n\n\n161\n161\n2014-05-31\n28.6978\n\n\n162\n162\n2014-06-17\n28.5005\n\n\n163\n163\n2014-06-19\n30.1999\n\n\n164\n164\n2014-07-09\n28.6819\n\n\n\n\n\n\n\nNow we want to count the number of exceedances by year and plot how these occurrences are changing over time.\n\ntemp_decluster.Year = year.(temp_decluster.Day)\ntemp_count = combine(groupby(temp_decluster, :Year), nrow)\nbar(temp_count.Year, temp_count.nrow, label=\"Number of Clusters\")\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisually, it appears as though there has been an uptick in the number of clusters post-2000, with more years containing 3+ clusters. This would be hard to pull out without a model that depends on a broader warming signal, since the most of the data record looks stationary, so it’s unclear whether this is actually a trend. As a result, we might use stationary Poisson process for the occurrences, or we could try to fit one that was based on a covariate like global mean temperature.\nLet’s now fit a stationary GPD to see what how the distribution of conditional exceedances looks.\n\ntemp_exceedances = temp_high.TempMax .- 28\ngpd_fit = Optim.optimize(p -&gt; -sum(logpdf(GeneralizedPareto(0.0, p[1], p[2]), temp_exceedances)), [0.0, -Inf], [Inf, Inf], [1.0, 1.0]).minimizer\n\n2-element Vector{Float64}:\n  0.8510410229663868\n -0.18775785232439562\n\n\nLet’s visualize this distribution (Figure 3) and also use a Q-Q plot (Figure 4) to see how well if fits the data.\n\nplot(GeneralizedPareto(0.0, gpd_fit[1], gpd_fit[2]) .+ 28)\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: GPD fit for temperature exceedances over 28°C.\n\n\n\n\n\nqqplot(GeneralizedPareto(0.0, gpd_fit[1], gpd_fit[2]), temp_exceedances, xlabel=\"Theoretical Quantile\", ylabel=\"Empirical Quantile\")\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Q-Q plot for the fitted GPD distribution and the Ithaca temperature exceedances.\n\n\n\n\nThe fit looks generally good, though the tail of the data may be slightly thinner (“less extreme” extremes) than the GPD fit implies (as there is basically no fitted probability of exceedances at or above 31°C).\n\n\nProblem 4\nGRADED FOR 5850 STUDENTS ONLY\nIn class, we modeled the annual maxima of the San Francisco tide gauge data using a stationary GEV distribution. We could also hypothesize that the tide extremes are influenced by the Pacific Decadal Oscillation (PDO), which is a climate pattern related to the sea-surface temperatures in the Pacific Ocean (similar to the El Niño-Southern Oscillation), in the following fashion (where only the GEV location is variable):\n\\[y_t \\sim \\text{GEV}(\\mu_0 + \\mu_1 p_t, \\sigma, \\xi)\\]\nIn this problem:\n\nLoad the San Francisco tide gauge data (data/h551.csv) and the PDO index dataset (data/errst.v5.pdo.dat; this file is a space-delimited file, versus the comma-delimited .csv files, which can be loaded in Julia with CSV.read(data/errst.v5.pdo.dat, DataFrame; delim=\" \", header=2, ignorerepeated=true)). The PDO data is given as monthly values; convert these to yearly indices by taking the mean. You should also drop 2023 due to the incomplete record. You can use the function at the bottom of these instructions to load the data, or adapt accordingly to a different language.\nFind the MLE of the non-stationary GEV model and for a stationary GEV (constant \\(\\mu\\); we did this in class).\nDiscuss the difference(s) between the two fitted models based on the coefficient values (you can also bring to bear the range(s) of PDO values from the data), the 100- and 500-year return periods in 2022, and plotted hindcasts.\n\n\n## load the data from the file and return a DataFrame of DateTime values and gauge measurements\n\nfunction load_pdo(fname)\n    # This uses the DataFramesMeta.jl package, which makes it easy to string together commands to load and process data\n    df = DataFrame(CSVFiles.load(File(format\"CSV\", fname), spacedelim=true, skiplines_begin=1))\n    # take yearly average\n    @transform!(df, :PDO = mean(AsTable(names(df)[2:13])))\n    @select!(df, $[:Year, :PDO])\n    @rsubset!(df, :Year != 2023)\n    return df\nend\n\npdo = load_pdo(\"data/ersst.v5.pdo.dat\")\n# subset for years that match the tide gauge data\nyears = pdo[!, :Year]\n@rsubset!(pdo, :Year in years)\n\n\n169×2 DataFrame144 rows omitted\n\n\n\nRow\nYear\nPDO\n\n\n\nInt64\nFloat64\n\n\n\n\n1\n1854\n-0.818333\n\n\n2\n1855\n-0.743333\n\n\n3\n1856\n-0.2075\n\n\n4\n1857\n-0.0266667\n\n\n5\n1858\n0.196667\n\n\n6\n1859\n-1.74333\n\n\n7\n1860\n-0.954167\n\n\n8\n1861\n-0.955\n\n\n9\n1862\n-0.226667\n\n\n10\n1863\n-0.095\n\n\n11\n1864\n-0.281667\n\n\n12\n1865\n-0.553333\n\n\n13\n1866\n-0.9425\n\n\n⋮\n⋮\n⋮\n\n\n158\n2011\n-1.81167\n\n\n159\n2012\n-1.73333\n\n\n160\n2013\n-1.16583\n\n\n161\n2014\n0.554167\n\n\n162\n2015\n0.920833\n\n\n163\n2016\n0.673333\n\n\n164\n2017\n-0.0933333\n\n\n165\n2018\n-0.365\n\n\n166\n2019\n-0.154167\n\n\n167\n2020\n-1.145\n\n\n168\n2021\n-1.87417\n\n\n169\n2022\n-2.115\n\n\n\n\n\n\n\nSolution:\nWe have loaded the PDO data above, now let’s load the tide gauge data\n\n# read in data and get annual maxima\nfunction load_data(fname)\n    date_format = DateFormat(\"yyyy-mm-dd HH:MM:SS\")\n    df = DataFrame(CSVFiles.load(fname, header_exists=false))\n    rename!(df, \"Column1\" =&gt; \"year\", \"Column2\" =&gt; \"month\", \"Column3\" =&gt; \"day\", \"Column4\" =&gt; \"hour\", \"Column5\" =&gt; \"gauge\")\n        # need to reformat the decimal date in the data file\n    transform!(df, [:year, :month, :day, :hour] =&gt; ByRow(DateTime) =&gt; :datetime)\n    # replace -99999 with missing\n    df.gauge = ifelse.(abs.(df.gauge) .&gt;= 9999, missing, df.gauge)\n    select!(df, [:datetime, :gauge])\n    return df\nend\n\ndat = load_data(\"data/h551.csv\")\n# detrend the data to remove the effects of sea-level rise and seasonal dynamics\nma_length = 366\nma_offset = Int(floor(ma_length/2))\nmoving_average(series,n) = [mean(@view series[i-n:i+n]) for i in n+1:length(series)-n]\ndat_ma = DataFrame(datetime=dat.datetime[ma_offset+1:end-ma_offset], residual=dat.gauge[ma_offset+1:end-ma_offset] .- moving_average(dat.gauge, ma_offset))\n\n# group data by year and compute the annual maxima\ndat_ma = dropmissing(dat_ma) # drop missing data\ndat_annmax = combine(dat_ma -&gt; dat_ma[argmax(dat_ma.residual), :], groupby(transform(dat_ma, :datetime =&gt; x-&gt;year.(x)), :datetime_function))\ndelete!(dat_annmax, nrow(dat_annmax)) # delete 2023; haven't seen much of that year yet\nrename!(dat_annmax, :datetime_function =&gt; :Year)\nselect!(dat_annmax, [:Year, :residual])\n\n\n126×2 DataFrame101 rows omitted\n\n\n\nRow\nYear\nresidual\n\n\n\nInt64\nFloat64\n\n\n\n\n1\n1897\n1247.5\n\n\n2\n1898\n1237.02\n\n\n3\n1899\n1471.5\n\n\n4\n1900\n1276.62\n\n\n5\n1901\n1282.43\n\n\n6\n1902\n1165.63\n\n\n7\n1903\n1180.59\n\n\n8\n1904\n1221.89\n\n\n9\n1905\n1240.64\n\n\n10\n1906\n1175.34\n\n\n11\n1907\n1295.24\n\n\n12\n1908\n1360.57\n\n\n13\n1909\n1216.81\n\n\n⋮\n⋮\n⋮\n\n\n115\n2011\n1348.5\n\n\n116\n2012\n1298.43\n\n\n117\n2013\n1303.97\n\n\n118\n2014\n1264.41\n\n\n119\n2015\n1256.0\n\n\n120\n2016\n1355.96\n\n\n121\n2017\n1261.74\n\n\n122\n2018\n1273.0\n\n\n123\n2019\n1251.56\n\n\n124\n2020\n1273.07\n\n\n125\n2021\n1227.76\n\n\n126\n2022\n1320.36\n\n\n\n\n\n\n\nFitting the stationary GEV:\n\np_stat = Optim.optimize(p -&gt; -sum(logpdf(GeneralizedExtremeValue(p[1], p[2], p[3]), dat_annmax.residual)), [1000.0, 0.0, -5.0], [2000.0, 100.0, 5.0], [1500.0, 50.0, 0.0]).minimizer\n\n3-element Vector{Float64}:\n 1258.70967275348\n   56.26648203812787\n    0.017193689777705953\n\n\nThe non-stationary GEV differs in that we need to specify a regression model for the location, which involves an additional parameter. One key consideration is the range of the covariate, which would influence what reasonable ranges of \\(\\mu_0\\) and \\(\\mu_1\\) might be. Let’s plot the distribution.\n\nhistogram(pdo.PDO, legend=false)\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Histogram of Pacific Decadal Oscillation (PDO) values.\n\n\n\n\nNow we need to select a subset of PDO data which aligns with the storm surge years.\n\ndat = leftjoin(dat_annmax, pdo, on=\"Year\") # join data frames on time\n\n\n126×3 DataFrame101 rows omitted\n\n\n\nRow\nYear\nresidual\nPDO\n\n\n\nInt64\nFloat64\nFloat64?\n\n\n\n\n1\n1897\n1247.5\n0.140833\n\n\n2\n1898\n1237.02\n0.0933333\n\n\n3\n1899\n1471.5\n-0.351667\n\n\n4\n1900\n1276.62\n0.806667\n\n\n5\n1901\n1282.43\n-0.185833\n\n\n6\n1902\n1165.63\n1.22167\n\n\n7\n1903\n1180.59\n0.281667\n\n\n8\n1904\n1221.89\n1.16\n\n\n9\n1905\n1240.64\n1.3825\n\n\n10\n1906\n1175.34\n0.810833\n\n\n11\n1907\n1295.24\n0.275\n\n\n12\n1908\n1360.57\n0.73\n\n\n13\n1909\n1216.81\n-0.711667\n\n\n⋮\n⋮\n⋮\n⋮\n\n\n115\n2011\n1348.5\n-1.81167\n\n\n116\n2012\n1298.43\n-1.73333\n\n\n117\n2013\n1303.97\n-1.16583\n\n\n118\n2014\n1264.41\n0.554167\n\n\n119\n2015\n1256.0\n0.920833\n\n\n120\n2016\n1355.96\n0.673333\n\n\n121\n2017\n1261.74\n-0.0933333\n\n\n122\n2018\n1273.0\n-0.365\n\n\n123\n2019\n1251.56\n-0.154167\n\n\n124\n2020\n1273.07\n-1.145\n\n\n125\n2021\n1227.76\n-1.87417\n\n\n126\n2022\n1320.36\n-2.115\n\n\n\n\n\n\n\nFinally, we can set parameter ranges and optimize.\n\nlb = [800.0, -100.0, 0.0, -5.0]\nub = [2000.0, 100.0, 20.0, 5.0]\np0 = [1000.0, 10.0, 1.0, 0.25]\n\np_nonstat = Optim.optimize(p -&gt; -sum(logpdf.(GeneralizedExtremeValue.(p[1] .+ p[2] * dat.PDO, p[3], p[4]), dat.residual)), lb, ub, p0).minimizer\n\n4-element Vector{Float64}:\n 1198.088480280648\n   -7.443921155047124\n   19.99999999935508\n    0.5570827788570579\n\n\nThe parameter differences between p_stat and p_nonstat are quite striking. While the location parameter of the stationary GEV is 1258.71 can range between 1180.53 and 1214.73. The stationary model is also more highly skewed (based on the scale parameter), while the non-stationary model has a fatter tail (based on the shape parameter, which is more positive).\nTo see what these parameter differences imply, let’s look at the 100- and 500-year return levels for both models. To do that, we need to create the model objects.\n\n# set up models\ngev_stat = GeneralizedExtremeValue(p_stat[1], p_stat[2], p_stat[3])\n# notice that this is a vector of distributions\ngev_nonstat = GeneralizedExtremeValue.(p_nonstat[1] .+ p_nonstat[2] * dat.PDO, p_nonstat[3], p_nonstat[4])\n\n126-element Vector{GeneralizedExtremeValue{Float64}}:\n GeneralizedExtremeValue{Float64}(μ=1197.0401280513122, σ=19.99999999935508, ξ=0.5570827788570579)\n GeneralizedExtremeValue{Float64}(μ=1197.393714306177, σ=19.99999999935508, ξ=0.5570827788570579)\n GeneralizedExtremeValue{Float64}(μ=1200.7062592201728, σ=19.99999999935508, ξ=0.5570827788570579)\n GeneralizedExtremeValue{Float64}(μ=1192.0837172155766, σ=19.99999999935508, ξ=0.5570827788570579)\n GeneralizedExtremeValue{Float64}(μ=1199.4718089619607, σ=19.99999999935508, ξ=0.5570827788570579)\n GeneralizedExtremeValue{Float64}(μ=1188.994489936232, σ=19.99999999935508, ξ=0.5570827788570579)\n GeneralizedExtremeValue{Float64}(μ=1195.9917758219763, σ=19.99999999935508, ξ=0.5570827788570579)\n GeneralizedExtremeValue{Float64}(μ=1189.4535317407933, σ=19.99999999935508, ξ=0.5570827788570579)\n GeneralizedExtremeValue{Float64}(μ=1187.7972592837953, σ=19.99999999935508, ξ=0.5570827788570579)\n GeneralizedExtremeValue{Float64}(μ=1192.0527008774304, σ=19.99999999935508, ξ=0.5570827788570579)\n GeneralizedExtremeValue{Float64}(μ=1196.04140196301, σ=19.99999999935508, ξ=0.5570827788570579)\n GeneralizedExtremeValue{Float64}(μ=1192.6544178374636, σ=19.99999999935508, ξ=0.5570827788570579)\n GeneralizedExtremeValue{Float64}(μ=1203.3860708359898, σ=19.99999999935508, ξ=0.5570827788570579)\n ⋮\n GeneralizedExtremeValue{Float64}(μ=1211.5743841065416, σ=19.99999999935508, ξ=0.5570827788570579)\n GeneralizedExtremeValue{Float64}(μ=1210.9912769493963, σ=19.99999999935508, ξ=0.5570827788570579)\n GeneralizedExtremeValue{Float64}(μ=1206.766851693907, σ=19.99999999935508, ξ=0.5570827788570579)\n GeneralizedExtremeValue{Float64}(μ=1193.963307307226, σ=19.99999999935508, ξ=0.5570827788570579)\n GeneralizedExtremeValue{Float64}(μ=1191.2338695503754, σ=19.99999999935508, ξ=0.5570827788570579)\n GeneralizedExtremeValue{Float64}(μ=1193.0762400362496, σ=19.99999999935508, ξ=0.5570827788570579)\n GeneralizedExtremeValue{Float64}(μ=1198.783246255119, σ=19.99999999935508, ξ=0.5570827788570579)\n GeneralizedExtremeValue{Float64}(μ=1200.80551150224, σ=19.99999999935508, ξ=0.5570827788570579)\n GeneralizedExtremeValue{Float64}(μ=1199.236084792051, σ=19.99999999935508, ξ=0.5570827788570579)\n GeneralizedExtremeValue{Float64}(μ=1206.6117700031768, σ=19.99999999935508, ξ=0.5570827788570579)\n GeneralizedExtremeValue{Float64}(μ=1212.039629178732, σ=19.99999999935508, ξ=0.5570827788570579)\n GeneralizedExtremeValue{Float64}(μ=1213.8323735235726, σ=19.99999999935508, ξ=0.5570827788570579)\n\n\nFor the stationary model, these are the same every year as the model does not change; the 100-year return level is the 0.99 quantile, which is 1528.06 and the 500-year return level is the 0.998 quantile, which is 1627.69. For the non-stationary model, we need to look at this for a given year; we’ll use 2022. In that case, the 100-year return level is 1643.58 and the 500-year return level is 2321.91. The more positive shape parameter for the nonstationary model means that these return levels are much more extreme.\nLet’s look at hindcasts from the two models.\n\n# set up models\ngev_stat = GeneralizedExtremeValue(p_stat[1], p_stat[2], p_stat[3])\n# notice that this is a vector of distributions\ngev_nonstat = GeneralizedExtremeValue.(p_nonstat[1] .+ p_nonstat[2] * dat.PDO, p_nonstat[3], p_nonstat[4])\n\n# simulate from the models\nn_sample = 10_000\nsim_stat = rand(gev_stat, (n_sample, nrow(dat)))\nsim_nonstat = zeros(n_sample, nrow(dat))\nfor i in 1:nrow(dat)\n    sim_nonstat[:, i] = rand(gev_nonstat[i], n_sample)\nend\n\n# compute quantiles\nq_stat = mapslices(col -&gt; quantile(col, [0.025, 0.5, 0.975]), sim_stat, dims=1)\nq_nonstat = mapslices(col -&gt; quantile(col, [0.025, 0.5, 0.975]), sim_nonstat, dims=1)\n\n# make plot\nplot(dat.Year, q_stat[2, :], ribbon=(q_stat[2, :] - q_stat[1, :], q_stat[3, :] - q_stat[2, :]), color=:blue, fillalpha=0.3, label=\"Stationary Model\", legend=:topleft)\nplot!(dat.Year, q_nonstat[2, :], ribbon=(q_nonstat[2, :] - q_nonstat[1, :], q_nonstat[3, :] - q_nonstat[2, :]), color=:green, fillalpha=0.3, label=\"Nonstationary Model\")\nscatter!(dat.Year, dat.residual, label=\"Data\", color=:orange)\nxlabel!(\"Year\")\nylabel!(\"Annual Extreme Gauge Level (mm)\")\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Hindcasts from the stationary and non-stationary GEV models.\n\n\n\n\nWe can see from Figure 6 that the nonstationary model seems to capture some of the lower extremes better, but neither model captures the very extreme values between 1980 and 2010. In general, we might be able to say that the nonstationary model can be more flexible and account for variability in the data that might throw off the calibration of a stationary model, but that flexibility can be a blessing and a curse — if the covariate does not actually explain meaningful variability in the data, we might misinterpret the results or engage in overfitting. We could make the model even more flexible by making the scale and shape nonstationary, but that would exacerbate these risks."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BEE 4850/5850: Environmental Data Analysis and Simulation",
    "section": "",
    "text": "This is the course website for the Spring 2024 edition of BEE 4850/5850, Environmental Data Analysis and Simulation, taught at Cornell University by Vivek Srikrishnan.",
    "crumbs": [
      "Course Information",
      "Overview"
    ]
  },
  {
    "objectID": "index.html#course-information",
    "href": "index.html#course-information",
    "title": "BEE 4850/5850: Environmental Data Analysis and Simulation",
    "section": "Course Information",
    "text": "Course Information\n\nDetails on the class and course policies are provided in the syllabus.\nTopics and weekly materials can be found in the schedule.",
    "crumbs": [
      "Course Information",
      "Overview"
    ]
  },
  {
    "objectID": "index.html#regular-weekly-schedule",
    "href": "index.html#regular-weekly-schedule",
    "title": "BEE 4850/5850: Environmental Data Analysis and Simulation",
    "section": "Regular Weekly Schedule",
    "text": "Regular Weekly Schedule\n\n\n\n\n\n\n\nLectures\nMW 11:40-12:55pm, 160 Riley-Robb Hall\n\n\nOffice Hours\nMW 10:00-11:00am, M 1:00-2:00pm, 318 Riley-Robb Hall\n\n\nAssignments\nDue Fridays at 9pm on Gradescope.",
    "crumbs": [
      "Course Information",
      "Overview"
    ]
  },
  {
    "objectID": "slides/lecture13-1.html#bias-variance-tradeoff",
    "href": "slides/lecture13-1.html#bias-variance-tradeoff",
    "title": "Information Criteria",
    "section": "Bias-Variance Tradeoff",
    "text": "Bias-Variance Tradeoff\nKey Idea: Model selection consists of navigating the bias-variance tradeoff.\nModel error (e.g. RMSE) is a combination of irreducible error, bias, and variance."
  },
  {
    "objectID": "slides/lecture13-1.html#bias",
    "href": "slides/lecture13-1.html#bias",
    "title": "Information Criteria",
    "section": "Bias",
    "text": "Bias\nBias is error from mismatches between the model predictions and the data (\\(\\text{Bias}[\\hat{f}] = \\mathbb{E}[\\hat{f}] - y\\)).\nBias comes from under-fitting meaningful relationships between inputs and outputs.\n\ntoo few degrees of freedom (“too simple”)\nneglected processes."
  },
  {
    "objectID": "slides/lecture13-1.html#variance",
    "href": "slides/lecture13-1.html#variance",
    "title": "Information Criteria",
    "section": "Variance",
    "text": "Variance\nVariance is error from over-sensitivity to small fluctuations in inputs (\\(\\text{Variance} = \\text{Var}(\\hat{f})\\)).\nVariance can come from over-fitting noise in the data.\n\ntoo many degrees of freedom (“too complex”)\npoor identifiability"
  },
  {
    "objectID": "slides/lecture13-1.html#bias-variance-tradeoff-1",
    "href": "slides/lecture13-1.html#bias-variance-tradeoff-1",
    "title": "Information Criteria",
    "section": "Bias-Variance Tradeoff",
    "text": "Bias-Variance Tradeoff\nUpshot: For achieve a fixed error level, you can reduce bias (more “complex” model) or you can reduce variance (more “simple” model) but there’s a tradeoff."
  },
  {
    "objectID": "slides/lecture13-1.html#bias-variance-tradeoff-more-generally",
    "href": "slides/lecture13-1.html#bias-variance-tradeoff-more-generally",
    "title": "Information Criteria",
    "section": "Bias-Variance Tradeoff More Generally",
    "text": "Bias-Variance Tradeoff More Generally\nThis decomposition is for MSE, but the principle holds more generally.\n\nModels which perform better “on average” over the training data (low bias) are more likely to overfit (high variance);\nModels which have less uncertainty for training data (low variance) will do worse “on average”."
  },
  {
    "objectID": "slides/lecture13-1.html#cross-validation",
    "href": "slides/lecture13-1.html#cross-validation",
    "title": "Information Criteria",
    "section": "Cross-Validation",
    "text": "Cross-Validation\nThe “gold standard” way to test for predictive performance is cross-validation:\n\nSplit data into training/testing sets;\nCalibrate model to training set;\nCheck for predictive ability on testing set."
  },
  {
    "objectID": "slides/lecture13-1.html#leave-one-out-cross-validation",
    "href": "slides/lecture13-1.html#leave-one-out-cross-validation",
    "title": "Information Criteria",
    "section": "Leave-One-Out Cross-Validation",
    "text": "Leave-One-Out Cross-Validation\n\nDrop one value \\(y_i\\).\nRefit model on rest of data \\(y_{-i}\\).\nEvaluate \\(\\log p(y_i | y_{-i})\\).\nRepeat on rest of data set."
  },
  {
    "objectID": "slides/lecture13-1.html#leave-k-out-cross-validation",
    "href": "slides/lecture13-1.html#leave-k-out-cross-validation",
    "title": "Information Criteria",
    "section": "Leave-\\(k\\)-Out Cross-Validation",
    "text": "Leave-\\(k\\)-Out Cross-Validation\nDrop \\(k\\) values, refit model on rest of data, check for predictive skill.\nAs \\(k \\to n\\), this reduces to the prior predictive distribution \\[p(y^{\\text{rep}}) = \\int_{\\theta} p(y^{\\text{rep}} | \\theta) p(\\theta) d\\theta.\\]"
  },
  {
    "objectID": "slides/lecture13-1.html#expected-out-of-sample-predictive-accuracy",
    "href": "slides/lecture13-1.html#expected-out-of-sample-predictive-accuracy",
    "title": "Information Criteria",
    "section": "Expected Out-Of-Sample Predictive Accuracy",
    "text": "Expected Out-Of-Sample Predictive Accuracy\nThe out-of-sample predictive fit of a new data point \\(\\tilde{y}_i\\) is\n\\[\n\\begin{align}\n\\log p_\\text{post}(\\tilde{y}_i) &= \\log \\mathbb{E}_\\text{post}\\left[p(\\tilde{y}_i | \\theta)\\right] \\\\\n&= \\log \\int p(\\tilde{y_i} | \\theta) p_\\text{post}(\\theta)\\,d\\theta.\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/lecture13-1.html#expected-out-of-sample-predictive-accuracy-1",
    "href": "slides/lecture13-1.html#expected-out-of-sample-predictive-accuracy-1",
    "title": "Information Criteria",
    "section": "Expected Out-Of-Sample Predictive Accuracy",
    "text": "Expected Out-Of-Sample Predictive Accuracy\nHowever, the out-of-sample data \\(\\tilde{y}_i\\) is itself unknown, so we need to compute the expected out-of-sample log-predictive density\n\\[\n\\begin{align}\n\\text{elpd} &= \\text{expected log-predictive density for } \\tilde{y}_i \\\\\n&= \\mathbb{E}_P \\left[\\log p_\\text{post}(\\tilde{y}_i)\\right] \\\\\n&= \\int \\log\\left(p_\\text{post}(\\tilde{y}_i)\\right) P(\\tilde{y}_i)\\,d\\tilde{y}.\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/lecture13-1.html#expected-out-of-sample-predictive-accuracy-2",
    "href": "slides/lecture13-1.html#expected-out-of-sample-predictive-accuracy-2",
    "title": "Information Criteria",
    "section": "Expected Out-Of-Sample Predictive Accuracy",
    "text": "Expected Out-Of-Sample Predictive Accuracy\nWhat is the challenge?\n\nWe don’t know \\(P\\) (the distribution of new data)!\nWe need some measure of the error induced by using an approximating distribution \\(Q\\) from some model."
  },
  {
    "objectID": "slides/lecture13-1.html#information-criteria-1",
    "href": "slides/lecture13-1.html#information-criteria-1",
    "title": "Information Criteria",
    "section": "Information Criteria",
    "text": "Information Criteria\n“Information criteria” refers to a category of estimators of prediction error.\nThe idea: estimate predictive error using the fitted model."
  },
  {
    "objectID": "slides/lecture13-1.html#information-criteria-overview",
    "href": "slides/lecture13-1.html#information-criteria-overview",
    "title": "Information Criteria",
    "section": "Information Criteria Overview",
    "text": "Information Criteria Overview\nThere is a common framework for all of these:\nIf we compute the expected log-predictive density for the existing data \\(p(y | \\theta)\\), this will be too good of a fit and will overestimate the predictive skill for new data."
  },
  {
    "objectID": "slides/lecture13-1.html#information-criteria-corrections",
    "href": "slides/lecture13-1.html#information-criteria-corrections",
    "title": "Information Criteria",
    "section": "Information Criteria Corrections",
    "text": "Information Criteria Corrections\nWe can adjust for that bias by correcting for the effective number of parameters, which can be thought of as the expected degrees of freedom in a model contributing to overfitting."
  },
  {
    "objectID": "slides/lecture13-1.html#akaike-information-criterion-aic",
    "href": "slides/lecture13-1.html#akaike-information-criterion-aic",
    "title": "Information Criteria",
    "section": "Akaike Information Criterion (AIC)",
    "text": "Akaike Information Criterion (AIC)\nThe “first” information criterion that most people see.\nUses a point estimate (the maximum-likelihood estimate \\(\\hat{\\theta}_\\text{MLE}\\)) to compute the log-predictive density for the data, corrected by the number of parameters \\(k\\):\n\\[\\widehat{\\text{elpd}}_\\text{AIC} = \\log p(y | \\hat{\\theta}_\\text{MLE}) - k.\\]"
  },
  {
    "objectID": "slides/lecture13-1.html#aic-formula",
    "href": "slides/lecture13-1.html#aic-formula",
    "title": "Information Criteria",
    "section": "AIC Formula",
    "text": "AIC Formula\nThe AIC is defined as \\(-2\\widehat{\\text{elpd}}_\\text{AIC}\\).\nDue to this convention, lower AICs are better (they correspond to a higher predictive skill)."
  },
  {
    "objectID": "slides/lecture13-1.html#aic-correction-term",
    "href": "slides/lecture13-1.html#aic-correction-term",
    "title": "Information Criteria",
    "section": "AIC Correction Term",
    "text": "AIC Correction Term\nIn the case of a normal model with independent and identically-distributed data and uniform priors, \\(k\\) is the asymptotically “correct” bias term (there are modified corrections for small sample sizes).\nHowever, with more informative priors and/or hierarchical models, the bias correction \\(k\\) is no longer appropriate, as there is less “freedom” associated with each parameter."
  },
  {
    "objectID": "slides/lecture13-1.html#aic-storm-surge-example",
    "href": "slides/lecture13-1.html#aic-storm-surge-example",
    "title": "Information Criteria",
    "section": "AIC: Storm Surge Example",
    "text": "AIC: Storm Surge Example\nModels:\n\nStationary (“null”) model, \\(y_t \\sim \\text{GEV}(\\mu, \\sigma, \\xi);\\)\nTime nonstationary (“null-ish”) model, \\(y_t \\sim \\text{GEV}(\\mu_0 + \\mu_1 t, \\sigma, \\xi);\\)\nPDO nonstationary model, \\(y_t \\sim \\text{GEV}(\\mu_0 + \\mu_1 \\text{PDO}_t, \\sigma, \\xi)\\)"
  },
  {
    "objectID": "slides/lecture13-1.html#aic-example",
    "href": "slides/lecture13-1.html#aic-example",
    "title": "Information Criteria",
    "section": "AIC Example",
    "text": "AIC Example\n\n\nCode\nstat_mle = [1258.71, 56.27, 0.017]\nstat_ll = -707.67\nnonstat_mle = [1231.58, 0.42, 52.07, 0.075]\nnonstat_ll = -702.45\npdo_mle = [1255.87, -12.39, 54.73, 0.033]\npdo_ll = -705.24\n\n# compute AIC values\nstat_aic = stat_ll - 3\nnonstat_aic = nonstat_ll - 4\npdo_aic = pdo_ll - 4\n\nmodel_aic = DataFrame(Model=[\"Stationary\", \"Time\", \"PDO\"], LogLik=trunc.(Int64, round.([stat_ll, nonstat_ll, pdo_ll]; digits=0)), AIC=trunc.(Int64, round.(-2 * [stat_aic, nonstat_aic, pdo_aic]; digits=0)))\n\n\n\n3×3 DataFrame\n\n\n\nRow\nModel\nLogLik\nAIC\n\n\n\nString\nInt64\nInt64\n\n\n\n\n1\nStationary\n-708\n1421\n\n\n2\nTime\n-702\n1413\n\n\n3\nPDO\n-705\n1418"
  },
  {
    "objectID": "slides/lecture13-1.html#aic-interpretation",
    "href": "slides/lecture13-1.html#aic-interpretation",
    "title": "Information Criteria",
    "section": "AIC Interpretation",
    "text": "AIC Interpretation\nAbsolute AIC values have no meaning, only the differences \\(\\Delta_i = \\text{AIC}_i - \\text{AIC}_\\text{min}\\).\nSome basic rules of thumb (from Burnham & Anderson (2004)):\n\n\\(\\Delta_i &lt; 2\\) means the model has “strong” support across \\(\\mathcal{M}\\);\n\\(4 &lt; \\Delta_i &lt; 7\\) suggests “less” support;\n\\(\\Delta_i &gt; 10\\) suggests “weak” or “no” support."
  },
  {
    "objectID": "slides/lecture13-1.html#aic-and-model-evidence",
    "href": "slides/lecture13-1.html#aic-and-model-evidence",
    "title": "Information Criteria",
    "section": "AIC and Model Evidence",
    "text": "AIC and Model Evidence\n\\(\\exp(-\\Delta_i/2)\\) can be thought of as a measure of the likelihood of the model given the data \\(y\\).\nThe ratio \\[\\exp(-\\Delta_i/2) / \\exp(-\\Delta_j/2)\\] can approximate the relative evidence for \\(M_i\\) versus \\(M_j\\)."
  },
  {
    "objectID": "slides/lecture13-1.html#aic-and-model-averaging",
    "href": "slides/lecture13-1.html#aic-and-model-averaging",
    "title": "Information Criteria",
    "section": "AIC and Model Averaging",
    "text": "AIC and Model Averaging\nThis gives rise to the idea of Akaike weights: \\[w_i = \\frac{\\exp(-\\Delta_i/2)}{\\sum_{m=1}^M \\exp(-\\Delta_m/2)}.\\]\nModel projections can then be weighted based on \\(w_i\\), which can be interpreted as the probability that \\(M_i\\) is the best (in the sense of approximating the “true” predictive distribution) model in \\(\\mathcal{M}\\)."
  },
  {
    "objectID": "slides/lecture13-1.html#model-averaging-vs.-selection",
    "href": "slides/lecture13-1.html#model-averaging-vs.-selection",
    "title": "Information Criteria",
    "section": "Model Averaging vs. Selection",
    "text": "Model Averaging vs. Selection\nModel averaging can sometimes be beneficial vs. model selection.\nModel selection can introduce bias from the selection process (this is particularly acute for stepwise selection due to path-dependence)."
  },
  {
    "objectID": "slides/lecture13-1.html#key-takeaways",
    "href": "slides/lecture13-1.html#key-takeaways",
    "title": "Information Criteria",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nLOO-CV is ideal for navigating bias-variance tradeoff but can be computationally prohibitive.\nInformation Criteria are an approximation to LOO-CV based on “correcting” for model complexity.\nApproximation to out of sample predictive error as a penalty for potential to overfit."
  },
  {
    "objectID": "slides/lecture13-1.html#next-classes",
    "href": "slides/lecture13-1.html#next-classes",
    "title": "Information Criteria",
    "section": "Next Classes",
    "text": "Next Classes\nWednesday: Other Information Criteria"
  },
  {
    "objectID": "slides/lecture13-1.html#references-1",
    "href": "slides/lecture13-1.html#references-1",
    "title": "Information Criteria",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n\n\nBurnham, K. P., & Anderson, D. R. (2004). Multimodel Inference: Understanding AIC and BIC in Model Selection. Sociol. Methods Res., 33, 261–304. https://doi.org/10.1177/0049124104268644"
  },
  {
    "objectID": "slides/lecture11-2.html#what-weve-done",
    "href": "slides/lecture11-2.html#what-weve-done",
    "title": "Hypothesis Testing",
    "section": "What We’ve Done",
    "text": "What We’ve Done\n\nDevelop probability models consistent with data-generating processes;\nSimulate from probability models (Monte Carlo)\nCalibrate (fit) models to data and quantify uncertainties (bootstrap, MCMC)."
  },
  {
    "objectID": "slides/lecture11-2.html#whats-left",
    "href": "slides/lecture11-2.html#whats-left",
    "title": "Hypothesis Testing",
    "section": "What’s Left?",
    "text": "What’s Left?\n\nAssessing evidence/model selection\nMaybe: Emulation and surrogate modeling"
  },
  {
    "objectID": "slides/lecture11-2.html#what-is-hypothesis-testing",
    "href": "slides/lecture11-2.html#what-is-hypothesis-testing",
    "title": "Hypothesis Testing",
    "section": "What Is Hypothesis Testing?",
    "text": "What Is Hypothesis Testing?\n\n\nIs a time series stationary?\nDoes some environmental condition have an effect on water quality/etc?\nDoes a drug or treatment have some effect?"
  },
  {
    "objectID": "slides/lecture11-2.html#example-storm-surge-nonstationarity",
    "href": "slides/lecture11-2.html#example-storm-surge-nonstationarity",
    "title": "Hypothesis Testing",
    "section": "Example: Storm Surge Nonstationarity",
    "text": "Example: Storm Surge Nonstationarity\nStandard Extreme Value Models (GEV/GPD): No long-term trend in the data (extremes follow the same distribution)\nThis is an example of a null hypothesis: no meaningful effect."
  },
  {
    "objectID": "slides/lecture11-2.html#example-storm-surge-nonstationarity-1",
    "href": "slides/lecture11-2.html#example-storm-surge-nonstationarity-1",
    "title": "Hypothesis Testing",
    "section": "Example: Storm Surge Nonstationarity",
    "text": "Example: Storm Surge Nonstationarity\nAlternative Hypothesis: There is a long-term trend in the data.\n\nHow can we draw conclusions about whether apparent trends are “real” (alternative hypothesis) or noise (null hypothesis)?"
  },
  {
    "objectID": "slides/lecture11-2.html#hypothesis-testing-notation",
    "href": "slides/lecture11-2.html#hypothesis-testing-notation",
    "title": "Hypothesis Testing",
    "section": "Hypothesis Testing Notation",
    "text": "Hypothesis Testing Notation\n\n\\(\\mathcal{H}_0\\): null\n\\(\\mathcal{H}\\): alternative"
  },
  {
    "objectID": "slides/lecture11-2.html#san-francisco-tide-gauge-data",
    "href": "slides/lecture11-2.html#san-francisco-tide-gauge-data",
    "title": "Hypothesis Testing",
    "section": "San Francisco Tide Gauge Data",
    "text": "San Francisco Tide Gauge Data\n\n\nCode\n# load SF tide gauge data\n# read in data and get annual maxima\nfunction load_data(fname)\n    date_format = DateFormat(\"yyyy-mm-dd HH:MM:SS\")\n    # This uses the DataFramesMeta.jl package, which makes it easy to string together commands to load and process data\n    df = @chain fname begin\n        CSV.read(DataFrame; header=false)\n        rename(\"Column1\" =&gt; \"year\", \"Column2\" =&gt; \"month\", \"Column3\" =&gt; \"day\", \"Column4\" =&gt; \"hour\", \"Column5\" =&gt; \"gauge\")\n        # need to reformat the decimal date in the data file\n        @transform :datetime = DateTime.(:year, :month, :day, :hour)\n        # replace -99999 with missing\n        @transform :gauge = ifelse.(abs.(:gauge) .&gt;= 9999, missing, :gauge)\n        select(:datetime, :gauge)\n    end\n    return df\nend\n\ndat = load_data(\"data/surge/h551.csv\")\n\n# detrend the data to remove the effects of sea-level rise and seasonal dynamics\nma_length = 366\nma_offset = Int(floor(ma_length/2))\nmoving_average(series,n) = [mean(@view series[i-n:i+n]) for i in n+1:length(series)-n]\ndat_ma = DataFrame(datetime=dat.datetime[ma_offset+1:end-ma_offset], residual=dat.gauge[ma_offset+1:end-ma_offset] .- moving_average(dat.gauge, ma_offset))\n\n# group data by year and compute the annual maxima\ndat_ma = dropmissing(dat_ma) # drop missing data\ndat_annmax = combine(dat_ma -&gt; dat_ma[argmax(dat_ma.residual), :], groupby(transform(dat_ma, :datetime =&gt; x-&gt;year.(x)), :datetime_function))\ndelete!(dat_annmax, nrow(dat_annmax)) # delete 2023; haven't seen much of that year yet\nrename!(dat_annmax, :datetime_function =&gt; :Year)\nselect!(dat_annmax, [:Year, :residual])\ndat_annmax.residual = dat_annmax.residual / 1000 # convert to m\n\n# make plots\np1 = plot(\n    dat_annmax.Year,\n    dat_annmax.residual;\n    xlabel=\"Year\",\n    ylabel=\"Annual Max Tide Level (m)\",\n    label=false,\n    marker=:circle,\n    markersize=5,\n    tickfontsize=16,\n    guidefontsize=18,\n    left_margin=5mm, \n    bottom_margin=5mm\n)\n\nn = nrow(dat_annmax)\nlinfit = lm(@formula(residual ~ Year), dat_annmax)\npred = coef(linfit)[1] .+ coef(linfit)[2] * dat_annmax.Year\n\nplot!(p1, dat_annmax.Year, pred, linewidth=3, label=\"Linear Trend\")\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Annual maxima surge data from the San Francisco, CA tide gauge."
  },
  {
    "objectID": "slides/lecture11-2.html#non-parametric-statistics-for-non-stationarity",
    "href": "slides/lecture11-2.html#non-parametric-statistics-for-non-stationarity",
    "title": "Hypothesis Testing",
    "section": "Non-Parametric Statistics for Non-Stationarity",
    "text": "Non-Parametric Statistics for Non-Stationarity\nMann-Kendall Test: Assume data is independent and no periodic signals, but no specific distributional assumption\n\\[S = \\sum_{i=1}^{n-1} \\sum_{j={1+1}}^n \\text{sgn}\\left(y_j - y_i\\right).\\]\nSF Tide Gauge Data: \\(S=921\\)."
  },
  {
    "objectID": "slides/lecture11-2.html#parametric-statistics-for-non-stationarity",
    "href": "slides/lecture11-2.html#parametric-statistics-for-non-stationarity",
    "title": "Hypothesis Testing",
    "section": "Parametric Statistics for Non-Stationarity",
    "text": "Parametric Statistics for Non-Stationarity\nFit a regression\n\\[\n\\begin{gather*}\ny_i = \\beta_0 + \\beta_1 t + \\varepsilon_i, \\\\\n\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2\\mathbb{I}).\n\\end{gather*}\n\\]\nSF Tide Gauge Data: \\(\\hat{\\beta} \\approx (1.26, 4 \\times 10^{-4})^T\\)"
  },
  {
    "objectID": "slides/lecture11-2.html#statistical-significance",
    "href": "slides/lecture11-2.html#statistical-significance",
    "title": "Hypothesis Testing",
    "section": "Statistical Significance",
    "text": "Statistical Significance\nIs the value of the test statistic consistent with the null hypothesis?\nMore formally, could the test statistic have been reasonably observed from a random sample given the null hypothesis?"
  },
  {
    "objectID": "slides/lecture11-2.html#statistical-significance-1",
    "href": "slides/lecture11-2.html#statistical-significance-1",
    "title": "Hypothesis Testing",
    "section": "Statistical Significance",
    "text": "Statistical Significance\nNote: Statistical significance does not mean anything about whether the alternative hypothesis is “true” or an accurate reflection of the data-generating process.\nMore clever null hypothesis setups can get around this, but they aren’t the default."
  },
  {
    "objectID": "slides/lecture11-2.html#non-parametric-null-hypothesis",
    "href": "slides/lecture11-2.html#non-parametric-null-hypothesis",
    "title": "Hypothesis Testing",
    "section": "Non-Parametric Null Hypothesis",
    "text": "Non-Parametric Null Hypothesis\n\\(\\mathcal{H}_0: S = 0\\) (no average trend).\nThe sampling distribution of \\(S\\) consistent with that hypothesis is \\[S \\sim \\mathcal{N}\\left(0, \\frac{n(n-1)(2n+5)}{18}\\right).\\]\nSF Tide Gauge Data: \\(Var(S) = 224875\\)"
  },
  {
    "objectID": "slides/lecture11-2.html#parametric-null-hypothesis",
    "href": "slides/lecture11-2.html#parametric-null-hypothesis",
    "title": "Hypothesis Testing",
    "section": "Parametric Null Hypothesis",
    "text": "Parametric Null Hypothesis\n\\(\\mathcal{H}_0: \\beta_1 = 0\\) (time does not explain trends).\nUse the \\(t\\)-statistic:\n\\[\\hat{t} = \\frac{\\hat{\\beta_1}}{se(\\hat{\\beta_1)}},\\] \\(\\hat{t} \\sim t_{n-2}\\).\nSF Data: \\(t = 2.31\\)."
  },
  {
    "objectID": "slides/lecture11-2.html#error-types",
    "href": "slides/lecture11-2.html#error-types",
    "title": "Hypothesis Testing",
    "section": "Error Types",
    "text": "Error Types\n\n\n\n\n\n\n\nNull Hypothesis Is\n\n\n\n\n\n\n\n\nTrue\n\n\nFalse\n\n\n\n\nDecision About Null Hypothesis\n\n\nDon’t reject\n\n\nTrue negative (probability \\(1-\\alpha\\))\n\n\nType II error (false negative, probability \\(\\beta\\))\n\n\n\n\nReject\n\n\nType I Error (false positive, probability \\(\\alpha\\))\n\n\nTrue positive (probability \\(1-\\beta\\))"
  },
  {
    "objectID": "slides/lecture11-2.html#type-i-and-type-ii-errors",
    "href": "slides/lecture11-2.html#type-i-and-type-ii-errors",
    "title": "Hypothesis Testing",
    "section": "Type I and Type II Errors",
    "text": "Type I and Type II Errors\nThe standard null hypothesis significance framework is based on balancing the chance of making Type I (false positive) and Type II (false negative) errors.\nIdea: Set a significance level \\(\\alpha\\) which is an “acceptable” probability of making a Type I error.\nAside: The probability \\(1-\\beta\\) of correctly rejecting \\(H_0\\) is the power."
  },
  {
    "objectID": "slides/lecture11-2.html#p-values",
    "href": "slides/lecture11-2.html#p-values",
    "title": "Hypothesis Testing",
    "section": "p-Values",
    "text": "p-Values\n\n\nThe p-value captures the probability that, assuming the null hypothesis, you would observe results at least as extreme as you observed.\n\n\n\nCode\ntest_dist = Normal(0, 3)\nx = -10:0.01:10\nplot(x, pdf.(test_dist, x), linewidth=3, legend=false, color=:black, xticks=false,  yticks=false, ylabel=\"Probability\", xlabel=L\"y\", bottom_margin=10mm, left_margin=10mm, guidefontsize=16)\nvline!([5], linestyle=:dash, color=:purple, linewidth=3, )\nareaplot!(5:0.01:10, pdf.(test_dist, 5:0.01:10), color=:green, alpha=0.4)\nquiver!([-4.5], [0.095], quiver=([1], [-0.02]), color=:black, linewidth=2)\nannotate!([-5], [0.11], text(\"Null\\nSampling\\nDistribution\", color=:black))\nquiver!([6.5], [0.03], quiver=([-1], [-0.015]), color=:green, linewidth=2)\nannotate!([6.85], [0.035], text(\"p-value\", :green))\nquiver!([3.5], [0.02], quiver=([1.5], [0]), color=:purple, linewidth=2)\nannotate!([0.9], [0.02], text(\"Observation\", :purple))\nplot!(size=(650, 500))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Illustration of a p-value"
  },
  {
    "objectID": "slides/lecture11-2.html#p-value-and-significance",
    "href": "slides/lecture11-2.html#p-value-and-significance",
    "title": "Hypothesis Testing",
    "section": "p-Value and Significance",
    "text": "p-Value and Significance\nIf the p-value is sufficiently small (below \\(\\alpha\\)), we say that we can reject the null hypothesis with \\(1-\\alpha\\) confidence, or that the finding is statistically significant at the \\(1-\\alpha\\) level.\nThis can mean:\n\n\nThe null hypothesis is not true for that data-generating process;\nThe null hypothesis is true but the data is an outlying sample."
  },
  {
    "objectID": "slides/lecture11-2.html#what-p-values-are-not",
    "href": "slides/lecture11-2.html#what-p-values-are-not",
    "title": "Hypothesis Testing",
    "section": "What p-Values Are Not",
    "text": "What p-Values Are Not\n\n\nProbability that the null hypothesis is true;\nProbability that the effect was produced by chance alone;\nAn indication of the effect size."
  },
  {
    "objectID": "slides/lecture11-2.html#aside-p-values",
    "href": "slides/lecture11-2.html#aside-p-values",
    "title": "Hypothesis Testing",
    "section": "Aside: p-Values",
    "text": "Aside: p-Values\n\n\n\n\nXKCD #1478\n\n\n\nSource: XKCD"
  },
  {
    "objectID": "slides/lecture11-2.html#sf-tide-gauge-data",
    "href": "slides/lecture11-2.html#sf-tide-gauge-data",
    "title": "Hypothesis Testing",
    "section": "SF Tide Gauge Data",
    "text": "SF Tide Gauge Data\nFor the SF tide gauge data, the p-value for the Mann-Kendall test is 0.5.\n\n\nWhat can you conclude?\nWhat can’t you conclude?"
  },
  {
    "objectID": "slides/lecture11-2.html#sf-tide-gauge-data-1",
    "href": "slides/lecture11-2.html#sf-tide-gauge-data-1",
    "title": "Hypothesis Testing",
    "section": "SF Tide Gauge Data",
    "text": "SF Tide Gauge Data\nThe p-value for the coefficient of the time-varying term in a regression is 0.02.\nWhat can/can’t you conclude?"
  },
  {
    "objectID": "slides/lecture11-2.html#what-is-any-statistical-test-doing",
    "href": "slides/lecture11-2.html#what-is-any-statistical-test-doing",
    "title": "Hypothesis Testing",
    "section": "What is Any Statistical Test Doing?",
    "text": "What is Any Statistical Test Doing?\n\nAssume the null hypothesis \\(H_0\\).\nCompute the test statistic \\(\\hat{S}\\) for the sample.\nObtain the sampling distribution of the test statistic \\(S\\) under \\(H_0\\).\nCalculate \\(\\mathbb{P}(S &gt; \\hat{S})\\) (the p-value)."
  },
  {
    "objectID": "slides/lecture11-2.html#some-criticisms-of-null-hypothesis-significance-testing",
    "href": "slides/lecture11-2.html#some-criticisms-of-null-hypothesis-significance-testing",
    "title": "Hypothesis Testing",
    "section": "Some Criticisms of Null Hypothesis Significance Testing",
    "text": "Some Criticisms of Null Hypothesis Significance Testing\n\n\nDoes a dichotomy between \\(H_0\\) and \\(H\\) make sense?\nWhat are the implications of (not) rejecting \\(H_0\\)?\nWhy was the significance level chosen?"
  },
  {
    "objectID": "slides/lecture11-2.html#what-might-be-more-satisfying",
    "href": "slides/lecture11-2.html#what-might-be-more-satisfying",
    "title": "Hypothesis Testing",
    "section": "What Might Be More Satisfying?",
    "text": "What Might Be More Satisfying?\n\n\nConsideration of multiple plausible (possibly more nuanced) hypotheses.\nAssessment/quantification of evidence consistent with different hypotheses.\nInsight into the effect size."
  },
  {
    "objectID": "slides/lecture11-2.html#model-assessment-through-simulation",
    "href": "slides/lecture11-2.html#model-assessment-through-simulation",
    "title": "Hypothesis Testing",
    "section": "Model Assessment Through Simulation",
    "text": "Model Assessment Through Simulation\nIf we have a model which permits simulation (through Monte Carlo or the bootstrap):\n\nCalibrate models under different assumptions;\nSimulate realizations from those models;\nCompute the distribution of the relevant statistic \\(S\\) from these realizations;\nAssess which distribution is most consistent with the observed quantity."
  },
  {
    "objectID": "slides/lecture11-2.html#advances-of-simulation-for-testing",
    "href": "slides/lecture11-2.html#advances-of-simulation-for-testing",
    "title": "Hypothesis Testing",
    "section": "Advances of Simulation for “Testing”",
    "text": "Advances of Simulation for “Testing”\n\nMore structural freedom (don’t need to write down the sampling distribution of \\(S\\) in closed form);\nDon’t need to set up a dichotomous “null vs alternative” test;\nModels can reflect more nuanced hypotheses about data generating processes."
  },
  {
    "objectID": "slides/lecture11-2.html#how-do-we-assess-models-for-selection",
    "href": "slides/lecture11-2.html#how-do-we-assess-models-for-selection",
    "title": "Hypothesis Testing",
    "section": "How Do We Assess Models For Selection?",
    "text": "How Do We Assess Models For Selection?\nGenerally, through predictive performance: how probable is some data (out-of-sample or the calibration dataset)?\nBut there are also metrics of how well you explain the existing data (RMSE, R2)."
  },
  {
    "objectID": "slides/lecture11-2.html#hypothesis-testing-1",
    "href": "slides/lecture11-2.html#hypothesis-testing-1",
    "title": "Hypothesis Testing",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\nNull Hypothesis Significance Testing: Compare a null hypothesis (no effect) to an alternative (effect)\n\\(p\\)-value: probability (under \\(H_0\\)) of more extreme test statistic than observed.\n\\(p\\)-values are often over-interpreted, with negative outcomes!\n“Significant” if \\(p\\)-value is below a significance level reflecting acceptable Type I error rate."
  },
  {
    "objectID": "slides/lecture11-2.html#next-classes",
    "href": "slides/lecture11-2.html#next-classes",
    "title": "Hypothesis Testing",
    "section": "Next Classes",
    "text": "Next Classes\nNext Week: Model Evaluation/Selection Criteria"
  },
  {
    "objectID": "slides/lecture11-2.html#assessments",
    "href": "slides/lecture11-2.html#assessments",
    "title": "Hypothesis Testing",
    "section": "Assessments",
    "text": "Assessments\nHomework 4: Released End of Week, Due 5/3 (Last One!)\nProject: Simulation Study Due Friday (4/12)."
  },
  {
    "objectID": "slides/lecture08-1.html#sampling-distributions",
    "href": "slides/lecture08-1.html#sampling-distributions",
    "title": "The Parametric Bootstrap",
    "section": "Sampling Distributions",
    "text": "Sampling Distributions\n\n\nThe sampling distribution of a statistic captures the uncertainty associated with random samples.\n\n\n\n\nSampling Distribution"
  },
  {
    "objectID": "slides/lecture08-1.html#the-bootstrap-principle",
    "href": "slides/lecture08-1.html#the-bootstrap-principle",
    "title": "The Parametric Bootstrap",
    "section": "The Bootstrap Principle",
    "text": "The Bootstrap Principle\n\n\nEfron (1979) suggested combining estimation with simulation: the bootstrap.\nKey idea: use the data to simulate a data-generating mechanism.\n\n\n\n\n\nBaron von Munchhausen Pulling Himself By His Hair\n\n\n\nSource: Wikipedia"
  },
  {
    "objectID": "slides/lecture08-1.html#why-does-the-bootstrap-work",
    "href": "slides/lecture08-1.html#why-does-the-bootstrap-work",
    "title": "The Parametric Bootstrap",
    "section": "Why Does The Bootstrap Work?",
    "text": "Why Does The Bootstrap Work?\nLet \\(t_0\\) the “true” value of a statistic, \\(\\hat{t}\\) the estimate of the statistic from the sample, and \\((\\tilde{t}_i)\\) the bootstrap estimates.\n\nVariance: \\(\\text{Var}[\\hat{t}] \\approx \\text{Var}[\\tilde{t}]\\)\nThen the bootstrap error distribution approximates the sampling distribution \\[(\\tilde{t}_i - \\hat{t}) \\overset{\\mathcal{D}}{\\sim} \\hat{t} - t_0\\]"
  },
  {
    "objectID": "slides/lecture08-1.html#the-non-parametric-bootstrap",
    "href": "slides/lecture08-1.html#the-non-parametric-bootstrap",
    "title": "The Parametric Bootstrap",
    "section": "The Non-Parametric Bootstrap",
    "text": "The Non-Parametric Bootstrap\n\n\nThe non-parametric bootstrap is the most “naive” approach to the bootstrap: resample-then-estimate.\n\n\n\n\nNon-Parametric Bootstrap"
  },
  {
    "objectID": "slides/lecture08-1.html#why-use-the-bootstrap",
    "href": "slides/lecture08-1.html#why-use-the-bootstrap",
    "title": "The Parametric Bootstrap",
    "section": "Why Use The Bootstrap?",
    "text": "Why Use The Bootstrap?\n\nDo not need to rely on variance asymptotics;\nCan obtain non-symmetric CIs."
  },
  {
    "objectID": "slides/lecture08-1.html#approaches-to-bootstrapping-structured-data",
    "href": "slides/lecture08-1.html#approaches-to-bootstrapping-structured-data",
    "title": "The Parametric Bootstrap",
    "section": "Approaches to Bootstrapping Structured Data",
    "text": "Approaches to Bootstrapping Structured Data\n\nCorrelations: Transform to uncorrelated data (principal components, etc.), sample, transform back.\nTime Series: Block bootstrap"
  },
  {
    "objectID": "slides/lecture08-1.html#generalizing-the-block-bootstrap",
    "href": "slides/lecture08-1.html#generalizing-the-block-bootstrap",
    "title": "The Parametric Bootstrap",
    "section": "Generalizing the Block Bootstrap",
    "text": "Generalizing the Block Bootstrap\nThe rough transitions in the block bootstrap can really degrade estimator quality.\n\nImprove transitions between blocks\nMoving blocks (allow overlaps)"
  },
  {
    "objectID": "slides/lecture08-1.html#sources-of-non-parametric-bootstrap-error",
    "href": "slides/lecture08-1.html#sources-of-non-parametric-bootstrap-error",
    "title": "The Parametric Bootstrap",
    "section": "Sources of Non-Parametric Bootstrap Error",
    "text": "Sources of Non-Parametric Bootstrap Error\n\nSampling error: error from using finitely many replications\nStatistical error: error in the bootstrap sampling distribution approximation"
  },
  {
    "objectID": "slides/lecture08-1.html#when-to-use-the-non-parametric-bootstrap",
    "href": "slides/lecture08-1.html#when-to-use-the-non-parametric-bootstrap",
    "title": "The Parametric Bootstrap",
    "section": "When To Use The Non-Parametric Bootstrap",
    "text": "When To Use The Non-Parametric Bootstrap\n\nSample is representative of the data distribution\nDoesn’t work well for extreme values!"
  },
  {
    "objectID": "slides/lecture08-1.html#the-parametric-bootstrap-1",
    "href": "slides/lecture08-1.html#the-parametric-bootstrap-1",
    "title": "The Parametric Bootstrap",
    "section": "The Parametric Bootstrap",
    "text": "The Parametric Bootstrap\n\nNon-Parametric Bootstrap: Resample directly from the data.\nParametric Bootstrap: Fit a model to the original data and simulate new samples, then calculate bootstrap estimates.\n\nThis lets us use additional information, such as a simulation or statistical model."
  },
  {
    "objectID": "slides/lecture08-1.html#parametric-bootstrap-scheme",
    "href": "slides/lecture08-1.html#parametric-bootstrap-scheme",
    "title": "The Parametric Bootstrap",
    "section": "Parametric Bootstrap Scheme",
    "text": "Parametric Bootstrap Scheme\n\n\nThe parametric bootstrap generates pseudodata using fitted model simulations.\n\n\n\n\nParametric Bootstrap"
  },
  {
    "objectID": "slides/lecture08-1.html#benefits-of-the-parametric-bootstrap",
    "href": "slides/lecture08-1.html#benefits-of-the-parametric-bootstrap",
    "title": "The Parametric Bootstrap",
    "section": "Benefits of the Parametric Bootstrap",
    "text": "Benefits of the Parametric Bootstrap\n\nCan quantify uncertainties in parameter values\nDeals better with structured data (model accounts for structure)"
  },
  {
    "objectID": "slides/lecture08-1.html#potential-drawbacks",
    "href": "slides/lecture08-1.html#potential-drawbacks",
    "title": "The Parametric Bootstrap",
    "section": "Potential Drawbacks",
    "text": "Potential Drawbacks\n\nNew source of error: model specification\nMisspecified models can completely distort estimates."
  },
  {
    "objectID": "slides/lecture08-1.html#example-100-year-return-periods",
    "href": "slides/lecture08-1.html#example-100-year-return-periods",
    "title": "The Parametric Bootstrap",
    "section": "Example: 100-Year Return Periods",
    "text": "Example: 100-Year Return Periods\nDetrended San Francisco Tide Gauge Data:\n\n\nCode\n# read in data and get annual maxima\nfunction load_data(fname)\n    date_format = DateFormat(\"yyyy-mm-dd HH:MM:SS\")\n    # This uses the DataFramesMeta.jl package, which makes it easy to string together commands to load and process data\n    df = @chain fname begin\n        CSV.read(DataFrame; header=false)\n        rename(\"Column1\" =&gt; \"year\", \"Column2\" =&gt; \"month\", \"Column3\" =&gt; \"day\", \"Column4\" =&gt; \"hour\", \"Column5\" =&gt; \"gauge\")\n        # need to reformat the decimal date in the data file\n        @transform :datetime = DateTime.(:year, :month, :day, :hour)\n        # replace -99999 with missing\n        @transform :gauge = ifelse.(abs.(:gauge) .&gt;= 9999, missing, :gauge)\n        select(:datetime, :gauge)\n    end\n    return df\nend\n\ndat = load_data(\"data/surge/h551.csv\")\n\n# detrend the data to remove the effects of sea-level rise and seasonal dynamics\nma_length = 366\nma_offset = Int(floor(ma_length/2))\nmoving_average(series,n) = [mean(@view series[i-n:i+n]) for i in n+1:length(series)-n]\ndat_ma = DataFrame(datetime=dat.datetime[ma_offset+1:end-ma_offset], residual=dat.gauge[ma_offset+1:end-ma_offset] .- moving_average(dat.gauge, ma_offset))\n\n# group data by year and compute the annual maxima\ndat_ma = dropmissing(dat_ma) # drop missing data\ndat_annmax = combine(dat_ma -&gt; dat_ma[argmax(dat_ma.residual), :], groupby(transform(dat_ma, :datetime =&gt; x-&gt;year.(x)), :datetime_function))\ndelete!(dat_annmax, nrow(dat_annmax)) # delete 2023; haven't seen much of that year yet\nrename!(dat_annmax, :datetime_function =&gt; :Year)\nselect!(dat_annmax, [:Year, :residual])\ndat_annmax.residual = dat_annmax.residual / 1000 # convert to m\n\n# make plots\np1 = plot(\n    dat_annmax.Year,\n    dat_annmax.residual;\n    xlabel=\"Year\",\n    ylabel=\"Annual Max Tide (m)\",\n    label=false,\n    marker=:circle,\n    markersize=5,\n    tickfontsize=16,\n    guidefontsize=18\n)\np2 = histogram(\n    dat_annmax.residual,\n    normalize=:pdf,\n    orientation=:horizontal,\n    label=:false,\n    xlabel=\"PDF\",\n    ylabel=\"\",\n    yticks=[],\n    tickfontsize=16,\n    guidefontsize=18\n)\n\nl = @layout [a{0.7w} b{0.3w}]\nplot(p1, p2; layout=l, link=:y, ylims=(1, 1.7), bottom_margin=10mm, left_margin=5mm)\nplot!(size=(1000, 350))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Annual maxima surge data from the San Francisco, CA tide gauge."
  },
  {
    "objectID": "slides/lecture08-1.html#parametric-bootstrap-strategy",
    "href": "slides/lecture08-1.html#parametric-bootstrap-strategy",
    "title": "The Parametric Bootstrap",
    "section": "Parametric Bootstrap Strategy",
    "text": "Parametric Bootstrap Strategy\n\nFit GEV Model\nCompute 0.99 Quantile\nRepeat \\(N\\) times:\n\nResample Extreme Values from GEV\nCalculate 0.99 quantile.\n\nCompute Confidence Intervals"
  },
  {
    "objectID": "slides/lecture08-1.html#parametric-bootstrap-results",
    "href": "slides/lecture08-1.html#parametric-bootstrap-results",
    "title": "The Parametric Bootstrap",
    "section": "Parametric Bootstrap Results",
    "text": "Parametric Bootstrap Results\n\n\nCode\n# function to fit GEV model for each data set\ninit_θ = [1.0, 1.0, 1.0]\ngev_lik(θ) = -sum(logpdf(GeneralizedExtremeValue(θ[1], θ[2], θ[3]), dat_annmax.residual))\n\n# get estimates from observations\nrp_emp = quantile(dat_annmax.residual, 0.99)\nθ_mle = Optim.optimize(gev_lik, init_θ).minimizer\n\np = histogram(dat_annmax.residual,  normalize=:pdf, xlabel=\"Annual Maximum Storm Tide (m)\", ylabel=\"Probability Density\", tickfontsize=16, guidefontsize=18, label=false, right_margin=5mm, bottom_margin=5mm, legendfontsize=18, left_margin=5mm)\nplot!(p, GeneralizedExtremeValue(θ_mle[1], θ_mle[2], θ_mle[3]), linewidth=3, label=\"Parametric Model\")\nvline!(p, [rp_emp], color=:red, linewidth=3, linestyle=:dash, label=\"Empirical Return Level\")\nxlims!(p ,0, 2)\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Fitted GEV Distribution"
  },
  {
    "objectID": "slides/lecture08-1.html#adding-bootstrap-samples",
    "href": "slides/lecture08-1.html#adding-bootstrap-samples",
    "title": "The Parametric Bootstrap",
    "section": "Adding Bootstrap Samples",
    "text": "Adding Bootstrap Samples\n\n\nCode\nn_boot = 1000\nboot_samp = rand(GeneralizedExtremeValue(θ_mle[1], θ_mle[2], θ_mle[3]), (nrow(dat_annmax), n_boot))\nrp_boot = mapslices(col -&gt; quantile(col, 0.99), boot_samp, dims=1)'\n\np = plot(GeneralizedExtremeValue(θ_mle[1], θ_mle[2], θ_mle[3]), linewidth=3, label=\"Parametric Model\", xlabel=\"Annual Maximum Storm Tide (m)\", ylabel=\"Probability Density\", tickfontsize=16, guidefontsize=18, right_margin=5mm, bottom_margin=5mm, legendfontsize=18, left_margin=5mm)\nvline!(p, [rp_emp], color=:red, linewidth=3, linestyle=:dash, label=\"Empirical Return Level\")\nxlims!(p ,0, 2)\nscatter!(p, boot_samp[:, 1], zeros(nrow(dat_annmax)), color=:black, label=\"Bootstrap Replicates\", markersize=5)\nvline!(p, [rp_boot[1]], color=:grey, linewidth=2, label=\"Bootstrap Estimate\")\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Initial bootstrap sample"
  },
  {
    "objectID": "slides/lecture08-1.html#bootstrap-confidence-interval",
    "href": "slides/lecture08-1.html#bootstrap-confidence-interval",
    "title": "The Parametric Bootstrap",
    "section": "Bootstrap Confidence Interval",
    "text": "Bootstrap Confidence Interval\n\n\nCode\np = histogram(rp_boot, xlabel=\"100-Year Return Period Estimate (m)\", ylabel=\"Count\", tickfontsize=16, guidefontsize=18, legendfontsize=18, label=false, right_margin=5mm, bottom_margin=5mm, left_margin=5mm)\nvline!(p, [rp_emp], color=:red, linewidth=3, linestyle=:dash, label=\"Empirical Estimate\")\nq_boot = 2 * rp_emp .- quantile(rp_boot, [0.975, 0.025])\nvspan!(p, q_boot, linecolor=:grey, fillcolor=:grey, alpha=0.3, fillalpha=0.3, label=\"95% CI\")\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Bootstrap histogram"
  },
  {
    "objectID": "slides/lecture08-1.html#when-to-use-the-parametric-bootstrap",
    "href": "slides/lecture08-1.html#when-to-use-the-parametric-bootstrap",
    "title": "The Parametric Bootstrap",
    "section": "When To Use The Parametric Bootstrap?",
    "text": "When To Use The Parametric Bootstrap?\n\nReasonable to specify model (but uncertain about parameters/statistics);\nInterested in statistics where model provides needed structure (e.g. extremes, dependent data);"
  },
  {
    "objectID": "slides/lecture08-1.html#residual-resampling-scheme",
    "href": "slides/lecture08-1.html#residual-resampling-scheme",
    "title": "The Parametric Bootstrap",
    "section": "Residual Resampling Scheme",
    "text": "Residual Resampling Scheme\n\nFit a trend/mechanistic model;\nNon-parametrically bootstrap residuals and refit model."
  },
  {
    "objectID": "slides/lecture08-1.html#example-bootstrapping-sea-level-rise",
    "href": "slides/lecture08-1.html#example-bootstrapping-sea-level-rise",
    "title": "The Parametric Bootstrap",
    "section": "Example: Bootstrapping Sea-Level Rise",
    "text": "Example: Bootstrapping Sea-Level Rise\n\\[\n\\begin{gather*}\nH_t = F_t(\\theta; T) + \\varepsilon_t \\\\\n\\varepsilon_t \\sim \\mathcal{N}(0, \\sigma)\n\\end{gather*}\n\\]\n\nFind MLE for \\(\\theta\\);\nResample residuals and add back;\nRefit model."
  },
  {
    "objectID": "slides/lecture08-1.html#residual-distribution-from-mle-fit",
    "href": "slides/lecture08-1.html#residual-distribution-from-mle-fit",
    "title": "The Parametric Bootstrap",
    "section": "Residual Distribution from MLE Fit",
    "text": "Residual Distribution from MLE Fit\n\n\nCode\n# load data files\nslr_data = CSV.read(\"data/sealevel/CSIRO_Recons_gmsl_yr_2015.csv\", DataFrame)\ngmt_data = CSV.read(\"data/climate/HadCRUT.5.0.1.0.analysis.summary_series.global.annual.csv\", DataFrame)\nslr_data[:, :Time] = slr_data[:, :Time] .- 0.5; # remove 0.5 from Times\ndat = leftjoin(slr_data, gmt_data, on=\"Time\") # join data frames on time\n\n# slr_model: function to simulate sea-level rise from global mean temperature based on the Rahmstorf (2007) model\nfunction slr_model(α, T₀, H₀, temp_data)\n    temp_effect = α .* (temp_data .- T₀)\n    slr_predict = cumsum(temp_effect) .+ H₀\n    return slr_predict\nend\n\n# split data structure into individual pieces\nyears = dat[:, 1]\nsealevels = dat[:, 2]\ntemp = dat[:, 4]\n\n# write function to calculate likelihood of residuals for given parameters\n# parameters are a vector [α, T₀, H₀, σ]\nfunction llik_normal(params, temp_data, slr_data)\n    slr_out = slr_model(params[1], params[2], params[3], temp_data)\n    resids = slr_out - slr_data\n    return sum(logpdf.(Normal(0, params[4]), resids))\nend\n\n# set up lower and upper bounds for the parameters for the optimization\nlbds = [0.0, -50.0, -200.0, 0.0]\nubds = [10.0, 1.0, 0.0, 20.0]\np0 = [5.0, -1.0, -100.0, 5.0]\np_mle = Optim.optimize(p -&gt; -llik_normal(p, temp, sealevels), lbds, ubds, p0).minimizer\n\n# compute residuals\nslr_model_out = slr_model(p_mle[1], p_mle[2], p_mle[3], temp)\nresids = slr_model_out - sealevels\nhistogram(resids, xlabel=\"Residual (mm)\", ylabel=\"Count\", legend=false, tickfontsize=16, guidefontsize=18, left_margin=5mm, bottom_margin=5mm)\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Residuals from fitted SLR Model"
  },
  {
    "objectID": "slides/lecture08-1.html#bootstrapped-realizations",
    "href": "slides/lecture08-1.html#bootstrapped-realizations",
    "title": "The Parametric Bootstrap",
    "section": "Bootstrapped Realizations",
    "text": "Bootstrapped Realizations\n\n\nCode\nnboot = 1000\nslr_boot = zeros(length(resids), nboot) # preallocate storage\nfor i = 1:nboot\n    slr_boot[:, i] = slr_model_out + sample(resids, length(resids); replace=true)\nend\np = plot(dat[:, 1], dat[:, 2], color=:black, linewidth=2, label=\"Observations\", xlabel=\"Year\", ylabel=\"Global Mean Sea Level (mm)\", tickfontsize=16, guidefontsize=18, legendfontsize=18, left_margin=5mm, bottom_margin=5mm)\nfor idx in 1:10\n    label = idx == 1 ? \"Bootstrap Realizations\" : false\n    plot!(p, dat[:, 1], slr_boot[:, idx]; color=:grey, alpha=0.5, label=label, linewidth=1)\nend\np\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Bootstrap realizations from the SLR model"
  },
  {
    "objectID": "slides/lecture08-1.html#bootstrap-estimates-of-slr-temperature-sensitivity",
    "href": "slides/lecture08-1.html#bootstrap-estimates-of-slr-temperature-sensitivity",
    "title": "The Parametric Bootstrap",
    "section": "Bootstrap Estimates of SLR Temperature Sensitivity",
    "text": "Bootstrap Estimates of SLR Temperature Sensitivity\n\n\nCode\n# fit model repeatedly\np_boot = mapslices(col -&gt; Optim.optimize(p -&gt; -llik_normal(p, temp, col), lbds, ubds, p0).minimizer, slr_boot, dims=1)\n\nplt = histogram(p_boot[1, :], xlabel=L\"$\\alpha$ (mm/°C)\", ylabel=\"Count\", label=false, tickfontsize=16, guidefontsize=18, legendfontsize=18, left_margin=5mm, bottom_margin=10mm)\nvline!(plt, [p_mle[1]], color=:red, linestyle=:dash, linewidth=3, label=\"Estimated Value\")\nq_boot = 2 * p_mle[1] .- quantile(p_boot[1, :], [0.975, 0.025])\nvspan!(plt, q_boot, linecolor=:grey, fillcolor=:grey, alpha=0.3, fillalpha=0.3, label=\"95% CI\")\nplot!(plt, size=(1000, 400))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: Bootstrap realizations from the SLR model"
  },
  {
    "objectID": "slides/lecture08-1.html#what-does-the-bootstrap-give-us",
    "href": "slides/lecture08-1.html#what-does-the-bootstrap-give-us",
    "title": "The Parametric Bootstrap",
    "section": "What Does The Bootstrap Give Us?",
    "text": "What Does The Bootstrap Give Us?\nThe bootstrap gives us an estimate of the sampling distribution of a statistic or parameter.\nWe can use this for confidence intervals, statistical tests, bias correction, etc."
  },
  {
    "objectID": "slides/lecture08-1.html#bootstrap-vs.-monte-carlo",
    "href": "slides/lecture08-1.html#bootstrap-vs.-monte-carlo",
    "title": "The Parametric Bootstrap",
    "section": "Bootstrap vs. Monte Carlo",
    "text": "Bootstrap vs. Monte Carlo\nBootstrap “if I had a different sample (conditional on the bootstrap principle), what could I have inferred”?\nMonte Carlo: Given specification of input uncertainty, what data could we generate?"
  },
  {
    "objectID": "slides/lecture08-1.html#bootstrap-distribution-and-monte-carlo",
    "href": "slides/lecture08-1.html#bootstrap-distribution-and-monte-carlo",
    "title": "The Parametric Bootstrap",
    "section": "Bootstrap Distribution and Monte Carlo",
    "text": "Bootstrap Distribution and Monte Carlo\nCould we use a bootstrap distribution for MC?\n\n\nSure, that’s just one specification of the data-generating process.\nNothing unique or particularly rigorous in using the bootstrap for this; substituting the bootstrap principle for other assumptions."
  },
  {
    "objectID": "slides/lecture08-1.html#bootstrap-vs.-bayes",
    "href": "slides/lecture08-1.html#bootstrap-vs.-bayes",
    "title": "The Parametric Bootstrap",
    "section": "Bootstrap vs. Bayes",
    "text": "Bootstrap vs. Bayes\nBootstrap: “if I had a different sample (conditional on the bootstrap principle), what could I have inferred”?\nBayesian Inference: “what different parameters could have produced the observed data”?"
  },
  {
    "objectID": "slides/lecture08-1.html#key-points",
    "href": "slides/lecture08-1.html#key-points",
    "title": "The Parametric Bootstrap",
    "section": "Key Points",
    "text": "Key Points\n\nBootstrap Principle: Use the data as a proxy for the population.\nKey: Bootstrap gives idea of sampling error in statistics (including model parameters)\nDistribution of \\(\\tilde{t} - \\hat{t}\\) approximates distribution around estimate \\(\\hat{t} - t_0\\).\nAllows us to estimate uncertainty of estimates (confidence intervals, bias, etc).\nParametric bootstrap introduces model specification error"
  },
  {
    "objectID": "slides/lecture08-1.html#bootstrap-variants",
    "href": "slides/lecture08-1.html#bootstrap-variants",
    "title": "The Parametric Bootstrap",
    "section": "Bootstrap Variants",
    "text": "Bootstrap Variants\n\nResample Cases (Non-Parametric)\nResample Residuals (Semi-Parametric)\nSimulate from Fitted Model (Parametric)"
  },
  {
    "objectID": "slides/lecture08-1.html#which-bootstrap-to-use",
    "href": "slides/lecture08-1.html#which-bootstrap-to-use",
    "title": "The Parametric Bootstrap",
    "section": "Which Bootstrap To Use?",
    "text": "Which Bootstrap To Use?\n\nBias-Variance Tradeoff: Parametric Bootstrap has narrowest intervals, Resampling Cases widest (Exercise 8)\nDepends on trust in model “correctness”:\n\nDo we trust the model parameters to be “correct”?\nDo we trust the shape of the regression model?\nDo we trust the data-generating process?"
  },
  {
    "objectID": "slides/lecture08-1.html#next-classes",
    "href": "slides/lecture08-1.html#next-classes",
    "title": "The Parametric Bootstrap",
    "section": "Next Classes",
    "text": "Next Classes\nWednesday: Bayesian Computation and Markov chains\nNext Week: Markov chain Monte Carlo"
  },
  {
    "objectID": "slides/lecture08-1.html#assessments",
    "href": "slides/lecture08-1.html#assessments",
    "title": "The Parametric Bootstrap",
    "section": "Assessments",
    "text": "Assessments\n\nReading: Rahmstorf & Coumou (2011)\nExercise 8: Due Friday\nHomework 3: Due 3/22"
  },
  {
    "objectID": "slides/lecture08-1.html#references-1",
    "href": "slides/lecture08-1.html#references-1",
    "title": "The Parametric Bootstrap",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n\n\nEfron, B. (1979). Bootstrap methods: Another look at the jackknife. Ann. Stat., 7, 1–26. https://doi.org/10.1214/aos/1176344552\n\n\nRahmstorf, S., & Coumou, D. (2011). Increase of extreme events in a warming world. Proceedings of the National Academy of Sciences, 108, 17905–17909. https://doi.org/10.1073/pnas.1101766108"
  },
  {
    "objectID": "slides/lecture12-2.html#simulation-as-hypothesis-testing",
    "href": "slides/lecture12-2.html#simulation-as-hypothesis-testing",
    "title": "Predictive Model Assessment",
    "section": "Simulation as Hypothesis Testing",
    "text": "Simulation as Hypothesis Testing\n\n\nDefine models for hypotheses (including baseline/null).\nFit models to data.\nSimulate alternative datasets.\nCompare output/test statistics to data.\nMaybe: Convert metrics to probabilities (condition on \\(\\mathcal{M}\\))."
  },
  {
    "objectID": "slides/lecture12-2.html#metrics-for-model-adequacyperformance",
    "href": "slides/lecture12-2.html#metrics-for-model-adequacyperformance",
    "title": "Predictive Model Assessment",
    "section": "Metrics for Model Adequacy/Performance",
    "text": "Metrics for Model Adequacy/Performance\n\nExplanatory Metrics\n\nBased on same training/calibration data\nRMSE, R2, \\(\\log p(y | M)\\)\n\nPredictive Metrics\n\nHeld-out data/cross-validation"
  },
  {
    "objectID": "slides/lecture12-2.html#what-is-the-goal-of-model-selection",
    "href": "slides/lecture12-2.html#what-is-the-goal-of-model-selection",
    "title": "Predictive Model Assessment",
    "section": "What Is The Goal of Model Selection?",
    "text": "What Is The Goal of Model Selection?\nKey Idea: Model selection consists of navigating the bias-variance tradeoff.\nModel error (e.g. RMSE) is a combination of irreducible error, bias, and variance."
  },
  {
    "objectID": "slides/lecture12-2.html#setting-for-model-selection",
    "href": "slides/lecture12-2.html#setting-for-model-selection",
    "title": "Predictive Model Assessment",
    "section": "Setting for Model Selection",
    "text": "Setting for Model Selection\nSuppose we have a data-generating model \\[y = f(x) + \\varepsilon, \\varepsilon \\sim N(0, \\sigma)\\]. We want to fit a model \\(\\hat{y} \\approx \\hat{f}(x)\\)."
  },
  {
    "objectID": "slides/lecture12-2.html#bias",
    "href": "slides/lecture12-2.html#bias",
    "title": "Predictive Model Assessment",
    "section": "Bias",
    "text": "Bias\nBias is error from mismatches between the model predictions and the data (\\(\\text{Bias}[\\hat{f}] = \\mathbb{E}[\\hat{f}] - y\\)).\nBias comes from under-fitting meaningful relationships between inputs and outputs.\n\ntoo few degrees of freedom (“too simple”)\nneglected processes."
  },
  {
    "objectID": "slides/lecture12-2.html#variance",
    "href": "slides/lecture12-2.html#variance",
    "title": "Predictive Model Assessment",
    "section": "Variance",
    "text": "Variance\nVariance is error from over-sensitivity to small fluctuations in inputs (\\(\\text{Variance} = \\text{Var}(\\hat{f})\\)).\nVariance can come from over-fitting noise in the data.\n\ntoo many degrees of freedom (“too complex”)\npoor identifiability"
  },
  {
    "objectID": "slides/lecture12-2.html#decomposition-of-mse",
    "href": "slides/lecture12-2.html#decomposition-of-mse",
    "title": "Predictive Model Assessment",
    "section": "Decomposition of MSE",
    "text": "Decomposition of MSE\n\\[\n\\begin{align*}\n\\text{MSE} &= \\mathbb{E}[y - \\hat{f}^2] \\\\\n&= \\mathbb{E}[y^2 - 2y\\hat{f}(x) + \\hat{f}^2] \\\\\n&= \\mathbb{E}[y^2] - 2\\mathbb{E}[y\\hat{f}] + E[\\hat{f}^2] \\\\\n&= \\mathbb{E}[(f + \\varepsilon)^2] - \\mathbb{E}[(f + \\varepsilon)\\hat{f}] + E[\\hat{f}^2] \\\\\n&= \\vdots \\\\\n&= \\text{Bias}(\\hat{f})^2 + \\text{Var}(\\hat{f}) + \\sigma^2\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/lecture12-2.html#bias-variance-tradeoff-1",
    "href": "slides/lecture12-2.html#bias-variance-tradeoff-1",
    "title": "Predictive Model Assessment",
    "section": "Bias-Variance Tradeoff",
    "text": "Bias-Variance Tradeoff\nUpshot: For achieve a fixed error level, you can reduce bias (more “complex” model) or you can reduce variance (more “simple” model) but there’s a tradeoff."
  },
  {
    "objectID": "slides/lecture12-2.html#bias-variance-tradeoff-more-generally",
    "href": "slides/lecture12-2.html#bias-variance-tradeoff-more-generally",
    "title": "Predictive Model Assessment",
    "section": "Bias-Variance Tradeoff More Generally",
    "text": "Bias-Variance Tradeoff More Generally\nThis decomposition is for MSE, but the principle holds more generally.\n\nModels which perform better “on average” over the training data (low bias) are more likely to overfit (high variance);\nModels which have less uncertainty for training data (low variance) will do worse “on average”."
  },
  {
    "objectID": "slides/lecture12-2.html#model-complexity",
    "href": "slides/lecture12-2.html#model-complexity",
    "title": "Predictive Model Assessment",
    "section": "Model Complexity",
    "text": "Model Complexity\nModel complexity is not necessarily the same as the number of parameters.\nSometimes processes in the model can compensate for each other (“reducing degrees of freedom”)\nThis can help improve the representation of the dynamics and reduce error/uncertainty even when additional parameters are included."
  },
  {
    "objectID": "slides/lecture12-2.html#bias-variance-and-complexity",
    "href": "slides/lecture12-2.html#bias-variance-and-complexity",
    "title": "Predictive Model Assessment",
    "section": "Bias-Variance and Complexity",
    "text": "Bias-Variance and Complexity\n\n\n\n\nBias-Variance Tradeoff\n\n\n\nSource: Wikipedia"
  },
  {
    "objectID": "slides/lecture12-2.html#occams-razor",
    "href": "slides/lecture12-2.html#occams-razor",
    "title": "Predictive Model Assessment",
    "section": "Occam’s Razor",
    "text": "Occam’s Razor\n\n\nEntities are not to be multiplied without necessity.\n\n\nCredited to William of Ockham, appears much earlier in the works of Maimonides, Ptolemy, and Aristotle, first formulated as such by John Punch (1639)"
  },
  {
    "objectID": "slides/lecture12-2.html#zebra-principle",
    "href": "slides/lecture12-2.html#zebra-principle",
    "title": "Predictive Model Assessment",
    "section": "“Zebra Principle”",
    "text": "“Zebra Principle”\nMore colloquially:\n\n\nWhen you hear hoofbeats, think of horses, not zebras.\n\n\n— Theodore Woodward"
  },
  {
    "objectID": "slides/lecture12-2.html#log-likelihood-as-predictive-fit-measure",
    "href": "slides/lecture12-2.html#log-likelihood-as-predictive-fit-measure",
    "title": "Predictive Model Assessment",
    "section": "Log-Likelihood as Predictive Fit Measure",
    "text": "Log-Likelihood as Predictive Fit Measure\nThe measure of predictive fit that we will use:\nThe log predictive density or log-likelihood of a replicated data point/set, \\[p(y^{rep} | \\theta).\\]"
  },
  {
    "objectID": "slides/lecture12-2.html#why-use-log-likelihood",
    "href": "slides/lecture12-2.html#why-use-log-likelihood",
    "title": "Predictive Model Assessment",
    "section": "Why Use Log-Likelihood?",
    "text": "Why Use Log-Likelihood?\nWhy use the log-likelihood density instead of the log-posterior?\n\nThe likelihood captures the data-generating process;\nThe posterior includes the prior, which is only relevant for parameter estimation.\n\nImportant: This means that the prior is still relevant in predictive model assessment, and should be thought of as part of the model structure!"
  },
  {
    "objectID": "slides/lecture12-2.html#cross-validation",
    "href": "slides/lecture12-2.html#cross-validation",
    "title": "Predictive Model Assessment",
    "section": "Cross-Validation",
    "text": "Cross-Validation\nThe “gold standard” way to test for predictive performance is cross-validation:\n\nSplit data into training/testing sets;\nCalibrate model to training set;\nCheck for predictive ability on testing set."
  },
  {
    "objectID": "slides/lecture12-2.html#leave-one-out-cross-validation",
    "href": "slides/lecture12-2.html#leave-one-out-cross-validation",
    "title": "Predictive Model Assessment",
    "section": "Leave-One-Out Cross-Validation",
    "text": "Leave-One-Out Cross-Validation\n\nDrop one value \\(y_i\\).\nRefit model on rest of data \\(y_{-i}\\).\nEvaluate \\(\\log p(y_i | y_{-i})\\).\nRepeat on rest of data set."
  },
  {
    "objectID": "slides/lecture12-2.html#loo-cv-example",
    "href": "slides/lecture12-2.html#loo-cv-example",
    "title": "Predictive Model Assessment",
    "section": "LOO-CV Example",
    "text": "LOO-CV Example\n\n\nCode\ndrop_idx = rand(1:nrow(dat_annmax), 1)\np1 = plot(\n    dat_annmax.Year[setdiff(1:end, drop_idx)],\n    dat_annmax.residual[setdiff(1:end, drop_idx)];\n    xlabel=\"Year\",\n    ylabel=\"Annual Max Tide Level (m)\",\n    label=false,\n    marker=:circle,\n    markersize=5,\n    tickfontsize=16,\n    guidefontsize=18,\n    left_margin=5mm, \n    bottom_margin=5mm\n)\nscatter!(p1,\n    dat_annmax.Year[drop_idx],\n    dat_annmax.residual[drop_idx],\n    color=:red,\n    label=false,\n    markersize=5,\n    marker=:circle\n)\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1"
  },
  {
    "objectID": "slides/lecture12-2.html#loo-cv-example-1",
    "href": "slides/lecture12-2.html#loo-cv-example-1",
    "title": "Predictive Model Assessment",
    "section": "LOO-CV Example",
    "text": "LOO-CV Example\n\n\n\n\nCode\n# fit held-out data\nstat_lb = [1000.0, 0.0, -1.0]\nstat_ub = [2000.0, 100.0, 1.0]\nstat_p0 = [1200.0, 50.0, 0.01]\nnonstat_lb = [1000.0, -20.0, 0.0, -5.0]\nnonstat_ub = [2000.0, 20.0, 100.0, 5.0]\nnonstat_p0 = [1200.0, 0.5, 50.0, 0.05]\n\nstat_cv = Optim.optimize(p -&gt; -sum(logpdf.(GeneralizedExtremeValue(p[1], p[2], p[3]), dat_annmax.residual[setdiff(1:end, drop_idx)])), stat_lb, stat_ub, stat_p0)\nstat_cv_mle = stat_cv.minimizer\nstat_cv_logp = logpdf(GeneralizedExtremeValue(stat_cv_mle[1], stat_cv_mle[2], stat_cv_mle[3]), dat_annmax.residual[drop_idx])\n\np = plot(GeneralizedExtremeValue(stat_cv_mle[1], stat_cv_mle[2], stat_cv_mle[3]),\n    xlabel=\"Storm Surge Extreme (mm)\",\n    ylabel=\"Probability Density\",\n    xlims=(1000, 1800),\n    linewidth=3,\n    label=false,\n    tickfontsize=16,\n    guidefontsize=18,\n    legendfontsize=16,\n    left_margin=5mm, \n    bottom_margin=5mm,\n    right_margin=10mm\n)\nvline!(p, [dat_annmax.residual[drop_idx]], color=:red, linestyle=:dash, label=\"Held-out Value\", linewidth=2)\nplot!(p, size=(600, 400))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: MLE for GEV fit without held out data point.\n\n\n\n\n\n\\(\\log p(y_i | y_{-i})\\):\n-5.1."
  },
  {
    "objectID": "slides/lecture12-2.html#loo-cv-example-2",
    "href": "slides/lecture12-2.html#loo-cv-example-2",
    "title": "Predictive Model Assessment",
    "section": "LOO-CV Example",
    "text": "LOO-CV Example\n\n\n\n2×2 DataFrame\n\n\n\nRow\nModel\nLOOCV\n\n\n\nString\nFloat64\n\n\n\n\n1\nStationary\n-711.064\n\n\n2\nNonstationary\n-706.663"
  },
  {
    "objectID": "slides/lecture12-2.html#bayesian-loo-cv",
    "href": "slides/lecture12-2.html#bayesian-loo-cv",
    "title": "Predictive Model Assessment",
    "section": "Bayesian LOO-CV",
    "text": "Bayesian LOO-CV\nBy default, Bayesian LOO-CV is extremely expensive:\n\\[\\text{loo-cv} = \\sum_{i=1}^n \\log p_{\\text{post}(-i)}(y_i),\\]\nwhich requires refitting the model without \\(y_i\\) for every data point."
  },
  {
    "objectID": "slides/lecture12-2.html#leave-k-out-cross-validation",
    "href": "slides/lecture12-2.html#leave-k-out-cross-validation",
    "title": "Predictive Model Assessment",
    "section": "Leave-\\(k\\)-Out Cross-Validation",
    "text": "Leave-\\(k\\)-Out Cross-Validation\nDrop \\(k\\) values, refit model on rest of data, check for predictive skill.\nAs \\(k \\to n\\), this reduces to the prior predictive distribution \\[p(y^{\\text{rep}}) = \\int_{\\theta} p(y^{\\text{rep}} | \\theta) p(\\theta) d\\theta.\\]"
  },
  {
    "objectID": "slides/lecture12-2.html#challenges-with-cross-validation",
    "href": "slides/lecture12-2.html#challenges-with-cross-validation",
    "title": "Predictive Model Assessment",
    "section": "Challenges with Cross-Validation",
    "text": "Challenges with Cross-Validation\n\n\nThis can be very computationally expensive!\nWe often don’t have a lot of data for calibration, so holding some back can be a problem.\nHow to divide data with spatial or temporal structure? This can be addressed by partitioning the data more cleverly: \\[y = \\{y_{1:t}, y_{-((t+1):T)}\\}\\] but this makes the data problem worse."
  },
  {
    "objectID": "slides/lecture12-2.html#expected-out-of-sample-predictive-accuracy",
    "href": "slides/lecture12-2.html#expected-out-of-sample-predictive-accuracy",
    "title": "Predictive Model Assessment",
    "section": "Expected Out-Of-Sample Predictive Accuracy",
    "text": "Expected Out-Of-Sample Predictive Accuracy\nThe out-of-sample predictive fit of a new data point \\(\\tilde{y}_i\\) is\n\\[\n\\begin{align}\n\\log p_\\text{post}(\\tilde{y}_i) &= \\log \\mathbb{E}_\\text{post}\\left[p(\\tilde{y}_i | \\theta)\\right] \\\\\n&= \\log \\int p(\\tilde{y_i} | \\theta) p_\\text{post}(\\theta)\\,d\\theta.\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/lecture12-2.html#expected-out-of-sample-predictive-accuracy-1",
    "href": "slides/lecture12-2.html#expected-out-of-sample-predictive-accuracy-1",
    "title": "Predictive Model Assessment",
    "section": "Expected Out-Of-Sample Predictive Accuracy",
    "text": "Expected Out-Of-Sample Predictive Accuracy\nHowever, the out-of-sample data \\(\\tilde{y}_i\\) is itself unknown, so we need to compute the expected out-of-sample log-predictive density\n\\[\n\\begin{align}\n\\text{elpd} &= \\text{expected log-predictive density for } \\tilde{y}_i \\\\\n&= \\mathbb{E}_P \\left[\\log p_\\text{post}(\\tilde{y}_i)\\right] \\\\\n&= \\int \\log\\left(p_\\text{post}(\\tilde{y}_i)\\right) P(\\tilde{y}_i)\\,d\\tilde{y}.\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/lecture12-2.html#expected-out-of-sample-predictive-accuracy-2",
    "href": "slides/lecture12-2.html#expected-out-of-sample-predictive-accuracy-2",
    "title": "Predictive Model Assessment",
    "section": "Expected Out-Of-Sample Predictive Accuracy",
    "text": "Expected Out-Of-Sample Predictive Accuracy\nWhat is the challenge?\n\nWe don’t know \\(P\\) (the distribution of new data)!\nWe need some measure of the error induced by using an approximating distribution \\(Q\\) from some model."
  },
  {
    "objectID": "slides/lecture12-2.html#key-takeaways",
    "href": "slides/lecture12-2.html#key-takeaways",
    "title": "Predictive Model Assessment",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nModel selection is a balance between bias (underfitting) and variance (overfitting).\nFor predictive assessment, leave-one-out cross-validation is an ideal, but hard to implement in practice (particularly for time series)."
  },
  {
    "objectID": "slides/lecture12-2.html#an-important-caveat",
    "href": "slides/lecture12-2.html#an-important-caveat",
    "title": "Predictive Model Assessment",
    "section": "An Important Caveat",
    "text": "An Important Caveat\nModel selection can result in significant overfitting when separated from hypothesis-driven model development (Freedman, 1983; Smith, 2018)"
  },
  {
    "objectID": "slides/lecture12-2.html#an-important-caveat-1",
    "href": "slides/lecture12-2.html#an-important-caveat-1",
    "title": "Predictive Model Assessment",
    "section": "An Important Caveat",
    "text": "An Important Caveat\n\nBetter off thinking about the scientific or engineering problem you want to solve and use domain knowledge/checks rather than throwing a large number of possible models into the selection machinery.\nRegularizing priors reduce potential for overfitting.\nModel averaging (Hoeting et al., 2021) and stacking (Yao et al., 2018) can combine multiple models as an alternative to selection."
  },
  {
    "objectID": "slides/lecture12-2.html#next-classes",
    "href": "slides/lecture12-2.html#next-classes",
    "title": "Predictive Model Assessment",
    "section": "Next Classes",
    "text": "Next Classes\nMonday: Information Criteria"
  },
  {
    "objectID": "slides/lecture12-2.html#references-1",
    "href": "slides/lecture12-2.html#references-1",
    "title": "Predictive Model Assessment",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n\n\nFreedman, D. A. (1983). A note on screening regression equations. Am. Stat., 37, 152. https://doi.org/10.2307/2685877\n\n\nHoeting, J. A., Madigan, D., Raftery, A. E., & Volinsky, C. T. (2021). Bayesian model averaging: a tutorial. Stat. Sci., 14, 382–401. https://doi.org/10.1214/ss/1009212519\n\n\nSmith, G. (2018). Step away from stepwise. J. Big Data, 5, 1–12. https://doi.org/10.1186/s40537-018-0143-6\n\n\nYao, Y., Vehtari, A., Simpson, D., & Gelman, A. (2018). Using stacking to average Bayesian predictive distributions. arXiv [Stat.ME]. https://doi.org/10.1214/17-BA1091"
  },
  {
    "objectID": "slides/lecture12-1.html#hypothesis-testing",
    "href": "slides/lecture12-1.html#hypothesis-testing",
    "title": "Model Assessment",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\nStandard null hypothesis significance testing framework:\n\nFrame problem with null hypothesis \\(\\mathcal{H}_0\\) and alternative hypothesis \\(\\mathcal{H}\\).\nSet significance level \\(\\alpha\\).\nFind \\(p\\)-value \\[\\mathbb{P}\\left(y_{\\mathcal{H}_0} &gt; y\\right)\\]"
  },
  {
    "objectID": "slides/lecture12-1.html#what-is-a-p-value",
    "href": "slides/lecture12-1.html#what-is-a-p-value",
    "title": "Model Assessment",
    "section": "What is a \\(p\\)-value?",
    "text": "What is a \\(p\\)-value?\n\n\nThe p-value captures the probability that, assuming the null hypothesis, you would observe results at least as extreme as you observed.\n\n\n\nCode\ntest_dist = Normal(0, 3)\nx = -10:0.01:10\nplot(x, pdf.(test_dist, x), linewidth=3, legend=false, color=:black, xticks=false,  yticks=false, ylabel=\"Probability\", xlabel=L\"y\", bottom_margin=10mm, left_margin=10mm, guidefontsize=16)\nvline!([5], linestyle=:dash, color=:purple, linewidth=3, )\nareaplot!(5:0.01:10, pdf.(test_dist, 5:0.01:10), color=:green, alpha=0.4)\nquiver!([-4.5], [0.095], quiver=([1], [-0.02]), color=:black, linewidth=2)\nannotate!([-5], [0.11], text(\"Null\\nSampling\\nDistribution\", color=:black))\nquiver!([6.5], [0.03], quiver=([-1], [-0.015]), color=:green, linewidth=2)\nannotate!([6.85], [0.035], text(\"p-value\", :green))\nquiver!([3.5], [0.02], quiver=([1.5], [0]), color=:purple, linewidth=2)\nannotate!([0.9], [0.02], text(\"Observation\", :purple))\nplot!(size=(650, 500))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Illustration of a p-value"
  },
  {
    "objectID": "slides/lecture12-1.html#rejecting-the-null",
    "href": "slides/lecture12-1.html#rejecting-the-null",
    "title": "Model Assessment",
    "section": "Rejecting the Null",
    "text": "Rejecting the Null\nIf \\(p\\text{-value} &lt; \\alpha\\), “reject” the null hypothesis in favor of the alternative and say that the effect is “statistically significant.”\nOtherwise, do not reject the null.\nThe goal is to strike a balance between Type I and Type II errors."
  },
  {
    "objectID": "slides/lecture12-1.html#error-types",
    "href": "slides/lecture12-1.html#error-types",
    "title": "Model Assessment",
    "section": "Error Types",
    "text": "Error Types\n\n\n\n\n\n\n\nNull Hypothesis Is\n\n\n\n\n\n\n\n\nTrue\n\n\nFalse\n\n\n\n\nDecision About Null Hypothesis\n\n\nDon’t reject\n\n\nTrue negative (probability \\(1-\\alpha\\))\n\n\nType II error (false negative, probability \\(\\beta\\))\n\n\n\n\nReject\n\n\nType I Error (false positive, probability \\(\\alpha\\))\n\n\nTrue positive (probability \\(1-\\beta\\))"
  },
  {
    "objectID": "slides/lecture12-1.html#but-what-is-statistical-significance",
    "href": "slides/lecture12-1.html#but-what-is-statistical-significance",
    "title": "Model Assessment",
    "section": "But What Is Statistical Significance?",
    "text": "But What Is Statistical Significance?\n\nBut, this doesn’t mean:\n\n\nThat the null is “wrong”;\nThat the alternative is a better descriptor of the data-generating process;\nThat the effect sized of the hypothesized mechanism is “significant”."
  },
  {
    "objectID": "slides/lecture12-1.html#what-a-p-value-is-not",
    "href": "slides/lecture12-1.html#what-a-p-value-is-not",
    "title": "Model Assessment",
    "section": "What a \\(p\\)-value is Not",
    "text": "What a \\(p\\)-value is Not\n\n\nProbability that the null hypothesis is true;\nProbability that the effect was produced by chance alone;\nAn indication of the effect size."
  },
  {
    "objectID": "slides/lecture12-1.html#how-might-we-do-better",
    "href": "slides/lecture12-1.html#how-might-we-do-better",
    "title": "Model Assessment",
    "section": "How Might We Do Better?",
    "text": "How Might We Do Better?\n\n\nConsideration of multiple plausible (possibly more nuanced) hypotheses.\nAssessment/quantification of evidence consistent with different hypotheses.\nInsight into the effect size."
  },
  {
    "objectID": "slides/lecture12-1.html#fundamental-data-analysis-challenge",
    "href": "slides/lecture12-1.html#fundamental-data-analysis-challenge",
    "title": "Model Assessment",
    "section": "Fundamental Data Analysis Challenge",
    "text": "Fundamental Data Analysis Challenge\nGoal (often): Explain data and/or make predictions about unobserved data.\nChallenges: Environmental systems are:\n\n\nhigh-dimensional\nmulti-scale\nnonlinear\nsubject to many uncertainties"
  },
  {
    "objectID": "slides/lecture12-1.html#multiplicities-of-models",
    "href": "slides/lecture12-1.html#multiplicities-of-models",
    "title": "Model Assessment",
    "section": "Multiplicities of Models",
    "text": "Multiplicities of Models\nIn general, we are in an \\(\\mathcal{M}\\)-open setting: no model is the “true” data-generating model, so we want to pick a model which performs well enough for the intended purpose.\nThe contrast to this is \\(\\mathcal{M}\\)-closed, in which one of the models under consideration is the “true” data-generating model, and we would like to recover it."
  },
  {
    "objectID": "slides/lecture12-1.html#what-is-any-statistical-test-doing",
    "href": "slides/lecture12-1.html#what-is-any-statistical-test-doing",
    "title": "Model Assessment",
    "section": "What Is Any Statistical Test Doing?",
    "text": "What Is Any Statistical Test Doing?\nIf we think about what a test like Mann-Kendall is doing:\n\nAssume the null hypothesis \\(H_0\\);\nObtain the sampling distribution of a test statistic \\(S\\) which captures the property of interest under \\(H_0\\);\nCompute the test statistic \\(\\hat{S}\\) on the data.\nCalculate the probability of \\(S\\) more extreme than \\(\\hat{S}\\) (the \\(p\\)-value).\n\n\nNone of this requires a NHST framework!"
  },
  {
    "objectID": "slides/lecture12-1.html#simulation-for-statistical-testing",
    "href": "slides/lecture12-1.html#simulation-for-statistical-testing",
    "title": "Model Assessment",
    "section": "Simulation for Statistical Testing",
    "text": "Simulation for Statistical Testing\nInstead, if we have a model which permits simulation:\n\nCalibrate models under different assumptions (e.g. stationarity vs. nonstationary based on different covariates);\nSimulate realizations from those models;\nCompute the distribution of the relevant statistic \\(S\\) from these realizations;\nAssess which distribution is most consistent with the observed quantity."
  },
  {
    "objectID": "slides/lecture12-1.html#model-assessment-criteria",
    "href": "slides/lecture12-1.html#model-assessment-criteria",
    "title": "Model Assessment",
    "section": "Model Assessment Criteria",
    "text": "Model Assessment Criteria\nHow do we assess models?\nTwo general categories:\n\n\nHow well do we explain the data?\nHow well do we predict new data?"
  },
  {
    "objectID": "slides/lecture12-1.html#explanatory-criteria",
    "href": "slides/lecture12-1.html#explanatory-criteria",
    "title": "Model Assessment",
    "section": "Explanatory Criteria",
    "text": "Explanatory Criteria\nGenerally based on the error (RMSE, MAE) or probability of the data \\(p(y | M)\\).\nTo select a model:\n\\[\\underset{M_i \\in \\mathcal{M}}{\\operatorname{argmax}} p(y |M_i)\\]"
  },
  {
    "objectID": "slides/lecture12-1.html#sf-tide-gauge-data",
    "href": "slides/lecture12-1.html#sf-tide-gauge-data",
    "title": "Model Assessment",
    "section": "SF Tide Gauge Data",
    "text": "SF Tide Gauge Data\n\n\nCode\n# load SF tide gauge data\n# read in data and get annual maxima\nfunction load_data(fname)\n    date_format = DateFormat(\"yyyy-mm-dd HH:MM:SS\")\n    # This uses the DataFramesMeta.jl package, which makes it easy to string together commands to load and process data\n    df = @chain fname begin\n        CSV.read(DataFrame; header=false)\n        rename(\"Column1\" =&gt; \"year\", \"Column2\" =&gt; \"month\", \"Column3\" =&gt; \"day\", \"Column4\" =&gt; \"hour\", \"Column5\" =&gt; \"gauge\")\n        # need to reformat the decimal date in the data file\n        @transform :datetime = DateTime.(:year, :month, :day, :hour)\n        # replace -99999 with missing\n        @transform :gauge = ifelse.(abs.(:gauge) .&gt;= 9999, missing, :gauge)\n        select(:datetime, :gauge)\n    end\n    return df\nend\n\ndat = load_data(\"data/surge/h551.csv\")\n\n# detrend the data to remove the effects of sea-level rise and seasonal dynamics\nma_length = 366\nma_offset = Int(floor(ma_length/2))\nmoving_average(series,n) = [mean(@view series[i-n:i+n]) for i in n+1:length(series)-n]\ndat_ma = DataFrame(datetime=dat.datetime[ma_offset+1:end-ma_offset], residual=dat.gauge[ma_offset+1:end-ma_offset] .- moving_average(dat.gauge, ma_offset))\n\n# group data by year and compute the annual maxima\ndat_ma = dropmissing(dat_ma) # drop missing data\ndat_annmax = combine(dat_ma -&gt; dat_ma[argmax(dat_ma.residual), :], groupby(DataFrames.transform(dat_ma, :datetime =&gt; x-&gt;year.(x)), :datetime_function))\ndelete!(dat_annmax, nrow(dat_annmax)) # delete 2023; haven't seen much of that year yet\nrename!(dat_annmax, :datetime_function =&gt; :Year)\nselect!(dat_annmax, [:Year, :residual])\n\n# make plots\np1 = plot(\n    dat_annmax.Year,\n    dat_annmax.residual;\n    xlabel=\"Year\",\n    ylabel=\"Annual Max Tide Level (m)\",\n    label=false,\n    marker=:circle,\n    markersize=5,\n    tickfontsize=16,\n    guidefontsize=18,\n    left_margin=5mm, \n    bottom_margin=5mm\n)\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Annual maxima surge data from the San Francisco, CA tide gauge."
  },
  {
    "objectID": "slides/lecture12-1.html#models-under-consideration",
    "href": "slides/lecture12-1.html#models-under-consideration",
    "title": "Model Assessment",
    "section": "Models Under Consideration",
    "text": "Models Under Consideration\nThree hypotheses (there could be many more!):\n\nStationary (“null”) model, \\(y_t \\sim \\text{GEV}(\\mu, \\sigma, \\xi);\\)\nTime nonstationary (“null-ish”) model, \\(y_t \\sim \\text{GEV}(\\mu_0 + \\mu_1 t, \\sigma, \\xi);\\)\nPDO nonstationary model, \\(y_t \\sim \\text{GEV}(\\mu_0 + \\mu_1 \\text{PDO}_t, \\sigma, \\xi)\\)"
  },
  {
    "objectID": "slides/lecture12-1.html#stationary-model",
    "href": "slides/lecture12-1.html#stationary-model",
    "title": "Model Assessment",
    "section": "Stationary Model",
    "text": "Stationary Model\n\n@model function sf_stat(y)\n    μ ~ truncated(Normal(1500, 200), lower=0)\n    σ ~ truncated(Normal(100, 25), lower=0)\n    ξ ~ Normal(0, 1)\n\n    for i in 1:length(y)\n        y[i] ~ GeneralizedExtremeValue(μ, σ, ξ)\n    end\nend\n\nstat_chain = sample(sf_stat(dat_annmax.residual), NUTS(), MCMCThreads(), 10_000, 4)\nsummarystats(stat_chain)"
  },
  {
    "objectID": "slides/lecture12-1.html#stationary-model-output",
    "href": "slides/lecture12-1.html#stationary-model-output",
    "title": "Model Assessment",
    "section": "Stationary Model",
    "text": "Stationary Model\n\n\nSummary Statistics\n  parameters        mean       std      mcse     ess_bulk     ess_tail      rh ⋯\n      Symbol     Float64   Float64   Float64      Float64      Float64   Float ⋯\n           μ   1259.4997    5.7712    0.0337   29339.3655   29203.0510    1.00 ⋯\n           σ     58.7998    4.3917    0.0288   23204.2945   25033.9178    1.00 ⋯\n           ξ      0.0223    0.0641    0.0004   22727.3890   21346.3533    1.00 ⋯\n                                                               2 columns omitted"
  },
  {
    "objectID": "slides/lecture12-1.html#nonstationary-time-model",
    "href": "slides/lecture12-1.html#nonstationary-time-model",
    "title": "Model Assessment",
    "section": "Nonstationary (Time) Model",
    "text": "Nonstationary (Time) Model\n\n@model function sf_nonstat(y)\n    a ~ truncated(Normal(1500, 200), lower=0)\n    b ~ Normal(0, 5)\n    σ ~ truncated(Normal(100, 25), lower=0)\n    ξ ~ Normal(0, 1)\n\n    T = length(y)\n    for i in 1:T\n        y[i] ~ GeneralizedExtremeValue.(a .+ b * i, σ, ξ)\n    end\nend\n\nnonstat_chain = sample(sf_nonstat(dat_annmax.residual), NUTS(), MCMCThreads(), 10_000, 4)\nsummarystats(nonstat_chain)"
  },
  {
    "objectID": "slides/lecture12-1.html#nonstationary-time-model-output",
    "href": "slides/lecture12-1.html#nonstationary-time-model-output",
    "title": "Model Assessment",
    "section": "Nonstationary (Time) Model",
    "text": "Nonstationary (Time) Model\n\n\nSummary Statistics\n  parameters        mean       std      mcse     ess_bulk     ess_tail      rh ⋯\n      Symbol     Float64   Float64   Float64      Float64      Float64   Float ⋯\n           a   1233.3188    9.8465    0.0717   18929.6497   21590.1106    1.00 ⋯\n           b      0.4049    0.1259    0.0009   19824.6827   22141.2461    1.00 ⋯\n           σ     55.0537    4.3812    0.0273   25640.0996   24242.6588    1.00 ⋯\n           ξ      0.0719    0.0755    0.0005   23415.4908   20872.0226    1.00 ⋯\n                                                               2 columns omitted"
  },
  {
    "objectID": "slides/lecture12-1.html#nonstationary-pdo-model",
    "href": "slides/lecture12-1.html#nonstationary-pdo-model",
    "title": "Model Assessment",
    "section": "Nonstationary (PDO) Model",
    "text": "Nonstationary (PDO) Model\n\n@model function sf_pdo(y, pdo)\n    a ~ truncated(Normal(1500, 200), lower=0)\n    b ~ Normal(0, 5)\n    σ ~ truncated(Normal(100, 25), lower=0)\n    ξ ~ Normal(0, 1)\n\n    for i in 1:length(y)\n        y[i] ~ GeneralizedExtremeValue.(a + b * pdo[i], σ, ξ)\n    end\nend\n\npdo_chain = sample(sf_pdo(dat_annmax.residual, pdo.PDO), NUTS(), MCMCThreads(), 10_000, 4)\nsummarystats(pdo_chain)"
  },
  {
    "objectID": "slides/lecture12-1.html#nonstationary-pdo-model-output",
    "href": "slides/lecture12-1.html#nonstationary-pdo-model-output",
    "title": "Model Assessment",
    "section": "Nonstationary (PDO) Model",
    "text": "Nonstationary (PDO) Model\n\n\nSummary Statistics\n  parameters        mean       std      mcse     ess_bulk     ess_tail      rh ⋯\n      Symbol     Float64   Float64   Float64      Float64      Float64   Float ⋯\n           a   1258.2872    5.7574    0.0332   30214.7749   29815.5208    1.00 ⋯\n           b     -5.2382    3.8411    0.0208   34129.3540   29277.3890    1.00 ⋯\n           σ     57.8907    4.3793    0.0271   26091.9447   25579.3171    1.00 ⋯\n           ξ      0.0318    0.0651    0.0004   25707.9710   23214.9304    1.00 ⋯\n                                                               2 columns omitted"
  },
  {
    "objectID": "slides/lecture12-1.html#model-simulations",
    "href": "slides/lecture12-1.html#model-simulations",
    "title": "Model Assessment",
    "section": "Model Simulations",
    "text": "Model Simulations\n\n\nCode\n# make predictions from each model\nstat_sim = predict(sf_stat(Vector{Union{Missing, Float64}}(undef, length(dat_annmax.residual))), stat_chain)\nnonstat_sim = predict(sf_nonstat(Vector{Union{Missing, Float64}}(undef, length(dat_annmax.residual))), nonstat_chain)\npdo_sim = predict(sf_pdo(Vector{Union{Missing, Float64}}(undef, length(dat_annmax.residual)), pdo.PDO), pdo_chain)\n\n# get quantiles\nstat_q = mapslices(x -&gt; quantile(x, [0.05, 0.5, 0.95]), stat_sim.value.data[:, :, 1], dims=1)\nnonstat_q = mapslices(x -&gt; quantile(x, [0.05, 0.5, 0.95]), nonstat_sim.value.data[:, :, 1], dims=1)\npdo_q = mapslices(x -&gt; quantile(x, [0.05, 0.5, 0.95]), pdo_sim.value.data[:, :, 1], dims=1)\n\n# plot\np = plot(; xlabel=\"Year\", ylabel=\"Storm Tide Annual Maximum (mm)\", left_margin=5mm, bottom_margin=5mm, right_margin=5mm, tickfontsize=16, guidefontsize=18, legendfontsize=16, legend=:outerright)\nplot!(p, dat_annmax.Year, stat_q[2, :], ribbon=(stat_q[2, :] - stat_q[1, :], stat_q[3, :] - stat_q[2, :]), color=:sienna, label=\"Stationary\", fillalpha=0.2, linewidth=3)\nplot!(p, dat_annmax.Year, nonstat_q[2, :], ribbon=(nonstat_q[2, :] - nonstat_q[1, :], nonstat_q[3, :] - nonstat_q[2, :]), color=:red, label=\"Nonstationary (Time)\", fillalpha=0.2, linewidth=3)\nplot!(p, dat_annmax.Year, pdo_q[2, :], ribbon=(pdo_q[2, :] - pdo_q[1, :], pdo_q[3, :] - pdo_q[2, :]), color=:teal, label=\"Nonstationary (PDO)\", fillalpha=0.2, linewidth=3)\nscatter!(p, dat_annmax.Year, dat_annmax.residual, color=:black, markersize=5, label=\"Observations\", alpha=0.5)\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Model simulations for the three storm surge models."
  },
  {
    "objectID": "slides/lecture12-1.html#point-estimates-vs.-posteriors",
    "href": "slides/lecture12-1.html#point-estimates-vs.-posteriors",
    "title": "Model Assessment",
    "section": "Point Estimates vs. Posteriors",
    "text": "Point Estimates vs. Posteriors\nCan calculate these metrics using some point estimate \\(\\hat{\\theta}\\) (MLE, MAP, etc):\n\\[\\underset{M_i \\in \\mathcal{M}}{\\operatorname{argmax}} p(y | \\hat{\\theta}, \\mathcal{M}_i)\\]\nor the posterior \\(\\Theta\\) (if this was found):\n\\[\\underset{M_i \\in \\mathcal{M}}{\\operatorname{argmax}} \\int_{\\theta \\in \\Theta} p(y | \\theta, M_i)\\]\nWhat do we think the differences are?"
  },
  {
    "objectID": "slides/lecture12-1.html#relative-evidence-for-models",
    "href": "slides/lecture12-1.html#relative-evidence-for-models",
    "title": "Model Assessment",
    "section": "Relative Evidence for Models",
    "text": "Relative Evidence for Models\nSimplest approach: What is the log-posterior probability of the data \\(\\log p(y | M_i)\\)?\nCan convert to a relative probability:\n\\[\\mathbb{P}(M_i | \\mathcal{M}) = \\frac{p(y | M_i)}{\\sum_j p(y | M_j)}\\]"
  },
  {
    "objectID": "slides/lecture12-1.html#relative-evidence-for-models-1",
    "href": "slides/lecture12-1.html#relative-evidence-for-models-1",
    "title": "Model Assessment",
    "section": "Relative Evidence for Models",
    "text": "Relative Evidence for Models\n\n\nCode\nmodel_evidence = DataFrame(Model=[\"Stationary\", \"Time\", \"PDO\"], LogPost=[mean(stat_chain[:lp]), mean(nonstat_chain[:lp]), mean(pdo_chain[:lp])])\nmodel_evidence.ModelProb = round.(exp.(model_evidence.LogPost .- log(sum(exp.(model_evidence.LogPost)))); digits=2)\nmodel_evidence.LogPost = round.(model_evidence.LogPost; digits=0)\nmodel_evidence\n\n\n\n3×3 DataFrame\n\n\n\nRow\nModel\nLogPost\nModelProb\n\n\n\nString\nFloat64\nFloat64\n\n\n\n\n1\nStationary\n-711.0\n0.16\n\n\n2\nTime\n-710.0\n0.82\n\n\n3\nPDO\n-714.0\n0.02"
  },
  {
    "objectID": "slides/lecture12-1.html#problems-with-explanatory-criteria",
    "href": "slides/lecture12-1.html#problems-with-explanatory-criteria",
    "title": "Model Assessment",
    "section": "Problems with Explanatory Criteria",
    "text": "Problems with Explanatory Criteria\n\n\nRisk of overfitting\nWhat if historical dynamics are similar, but out-of-sample predictions are distinct?\n\n\n\nThe alternative is predictive performance: how probable is unobserved data?"
  },
  {
    "objectID": "slides/lecture12-1.html#cross-validation",
    "href": "slides/lecture12-1.html#cross-validation",
    "title": "Model Assessment",
    "section": "Cross-Validation",
    "text": "Cross-Validation\nGold standard of predictive criteria: hold out data, fit model on remaining data, and see how well it performs.\n\nBut:\n\nWhat to do for data with structure, e.g. temporal or spatial data?\nAlso can be computationally expensive."
  },
  {
    "objectID": "slides/lecture12-1.html#posterior-predictive-distributions",
    "href": "slides/lecture12-1.html#posterior-predictive-distributions",
    "title": "Model Assessment",
    "section": "Posterior Predictive Distributions",
    "text": "Posterior Predictive Distributions\nNext class: will build up approximations to cross-validation. Idea: Consider a new realization \\(y^{\\text{rep}}\\) simulated from\n\\[p(y^{\\text{rep}} | y) = \\int_{\\theta} p(y^{\\text{rep}} | \\theta) p(\\theta | y) d\\theta.\\]\nCan sample:\n\\[p(\\theta | y) \\xrightarrow{\\hat{\\theta}} \\mathcal{M}(\\hat{\\theta}) \\rightarrow y^{\\text{rep}}\\]"
  },
  {
    "objectID": "slides/lecture12-1.html#key-points-1",
    "href": "slides/lecture12-1.html#key-points-1",
    "title": "Model Assessment",
    "section": "Key Points",
    "text": "Key Points\n\nSimulation lets us generalize hypothesis testing.\nAdvantage: Can look at probabilities of multiple candidate models, not just “rejecting” a null hypothesis.\nBayesian updating applies to these models: how does new data\nExplanatory vs. Predictive model evaluation."
  },
  {
    "objectID": "slides/lecture12-1.html#next-classes",
    "href": "slides/lecture12-1.html#next-classes",
    "title": "Model Assessment",
    "section": "Next Classes",
    "text": "Next Classes\nWednesday: Cross-Validation and Information Criteria"
  },
  {
    "objectID": "slides/lecture12-1.html#assessments",
    "href": "slides/lecture12-1.html#assessments",
    "title": "Model Assessment",
    "section": "Assessments",
    "text": "Assessments\nHomework 4: Released End of Week, Due 5/3 (Last One!)"
  },
  {
    "objectID": "slides/lecture01-1.html#about-me",
    "href": "slides/lecture01-1.html#about-me",
    "title": "Welcome to BEE 4850/5850!",
    "section": "About Me",
    "text": "About Me\nInstructor: Prof. Vivek Srikrishnan, viveks at cornell dot edu\nInterests:\n\nBridging Earth science, data science, and decision science to improve climate risk management;\nUnintended consequences which result from neglecting uncertainty or system dynamics."
  },
  {
    "objectID": "slides/lecture01-1.html#meet-my-supervisors",
    "href": "slides/lecture01-1.html#meet-my-supervisors",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Meet My Supervisors",
    "text": "Meet My Supervisors\n\n\n\n\nMy Supervisors"
  },
  {
    "objectID": "slides/lecture01-1.html#what-do-you-hope-to-get-out-of-this-course",
    "href": "slides/lecture01-1.html#what-do-you-hope-to-get-out-of-this-course",
    "title": "Welcome to BEE 4850/5850!",
    "section": "What Do You Hope To Get Out Of This Course?",
    "text": "What Do You Hope To Get Out Of This Course?\nTake a moment, write it down, and we’ll share!"
  },
  {
    "objectID": "slides/lecture01-1.html#why-does-data-analysis-matter",
    "href": "slides/lecture01-1.html#why-does-data-analysis-matter",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Why Does Data Analysis Matter?",
    "text": "Why Does Data Analysis Matter?\n\nScientific insight;\nDecision-making;\nUnderstanding uncertainty"
  },
  {
    "objectID": "slides/lecture01-1.html#the-ideal",
    "href": "slides/lecture01-1.html#the-ideal",
    "title": "Welcome to BEE 4850/5850!",
    "section": "The Ideal",
    "text": "The Ideal\n\n![]{https://imgs.xkcd.com/comics/statistics.png}\n\n\nSource: XKCD 2400"
  },
  {
    "objectID": "slides/lecture01-1.html#uniquechallenging-features-of-data",
    "href": "slides/lecture01-1.html#uniquechallenging-features-of-data",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Unique/Challenging Features Of Data",
    "text": "Unique/Challenging Features Of Data\nThere are many features of environmental (and biological!) data which make data analysis interesting and hard."
  },
  {
    "objectID": "slides/lecture01-1.html#extreme-events",
    "href": "slides/lecture01-1.html#extreme-events",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Extreme Events",
    "text": "Extreme Events\n\n\n\n\nSource: Doss-Gollin & Keller (2023)"
  },
  {
    "objectID": "slides/lecture01-1.html#extreme-events-1",
    "href": "slides/lecture01-1.html#extreme-events-1",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Extreme Events",
    "text": "Extreme Events\n\n\n\n\nSource: XKCD 2107"
  },
  {
    "objectID": "slides/lecture01-1.html#correlated-uncertainties",
    "href": "slides/lecture01-1.html#correlated-uncertainties",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Correlated Uncertainties",
    "text": "Correlated Uncertainties\n\n\n \n\n\nSource: Errickson et al. (2021)"
  },
  {
    "objectID": "slides/lecture01-1.html#non-stationarity",
    "href": "slides/lecture01-1.html#non-stationarity",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Non-Stationarity",
    "text": "Non-Stationarity\n\n\n\n\nSource: Fagnant et al. (2020)"
  },
  {
    "objectID": "slides/lecture01-1.html#forcing-structural-uncertainty",
    "href": "slides/lecture01-1.html#forcing-structural-uncertainty",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Forcing & Structural Uncertainty",
    "text": "Forcing & Structural Uncertainty\n\n\n\n\nSource: Doss-Gollin & Keller (2023)"
  },
  {
    "objectID": "slides/lecture01-1.html#deep-uncertainty",
    "href": "slides/lecture01-1.html#deep-uncertainty",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Deep Uncertainty",
    "text": "Deep Uncertainty\n\n\n\n\nSource: Srikrishnan et al. (2022)"
  },
  {
    "objectID": "slides/lecture01-1.html#modes-of-data-analysis",
    "href": "slides/lecture01-1.html#modes-of-data-analysis",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Modes of Data Analysis",
    "text": "Modes of Data Analysis"
  },
  {
    "objectID": "slides/lecture01-1.html#misspecification-can-bias-inferences",
    "href": "slides/lecture01-1.html#misspecification-can-bias-inferences",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Misspecification Can Bias Inferences…",
    "text": "Misspecification Can Bias Inferences…\n\n\n\n\n\n\n\nSource: Ruckert et al. (2017)"
  },
  {
    "objectID": "slides/lecture01-1.html#and-projections",
    "href": "slides/lecture01-1.html#and-projections",
    "title": "Welcome to BEE 4850/5850!",
    "section": "…And Projections",
    "text": "…And Projections\n\n\n\n\nSource: Ruckert et al. (2017)"
  },
  {
    "objectID": "slides/lecture01-1.html#some-problems-with-the-standard-data-analysis-toolkit",
    "href": "slides/lecture01-1.html#some-problems-with-the-standard-data-analysis-toolkit",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Some Problems With The “Standard” Data Analysis Toolkit",
    "text": "Some Problems With The “Standard” Data Analysis Toolkit\n\n\nStatistical assumptions may not be valid;\n“Null” vs “Alternative” hypotheses and tests may be chosen for computational convenience, not scientific relevance.\n\n\n\nImportant: “Big” data doesn’t solve the problem!"
  },
  {
    "objectID": "slides/lecture01-1.html#advantages-of-model-based-data-analysis",
    "href": "slides/lecture01-1.html#advantages-of-model-based-data-analysis",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Advantages of Model-Based Data Analysis",
    "text": "Advantages of Model-Based Data Analysis\nWe can:\n\nExamine logical implications of model assumptions.\nAssess evidence for multiple hypotheses by generating simulated data.\nIdentify opportunities to design future experiments or observations to distinguish between competing hypotheses."
  },
  {
    "objectID": "slides/lecture01-1.html#workflowcourse-organization",
    "href": "slides/lecture01-1.html#workflowcourse-organization",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Workflow/Course Organization",
    "text": "Workflow/Course Organization"
  },
  {
    "objectID": "slides/lecture01-1.html#background-knowledge-computing",
    "href": "slides/lecture01-1.html#background-knowledge-computing",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Background Knowledge: Computing",
    "text": "Background Knowledge: Computing\n\nBasics (at the level of CS 111x)\nSome extra work/effort may be needed if you haven’t coded in a while.\nMay need some additional familiarity with statistical packages (and “light” optimization)"
  },
  {
    "objectID": "slides/lecture01-1.html#background-knowledge-probabilitystatistics",
    "href": "slides/lecture01-1.html#background-knowledge-probabilitystatistics",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Background Knowledge: Probability/Statistics",
    "text": "Background Knowledge: Probability/Statistics\n\nENGRD 2700/CEE 3040\nSummary statistics of data\nProbability distributions\nBasic visualizations\nMonte Carlo basics"
  },
  {
    "objectID": "slides/lecture01-1.html#grades",
    "href": "slides/lecture01-1.html#grades",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Grades",
    "text": "Grades\n\n\n\nAssessment\n% of Grade\n\n\n\n\nExercises\n10%\n\n\nReadings\n10%\n\n\nLiterature Critique\n15%\n\n\nHomework Assignments\n30%\n\n\nTerm Project\n35%"
  },
  {
    "objectID": "slides/lecture01-1.html#overall-guidelines",
    "href": "slides/lecture01-1.html#overall-guidelines",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Overall Guidelines",
    "text": "Overall Guidelines\n\nCollaboration highly encouraged, but all work must reflect your own understanding\nSubmit PDFs on Gradescope\n50% penalty for late submission (up to 24 hours)\nStandard rubric available on website\nAlways cite external references"
  },
  {
    "objectID": "slides/lecture01-1.html#exercises",
    "href": "slides/lecture01-1.html#exercises",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Exercises",
    "text": "Exercises\n\n(Mostly) weekly problem sets\nFocus on conceptual material/small data analysis exercises\nWill drop one."
  },
  {
    "objectID": "slides/lecture01-1.html#readings",
    "href": "slides/lecture01-1.html#readings",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Readings",
    "text": "Readings\n\nSeveral readings assigned for discussion throughout the semester.\nOne student responsible for leading the discussion (Ed/in class)"
  },
  {
    "objectID": "slides/lecture01-1.html#literature-critique",
    "href": "slides/lecture01-1.html#literature-critique",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Literature Critique",
    "text": "Literature Critique\n\nSelect a peer-reviewed journal article which analyzes data;\nShort discussion paper analyzing:\n\nScientific hypotheses;\nModeling and statistical choices\n\nIn-class presentation before spring break\n5850 Students: Write a referee report"
  },
  {
    "objectID": "slides/lecture01-1.html#homework-assignments",
    "href": "slides/lecture01-1.html#homework-assignments",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Homework Assignments",
    "text": "Homework Assignments\n\nMore in-depth problems\nRoughly 2 weeks to complete\nWill not drop any by default\nRegrade requests must be made within one week\n5850 Students: Some extra problems"
  },
  {
    "objectID": "slides/lecture01-1.html#term-project",
    "href": "slides/lecture01-1.html#term-project",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Term Project",
    "text": "Term Project\n\nAnalyze a data set of interest using model(s) of your choice\nCan work individually or groups of 2\nSeveral deliverables throughout the semester\nFinal in-class presentation and report"
  },
  {
    "objectID": "slides/lecture01-1.html#attendance",
    "href": "slides/lecture01-1.html#attendance",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Attendance",
    "text": "Attendance\nNot required, but students tend to do better when they’re actively engaged in class."
  },
  {
    "objectID": "slides/lecture01-1.html#office-hours",
    "href": "slides/lecture01-1.html#office-hours",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Office Hours",
    "text": "Office Hours\n\nMW 10-11 AM, 318 Riley-Robb\nAlmost impossible to find a time that works for all (or even most); please feel free to make appointments as/if needed."
  },
  {
    "objectID": "slides/lecture01-1.html#accomodations",
    "href": "slides/lecture01-1.html#accomodations",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Accomodations",
    "text": "Accomodations\nIf you have any access barriers in this class, please seek out any helpful accomodations.\n\nGet an SDS letter.\nIf you need an accomodation before you have an official letter, please reach out to me ASAP!"
  },
  {
    "objectID": "slides/lecture01-1.html#academic-integrity",
    "href": "slides/lecture01-1.html#academic-integrity",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Academic Integrity",
    "text": "Academic Integrity\nHopefully not a concern…\n\nCollaboration is great and is encouraged!\nKnowing how to find and use helpful resources is a skill we want to develop.\nDon’t just copy…learn from others and give credit.\nSubmit your own original work."
  },
  {
    "objectID": "slides/lecture01-1.html#academic-integrity-1",
    "href": "slides/lecture01-1.html#academic-integrity-1",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Academic Integrity",
    "text": "Academic Integrity\nObviously, just copying down answers from Chegg or ChatGPT and passing them off as your own is not ok."
  },
  {
    "objectID": "slides/lecture01-1.html#chatgpt-the-stochastic-parrot",
    "href": "slides/lecture01-1.html#chatgpt-the-stochastic-parrot",
    "title": "Welcome to BEE 4850/5850!",
    "section": "ChatGPT: The Stochastic Parrot",
    "text": "ChatGPT: The Stochastic Parrot\nThink about ChatGPT as a drunk who tells stories for drinks.\nIt will give you plausible-looking text or code on any topic, but it doesn’t know anything beyond what it “overheard.”\n\n\n\n\n\n\nCaution\n\n\nChatGPT can be useful for certain tasks (e.g. understanding code errors), but may neglect context for why/when certain information or solutions work.\nJust think about it as an unreliable Google search."
  },
  {
    "objectID": "slides/lecture01-1.html#communications",
    "href": "slides/lecture01-1.html#communications",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Communications",
    "text": "Communications\nUse Ed Discussion for questions and discussions about class, homework assignments, etc.\n\nTry to use public posts so others can benefit from questions and can weigh in.\nI will make announcements through Ed."
  },
  {
    "objectID": "slides/lecture01-1.html#email",
    "href": "slides/lecture01-1.html#email",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Email",
    "text": "Email\nWhen urgency or privacy is required, email is ok.\n\n\n\n\n\n\nImportant\n\n\nPlease include BEE4850 in your email subject line! This will ensure it doesn’t get lost in the shuffle.\nBetter: Use Ed Discussion and reserve email for matters that are particular urgent and/or require privacy."
  },
  {
    "objectID": "slides/lecture01-1.html#course-website",
    "href": "slides/lecture01-1.html#course-website",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Course Website",
    "text": "Course Website\nhttps://viveks.me/simulation-data-analysis\n\nCentral hub for information, schedule, and policies\nWill add link and some information to Canvas (assignment due dates, etc)"
  },
  {
    "objectID": "slides/lecture01-1.html#computing-tools",
    "href": "slides/lecture01-1.html#computing-tools",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Computing Tools",
    "text": "Computing Tools\n\nCourse is programming language-agnostic.\nAssignments will have notebooks set up for Julia (environments, etc) on GitHub."
  },
  {
    "objectID": "slides/lecture01-1.html#some-tips-for-success",
    "href": "slides/lecture01-1.html#some-tips-for-success",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Some Tips For Success",
    "text": "Some Tips For Success\n\nStart the homeworks early; this gives time to sort out conceptual problems and debug.\nAsk questions (in class and online) and try to help each other.\nGive me feedback!"
  },
  {
    "objectID": "slides/lecture01-1.html#next-classes",
    "href": "slides/lecture01-1.html#next-classes",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Next Classes",
    "text": "Next Classes\nWednesday: Hypothesis testing and data analysis\nNext Week: Review of uncertainty and probability"
  },
  {
    "objectID": "slides/lecture01-1.html#assessments",
    "href": "slides/lecture01-1.html#assessments",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Assessments",
    "text": "Assessments\nHomework 1 available; due next Friday.\nExercise 1 due this Friday."
  },
  {
    "objectID": "slides/lecture01-1.html#references-scroll-for-full-list",
    "href": "slides/lecture01-1.html#references-scroll-for-full-list",
    "title": "Welcome to BEE 4850/5850!",
    "section": "References (Scroll for Full List)",
    "text": "References (Scroll for Full List)\n\n\n\n\n\n\n\n\nDoss-Gollin, J., & Keller, K. (2023). A subjective Bayesian framework for synthesizing deep uncertainties in climate risk management. Earths Future, 11, e2022EF003044. https://doi.org/10.1029/2022ef003044\n\n\nErrickson, F. C., Keller, K., Collins, W. D., Srikrishnan, V., & Anthoff, D. (2021). Equity is more important for the social cost of methane than climate uncertainty. Nature, 592, 564–570. https://doi.org/10.1038/s41586-021-03386-6\n\n\nFagnant, C., Gori, A., Sebastian, A., Bedient, P. B., & Ensor, K. B. (2020). Characterizing spatiotemporal trends in extreme precipitation in Southeast Texas. Nat. Hazards, 104, 1597–1621. https://doi.org/10.1007/s11069-020-04235-x\n\n\nRuckert, K. L., Guan, Y., Bakker, A. M. R., Forest, C. E., & Keller, K. (2017). The effects of time-varying observation errors on semi-empirical sea-level projections. Clim. Change, 140, 349–360. https://doi.org/10.1007/s10584-016-1858-z\n\n\nSrikrishnan, V., Guan, Y., Tol, R. S. J., & Keller, K. (2022). Probabilistic projections of baseline twenty-first century CO2 emissions using a simple calibrated integrated assessment model. Clim. Change, 170, 37. https://doi.org/10.1007/s10584-021-03279-7"
  },
  {
    "objectID": "slides/lecture01-2.html#modes-of-data-analysis",
    "href": "slides/lecture01-2.html#modes-of-data-analysis",
    "title": "Uncertainty and Probability Review",
    "section": "Modes of Data Analysis",
    "text": "Modes of Data Analysis"
  },
  {
    "objectID": "slides/lecture01-2.html#workflowcourse-organization",
    "href": "slides/lecture01-2.html#workflowcourse-organization",
    "title": "Uncertainty and Probability Review",
    "section": "Workflow/Course Organization",
    "text": "Workflow/Course Organization"
  },
  {
    "objectID": "slides/lecture01-2.html#questions",
    "href": "slides/lecture01-2.html#questions",
    "title": "Uncertainty and Probability Review",
    "section": "Questions?",
    "text": "Questions?\n\n\n\n\n\n\n\n\nText: VSRIKRISH to 22333\nURL: https://pollev.com/vsrikrish  See Results"
  },
  {
    "objectID": "slides/lecture01-2.html#what-is-uncertainty",
    "href": "slides/lecture01-2.html#what-is-uncertainty",
    "title": "Uncertainty and Probability Review",
    "section": "What Is Uncertainty?",
    "text": "What Is Uncertainty?\n\n\n\n…A departure from the (unachievable) ideal of complete determinism…\n\n\n— Walker et al. (2003)"
  },
  {
    "objectID": "slides/lecture01-2.html#types-of-uncertainty",
    "href": "slides/lecture01-2.html#types-of-uncertainty",
    "title": "Uncertainty and Probability Review",
    "section": "Types of Uncertainty",
    "text": "Types of Uncertainty\n\nAleatoric uncertainty: Uncertainties due to randomness/stochasticity;\nEpistemic uncertainty: Uncertainties due to lack of knowledge."
  },
  {
    "objectID": "slides/lecture01-2.html#data-relevant-uncertainty-taxonomy",
    "href": "slides/lecture01-2.html#data-relevant-uncertainty-taxonomy",
    "title": "Uncertainty and Probability Review",
    "section": "Data-Relevant Uncertainty Taxonomy",
    "text": "Data-Relevant Uncertainty Taxonomy\n\n\n\n\n\n\n\n\nUncertainty\nAssociated Uncertainties\nExamples\n\n\n\n\nStructural\nIncluded physical processes, mathematical form\nModel inadequacy, (epistemic) residual uncertainty\n\n\nParametric\nParameter uncertainty\nChoice of parameters, strength of coupling between models\n\n\nSampling\nNatural variability, (aleatoric) residual uncertainty\nInternal variability, uncertain boundary conditions"
  },
  {
    "objectID": "slides/lecture01-2.html#probability-distributions-1",
    "href": "slides/lecture01-2.html#probability-distributions-1",
    "title": "Uncertainty and Probability Review",
    "section": "Probability Distributions",
    "text": "Probability Distributions\nProbability distributions are often used to quantify uncertainty.\n\\[x \\to \\mathbb{P}_{\\color{green}\\nu}[x] = p_{\\color{green}\\nu}\\left(x | {\\color{purple}\\theta}\\right)\\]\n\n\\({\\color{green}\\nu}\\): probability distribution (often implicit);\n\\({\\color{purple}\\theta}\\): distribution parameters"
  },
  {
    "objectID": "slides/lecture01-2.html#sampling-notation",
    "href": "slides/lecture01-2.html#sampling-notation",
    "title": "Uncertainty and Probability Review",
    "section": "Sampling Notation",
    "text": "Sampling Notation\nTo write \\(x\\) is sampled from \\(p(x|\\theta)\\): \\[x \\sim f(\\theta)\\]\nFor example, for a normal distribution: \\[x \\sim \\mathcal{N}(\\mu, \\sigma)\\]"
  },
  {
    "objectID": "slides/lecture01-2.html#probability-density-function",
    "href": "slides/lecture01-2.html#probability-density-function",
    "title": "Uncertainty and Probability Review",
    "section": "Probability Density Function",
    "text": "Probability Density Function\nA continuous distribution \\(\\mathcal{D}\\) has a probability density function (PDF) \\(f_\\mathcal{D}(x) = p(x | \\theta)\\).\nThe probability of \\(x\\) occurring in an interval \\((a, b)\\) is \\[\\mathbb{P}[a \\leq x \\leq b] = \\int_a^b f_\\mathcal{D}(x)dx.\\]\n\n\n\n\n\n\nImportant\n\n\nThe probability that \\(x\\) has a specific value \\(x^*\\), \\(\\mathbb{P}(x = x^*)\\), is zero!"
  },
  {
    "objectID": "slides/lecture01-2.html#cumulative-density-functions",
    "href": "slides/lecture01-2.html#cumulative-density-functions",
    "title": "Uncertainty and Probability Review",
    "section": "Cumulative Density Functions",
    "text": "Cumulative Density Functions\nIf \\(\\mathcal{D}\\) is a distribution with PDF \\(f_\\mathcal{D}(x)\\), the cumulative density function (CDF) of \\(\\mathcal{D}\\) \\(F_\\mathcal{D}(x)\\):\n\\[F_\\mathcal{D}(x) = \\int_{-\\infty}^x f_\\mathcal{D}(u)du.\\]\nIf \\(f_\\mathcal{D}\\) is continuous at \\(x\\): \\[f_\\mathcal{D}(x) = \\frac{d}{dx}F_\\mathcal{D}(x).\\]"
  },
  {
    "objectID": "slides/lecture01-2.html#probability-mass-functions",
    "href": "slides/lecture01-2.html#probability-mass-functions",
    "title": "Uncertainty and Probability Review",
    "section": "Probability Mass Functions",
    "text": "Probability Mass Functions\nDiscrete distributions have probability mass functions (PMFs) which are defined at point values, e.g. \\(p(x = x^*) \\neq 0\\)."
  },
  {
    "objectID": "slides/lecture01-2.html#example-normal-distribution",
    "href": "slides/lecture01-2.html#example-normal-distribution",
    "title": "Uncertainty and Probability Review",
    "section": "Example: Normal Distribution",
    "text": "Example: Normal Distribution\n\\[f_\\mathcal{D}(x) = p(x | \\mu, \\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{1}{2}\\left(\\frac{x - \\mu}{\\sigma}^2\\right)\\right)\\]\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1"
  },
  {
    "objectID": "slides/lecture01-2.html#why-are-normal-distributions-so-commonly-used",
    "href": "slides/lecture01-2.html#why-are-normal-distributions-so-commonly-used",
    "title": "Uncertainty and Probability Review",
    "section": "Why Are Normal Distributions So Commonly Used?",
    "text": "Why Are Normal Distributions So Commonly Used?\n\nSymmetry/Unimodality\nLinearity\nCentral Limit Theorem"
  },
  {
    "objectID": "slides/lecture01-2.html#linearity",
    "href": "slides/lecture01-2.html#linearity",
    "title": "Uncertainty and Probability Review",
    "section": "Linearity",
    "text": "Linearity\n\nIf \\(X \\sim \\mathcal{N}(\\mu, \\sigma)\\): \\[\\bbox[yellow, 10px, border:5px solid red] {aX + b \\sim \\mathcal{N}\\left(a\\mu + b, |a|\\sigma\\right)}\\]\nIf \\(X_1 \\sim \\mathcal{N}(\\mu_1, \\sigma_1)\\), \\(X_2 \\sim \\mathcal{N}(\\mu_2, \\sigma_2)\\): \\[\\bbox[yellow, 5px, border:5px solid red] {X_1 + X_2 \\sim \\mathcal{N}\\left(\\mu_1 + \\mu_2, \\sqrt{\\sigma_1^2 + \\sigma_2^2}\\right)}\\]"
  },
  {
    "objectID": "slides/lecture01-2.html#central-limit-theorem-sampling-distributions",
    "href": "slides/lecture01-2.html#central-limit-theorem-sampling-distributions",
    "title": "Uncertainty and Probability Review",
    "section": "Central Limit Theorem: Sampling Distributions",
    "text": "Central Limit Theorem: Sampling Distributions\nThe sum or mean of a random sample is itself a random variable:\n\\[\\bar{X}_n = \\frac{1}{n}\\sum_{i=1}^n X_i \\sim \\mathcal{D}_n\\]\n\n\\(\\mathcal{D}_n\\): The sampling distribution of the mean (or sum, or other estimate of interest)."
  },
  {
    "objectID": "slides/lecture01-2.html#central-limit-theorem",
    "href": "slides/lecture01-2.html#central-limit-theorem",
    "title": "Uncertainty and Probability Review",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\nIf\n\n\\(\\mathbb{E}[X_i] = \\mu\\)\nand \\(\\text{Var}(X_i) = \\sigma^2 &lt; \\infty\\),\n\n\\[\\begin{align*}\n&\\bbox[yellow, 10px, border:5px solid red]\n{\\lim_{n \\to \\infty} \\sqrt{n}(\\bar{X}_n - \\mu ) = \\mathcal{N}(0, \\sigma^2)} \\\\\n\\Rightarrow &\\bbox[yellow, 10px, border:5px solid red] {\\bar{X}_n \\overset{\\text{approx}}{\\sim} \\mathcal{N}(\\mu, \\sigma^2/n)}\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/lecture01-2.html#central-limit-theorem-more-intuitive",
    "href": "slides/lecture01-2.html#central-limit-theorem-more-intuitive",
    "title": "Uncertainty and Probability Review",
    "section": "Central Limit Theorem (More Intuitive)",
    "text": "Central Limit Theorem (More Intuitive)\nFor a large enough set of samples, the sampling distribution of a sum or mean of random variables is approximately a normal distribution, even if the random variables themselves are not."
  },
  {
    "objectID": "slides/lecture01-2.html#why-are-normal-distributions-so-commonly-used-1",
    "href": "slides/lecture01-2.html#why-are-normal-distributions-so-commonly-used-1",
    "title": "Uncertainty and Probability Review",
    "section": "Why Are Normal Distributions So Commonly Used?",
    "text": "Why Are Normal Distributions So Commonly Used?\n\nCentral Limit Theorem: For a large enough dataset, can assume statistical quantities have an approximately normal distribution.\nLinearity/Other Mathematical Properties: Easy to work with/do calculations\n\n\nCan we think about when this might break down?"
  },
  {
    "objectID": "slides/lecture01-2.html#other-useful-distributions",
    "href": "slides/lecture01-2.html#other-useful-distributions",
    "title": "Uncertainty and Probability Review",
    "section": "Other Useful Distributions",
    "text": "Other Useful Distributions\n\nUniform: \\(\\text{Unif}(a, b)\\) (equal probability);\nPoisson: \\(\\text{Poisson}(\\lambda)\\) (count data);\nBernoulli: \\(\\text{Bernoulli}(p)\\) (coin flips);\nBinomial: \\(\\text{Binomial}(n, p)\\) (number of successes);\nCauchy: \\(\\text{Cauchy}(\\gamma)\\) (fat tails);\nGeneralized Extreme Value: \\(\\text{GEV}(\\mu, \\sigma, \\xi)\\) (maxima/minima)"
  },
  {
    "objectID": "slides/lecture01-2.html#what-is-probability",
    "href": "slides/lecture01-2.html#what-is-probability",
    "title": "Uncertainty and Probability Review",
    "section": "What Is Probability?",
    "text": "What Is Probability?\n\nHow we communicate/capture uncertainty depends on how we interpret probability:\n\nFrequentist: \\(\\mathbb{P}[A]\\) is the long-run frequency of event A occurring.\nBayesian: \\(\\mathbb{P}[A]\\) is the degree of belief (betting odds) of event A occurring."
  },
  {
    "objectID": "slides/lecture01-2.html#frequentist-probability",
    "href": "slides/lecture01-2.html#frequentist-probability",
    "title": "Uncertainty and Probability Review",
    "section": "Frequentist Probability",
    "text": "Frequentist Probability\n\n\nFrequentist:\n\nData are random, but there is a “true” parameter set for a given model.\nHow consistent are estimates for different data?\n\n\n\nBayesian:\n\nData and parameters are random;\nProbability of parameters and unobserved data as consistency with observations."
  },
  {
    "objectID": "slides/lecture01-2.html#confidence-intervals",
    "href": "slides/lecture01-2.html#confidence-intervals",
    "title": "Uncertainty and Probability Review",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\n\n\nFrequentist estimates have confidence intervals, which will contain the “true” parameter value for \\(\\alpha\\)% of data samples.\nNo guarantee that an individual CI contains the true value (with any “probability”)!\n\n\n\n\n\nSource: https://www.wikihow.com/Throw-a-Horseshoe"
  },
  {
    "objectID": "slides/lecture01-2.html#example-95-cis-for-n0.4-2",
    "href": "slides/lecture01-2.html#example-95-cis-for-n0.4-2",
    "title": "Uncertainty and Probability Review",
    "section": "Example: 95% CIs for N(0.4, 2)",
    "text": "Example: 95% CIs for N(0.4, 2)\n\nCode\n# set up distribution\nmean_true = 0.4\nn_cis = 100 # number of CIs to compute\ndist = Normal(mean_true, 2)\n\n# use sample size of 100\nsamples = rand(dist, (100, n_cis))\n# mapslices broadcasts over a matrix dimension, could also use a loop\nsample_means = mapslices(mean, samples; dims=1)\nsample_sd = mapslices(std, samples; dims=1) \nmc_sd = 1.96 * sample_sd / sqrt(100)\nmc_ci = zeros(n_cis, 2) # preallocate\nfor i = 1:n_cis\n    mc_ci[i, 1] = sample_means[i] - mc_sd[i]\n    mc_ci[i, 2] = sample_means[i] + mc_sd[i]\nend\n# find which CIs contain the true value\nci_true = (mc_ci[:, 1] .&lt; mean_true) .&& (mc_ci[:, 2] .&gt; mean_true)\n# compute percentage of CIs which contain the true value\nci_frac1 = 100 * sum(ci_true) ./ n_cis\n\n# plot CIs\np1 = plot([mc_ci[1, :]], [1, 1], linewidth=3, color=:blue, label=\"95% Confidence Interval\", title=\"Sample Size 100\", yticks=:false, tickfontsize=14, titlefontsize=20, legend=:false, guidefontsize=16)\nfor i = 2:n_cis\n    if ci_true[i]\n        plot!(p1, [mc_ci[i, :]], [i, i], linewidth=2, color=:blue, label=:false)\n    else\n        plot!(p1, [mc_ci[i, :]], [i, i], linewidth=2, color=:red, label=:false)\n    end\nend\nvline!(p1, [mean_true], color=:black, linewidth=2, linestyle=:dash, label=\"True Value\") # plot true value as a vertical line\nxaxis!(p1, \"Estimate\")\nplot!(p1, size=(500, 400)) # resize to fit slide\n\n# use sample size of 1000\nsamples = rand(dist, (1000, n_cis))\n# mapslices broadcasts over a matrix dimension, could also use a loop\nsample_means = mapslices(mean, samples; dims=1)\nsample_sd = mapslices(std, samples; dims=1) \nmc_sd = 1.96 * sample_sd / sqrt(1000)\nmc_ci = zeros(n_cis, 2) # preallocate\nfor i = 1:n_cis\n    mc_ci[i, 1] = sample_means[i] - mc_sd[i]\n    mc_ci[i, 2] = sample_means[i] + mc_sd[i]\nend\n# find which CIs contain the true value\nci_true = (mc_ci[:, 1] .&lt; mean_true) .&& (mc_ci[:, 2] .&gt; mean_true)\n# compute percentage of CIs which contain the true value\nci_frac2 = 100 * sum(ci_true) ./ n_cis\n\n# plot CIs\np2 = plot([mc_ci[1, :]], [1, 1], linewidth=3, color=:blue, label=\"95% Confidence Interval\", title=\"Sample Size 1,000\", yticks=:false, tickfontsize=14, titlefontsize=20, legend=:false, guidefontsize=16)\nfor i = 2:n_cis\n    if ci_true[i]\n        plot!(p2, [mc_ci[i, :]], [i, i], linewidth=2, color=:blue, label=:false)\n    else\n        plot!(p2, [mc_ci[i, :]], [i, i], linewidth=2, color=:red, label=:false)\n    end\nend\nvline!(p2, [mean_true], color=:black, linewidth=2, linestyle=:dash, label=\"True Value\") # plot true value as a vertical line\nxaxis!(p2, \"Estimate\")\nplot!(p2, size=(500, 400)) # resize to fit slide\n\ndisplay(p1)\ndisplay(p2)\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Sample Size 100\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Sample Size 1,000\n\n\n\n\n\n\n\nFigure 2: Display of 95% confidence intervals\n\n\n\n90% of the CIs contain the true value (left) vs. 94% (right)"
  },
  {
    "objectID": "slides/lecture01-2.html#correlations",
    "href": "slides/lecture01-2.html#correlations",
    "title": "Uncertainty and Probability Review",
    "section": "Correlations",
    "text": "Correlations\nCorrelation refers to whether two variables increase or decrease simultaneously.\nTypically measured with Pearson’s coefficient:\n\\[r = \\frac{\\text{Cov}(X, Y)}{\\sigma_X \\sigma_Y} \\in (-1, 1)\\]"
  },
  {
    "objectID": "slides/lecture01-2.html#correlation-examples",
    "href": "slides/lecture01-2.html#correlation-examples",
    "title": "Uncertainty and Probability Review",
    "section": "Correlation Examples",
    "text": "Correlation Examples\n\nCode\n# sample 1000 independent variables from a given normal distribution\nsample_independent = rand(Normal(0, 1), (2, 1000))\np1 = scatter(sample_independent[1, :], sample_independent[2, :], label=:false, title=\"Independent Variables\", tickfontsize=14, titlefontsize=18, guidefontsize=18)\nxlabel!(p1, L\"$x_1$\")\nylabel!(p1, L\"$x_2$\")\nplot!(p1, size=(400, 500))\n\n# sample 1000 correlated variables, with r=0.7\nsample_correlated = rand(MvNormal([0; 0], [1 0.7; 0.7 1]), 1000)\np2 = scatter(sample_correlated[1, :], sample_correlated[2, :], label=:false, title=L\"Correlated ($r=0.7$)\", tickfontsize=14, titlefontsize=18, guidefontsize=18)\nxlabel!(p2, L\"$x_1$\")\nylabel!(p2, L\"$x_2$\")\nplot!(p2, size=(400, 500))\n\n# sample 1000 anti-correlated variables, with r=-0.7\nsample_anticorrelated = rand(MvNormal([0; 0], [1 -0.7; -0.7 1]), 1000)\np3 = scatter(sample_anticorrelated[1, :], sample_anticorrelated[2, :], label=:false, title=L\"Anticorrelated ($r=-0.7$)\", tickfontsize=14, titlefontsize=18, guidefontsize=18)\nxlabel!(p3, L\"$x_1$\")\nylabel!(p3, L\"$x_2$\")\nplot!(p3, size=(400, 500))\n\ndisplay(p1)\ndisplay(p2)\ndisplay(p3)\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Independent Variables\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Correlated Variables (\\(r=0.7\\))\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Anti-Correlated Variables (\\(r=-0.7\\))\n\n\n\n\n\n\n\nFigure 3: Independent vs. Correlated Normal Variables"
  },
  {
    "objectID": "slides/lecture01-2.html#correlation-and-climate-models",
    "href": "slides/lecture01-2.html#correlation-and-climate-models",
    "title": "Uncertainty and Probability Review",
    "section": "Correlation and Climate Models",
    "text": "Correlation and Climate Models\n\n\n \n\n\nSource: Errickson et al. (2021)"
  },
  {
    "objectID": "slides/lecture01-2.html#autocorrelation",
    "href": "slides/lecture01-2.html#autocorrelation",
    "title": "Uncertainty and Probability Review",
    "section": "Autocorrelation",
    "text": "Autocorrelation\nTime series can also be auto-correlated, called an autoregressive model:\n\\[y_t = \\sum_{i=1}^{t-1} \\rho_i y_{t-i} + \\varepsilon_t\\]\n\nExample: A time series is autocorrelated with lag 1 (called an AR(1) model) if \\(y_t = \\rho y_{t-1} + \\varepsilon_t\\)."
  },
  {
    "objectID": "slides/lecture01-2.html#key-points",
    "href": "slides/lecture01-2.html#key-points",
    "title": "Uncertainty and Probability Review",
    "section": "Key Points",
    "text": "Key Points\n\nDifferent model-relevant uncertainties (will matter later!)\nReviewed probability distribution basics\n\nMany different distributions, suitable for different purposes\nProbability density functions vs. cumulative density functions"
  },
  {
    "objectID": "slides/lecture01-2.html#key-points-1",
    "href": "slides/lecture01-2.html#key-points-1",
    "title": "Uncertainty and Probability Review",
    "section": "Key Points",
    "text": "Key Points\n\nFrequentist vs. Bayesian probability (matters a bit later)\n\nFrequentist: parameters as fixed (trying to recover with enough experiments)\nBayesian: parameters as random (probability reflects degree of consistency with observations)\nIn both cases data are random!"
  },
  {
    "objectID": "slides/lecture01-2.html#key-points-2",
    "href": "slides/lecture01-2.html#key-points-2",
    "title": "Uncertainty and Probability Review",
    "section": "Key Points",
    "text": "Key Points\n\nConfidence Intervals:\n\n\\(\\alpha\\)% of \\(\\alpha\\)-CIs generated from different samples will contain the “true” parameter value\nDo not say anything about probability of including true value!"
  },
  {
    "objectID": "slides/lecture01-2.html#key-points-3",
    "href": "slides/lecture01-2.html#key-points-3",
    "title": "Uncertainty and Probability Review",
    "section": "Key Points",
    "text": "Key Points\n\nIndependence vs. Correlations\n\nDo two variables increase/decrease together/in opposition or are they unrelated?\nCan be very important scientifically!"
  },
  {
    "objectID": "slides/lecture01-2.html#references",
    "href": "slides/lecture01-2.html#references",
    "title": "Uncertainty and Probability Review",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n\n\nErrickson, F. C., Keller, K., Collins, W. D., Srikrishnan, V., & Anthoff, D. (2021). Equity is more important for the social cost of methane than climate uncertainty. Nature, 592, 564–570. https://doi.org/10.1038/s41586-021-03386-6\n\n\nWalker, W. E., Harremoës, P., Rotmans, J., Sluijs, J. P. van der, Asselt, M. B. A. van, Janssen, P., & Krayer von Krauss, M. P. (2003). Defining uncertainty: A conceptual basis for uncertainty management in model-based decision support. Integrated Assessment, 4, 5–17. https://doi.org/10.1076/iaij.4.1.5.16466"
  },
  {
    "objectID": "slides/lecture07-1.html#probability-models",
    "href": "slides/lecture07-1.html#probability-models",
    "title": "Monte Carlo Simulation",
    "section": "Probability Models",
    "text": "Probability Models\n\nProbability Models for Data-Generating Processes\nCan be used for statistical models and simulation models (discrepancy)"
  },
  {
    "objectID": "slides/lecture07-1.html#whats-next",
    "href": "slides/lecture07-1.html#whats-next",
    "title": "Monte Carlo Simulation",
    "section": "What’s Next?",
    "text": "What’s Next?\n\nHow do we:\n\nSimulate from models/propagate uncertainty;\nGo beyond MLE/MAP estimation for parameter values?\n\nModel assessment/selection"
  },
  {
    "objectID": "slides/lecture07-1.html#stochastic-simulation",
    "href": "slides/lecture07-1.html#stochastic-simulation",
    "title": "Monte Carlo Simulation",
    "section": "Stochastic Simulation",
    "text": "Stochastic Simulation\nGoal: Estimate \\(\\mathbb{E}_p\\left[f(x)\\right]\\), \\(x \\sim p(x)\\)\n\n\n\nStandard approach: Compute \\(\\mathbb{E}_p\\left[f(x)\\right] = \\int_X f(x)p(x)dx\\)\nMonte Carlo:\n\nSample \\(x^1, x^2, \\ldots, x^N \\sim p(x)\\)\nEstimate \\(\\mathbb{E}_p\\left[f(x)\\right] \\approx \\sum_{n=1}^N f(x^n)\\) / N"
  },
  {
    "objectID": "slides/lecture07-1.html#monte-carlo-process-schematic",
    "href": "slides/lecture07-1.html#monte-carlo-process-schematic",
    "title": "Monte Carlo Simulation",
    "section": "Monte Carlo Process Schematic",
    "text": "Monte Carlo Process Schematic\n\n\n\n\n\n\n\nG\n\n\n\na\n\nProbability\n Distribution\n\n\n\nb\n\nRandom\n Samples\n\n\n\na-&gt;b\n\n\nSample\n\n\n\nc\n\nModel\n\n\n\nb-&gt;c\n\n\nInput\n\n\n\nd\n\nOutputs\n\n\n\nc-&gt;d\n\n\nSimulate"
  },
  {
    "objectID": "slides/lecture07-1.html#goals-of-monte-carlo",
    "href": "slides/lecture07-1.html#goals-of-monte-carlo",
    "title": "Monte Carlo Simulation",
    "section": "Goals of Monte Carlo",
    "text": "Goals of Monte Carlo\nMonte Carlo is a broad method, which can be used to:\n\nObtain probability distributions of outputs;\nEstimate deterministic quantities (Monte Carlo estimation)."
  },
  {
    "objectID": "slides/lecture07-1.html#mc-example-finding-pi",
    "href": "slides/lecture07-1.html#mc-example-finding-pi",
    "title": "Monte Carlo Simulation",
    "section": "MC Example: Finding \\(\\pi\\)",
    "text": "MC Example: Finding \\(\\pi\\)\nHow can we use MC to estimate \\(\\pi\\)?\nHint: Think of \\(\\pi\\) as an expected value…"
  },
  {
    "objectID": "slides/lecture07-1.html#mc-example-finding-pi-1",
    "href": "slides/lecture07-1.html#mc-example-finding-pi-1",
    "title": "Monte Carlo Simulation",
    "section": "MC Example: Finding \\(\\pi\\)",
    "text": "MC Example: Finding \\(\\pi\\)\n\n\n\nFinding \\(\\pi\\) by sampling random values from the unit square and computing the fraction in the unit circle. This is an example of Monte Carlo integration.\n\\[\\frac{\\text{Area of Circle}}{\\text{Area of Square}} = \\frac{\\pi}{4}\\]\n\n\n\n\n\nCode\nfunction circleShape(r)\n    θ = LinRange(0, 2 * π, 500)\n    r * sin.(θ), r * cos.(θ)\nend\n\nnsamp = 3000\nunif = Uniform(-1, 1)\nx = rand(unif, (nsamp, 2))\nl = mapslices(v -&gt; sum(v.^2), x, dims=2)\nin_circ = l .&lt; 1\npi_est = [4 * mean(in_circ[1:i]) for i in 1:nsamp]\n\nplt1 = plot(\n    1,\n    xlim = (-1, 1),\n    ylim = (-1, 1),\n    legend = false,\n    markersize = 4,\n    framestyle = :origin,\n    tickfontsize=16,\n    grid=:false\n    )\nplt2 = plot(\n    1,\n    xlim = (1, nsamp),\n    ylim = (3, 3.5),\n    legend = :false,\n    linewidth=3, \n    color=:black,\n    tickfontsize=16,\n    guidefontsize=16,\n    xlabel=\"Iteration\",\n    ylabel=\"Estimate\",\n    right_margin=5mm\n)\nhline!(plt2, [π], color=:red, linestyle=:dash)\nplt = plot(plt1, plt2, layout=Plots.grid(2, 1, heights=[2/3, 1/3]), size=(600, 500))\n\nplot!(plt, circleShape(1), linecolor=:blue, lw=1, aspectratio=1, subplot=1)\n\n\nmc_anim = @animate for i = 1:nsamp\n    if l[i] &lt; 1\n        scatter!(plt[1], Tuple(x[i, :]), color=:blue, markershape=:x, subplot=1)\n    else\n        scatter!(plt[1], Tuple(x[i, :]), color=:red, markershape=:x, subplot=1)\n    end\n    push!(plt, 2, i, pi_est[i])\nend every 100\n\ngif(mc_anim, \"figures/mc_pi.gif\", fps=3)\n\n\n\n\n\n\n\nFigure 1: MCMC Estimation of pi"
  },
  {
    "objectID": "slides/lecture07-1.html#mc-example-dice",
    "href": "slides/lecture07-1.html#mc-example-dice",
    "title": "Monte Carlo Simulation",
    "section": "MC Example: Dice",
    "text": "MC Example: Dice\n\n\nWhat is the probability of rolling 4 dice for a total of 19?\n\nCan simulate dice rolls and find the frequency of 19s among the samples.\n\n\n\n\n\nCode\nfunction dice_roll_repeated(n_trials, n_dice)\n    dice_dist = DiscreteUniform(1, 6) \n    roll_results = zeros(n_trials)\n    for i=1:n_trials\n        roll_results[i] = sum(rand(dice_dist, n_dice))\n    end\n    return roll_results\nend\n\nnsamp = 10000\n# roll four dice 10000 times\nrolls = dice_roll_repeated(nsamp, 4) \n\n# calculate probability of 19\nsum(rolls .== 19) / length(rolls)\n\n# initialize storage for frequencies by sample length\navg_freq = zeros(length(rolls)) \nstd_freq = zeros(length(rolls)) \n\n# compute average frequencies of 19\navg_freq[1] = (rolls[1] == 19)\ncount = 1\nfor i=2:length(rolls)\n    avg_freq[i] = (avg_freq[i-1] * (i-1) + (rolls[i] == 19)) / i\n    std_freq[i] = 1/sqrt(i-1) * std(rolls[1:i] .== 19)\nend\n\nplt = plot(\n    1,\n    xlim = (1, nsamp),\n    ylim = (0, 0.1),\n    legend = :false,\n    tickfontsize=16,\n    guidefontsize=16,\n    xlabel=\"Iteration\",\n    ylabel=\"Estimate\",\n    right_margin=8mm,\n    color=:black,\n    linewidth=3,\n    size=(600, 400)\n)\nhline!(plt, [0.0432], color=\"red\", \n    linestyle=:dash) \n\nmc_anim = @animate for i = 1:nsamp\n    push!(plt, 1, i, avg_freq[i])\nend every 100\n\ngif(mc_anim, \"figures/mc_dice.gif\", fps=10)\n\n\n\n\n\n\n\nFigure 2: MCMC Estimation of pi"
  },
  {
    "objectID": "slides/lecture07-1.html#monte-carlo-and-uncertainty-propagation",
    "href": "slides/lecture07-1.html#monte-carlo-and-uncertainty-propagation",
    "title": "Monte Carlo Simulation",
    "section": "Monte Carlo and Uncertainty Propagation",
    "text": "Monte Carlo and Uncertainty Propagation\nMonte Carlo simulation: propagate uncertainties from inputs through a model to outputs.\nThis is an example of uncertainty propagation: draw samples from some distribution, and run them through one or more models to find the (conditional) probability of outcomes of interest (for good or bad)."
  },
  {
    "objectID": "slides/lecture07-1.html#uncertainty-propagation-flowchart",
    "href": "slides/lecture07-1.html#uncertainty-propagation-flowchart",
    "title": "Monte Carlo Simulation",
    "section": "Uncertainty Propagation Flowchart",
    "text": "Uncertainty Propagation Flowchart\n\n\nFor example: What is the probability that a levee will be overtopped given climate and extreme sea-level uncertainty?"
  },
  {
    "objectID": "slides/lecture07-1.html#what-do-we-need-for-mc",
    "href": "slides/lecture07-1.html#what-do-we-need-for-mc",
    "title": "Monte Carlo Simulation",
    "section": "What Do We Need For MC?",
    "text": "What Do We Need For MC?\n\n\nSimulation Model (Numerical/Statistical)\nInput Distributions"
  },
  {
    "objectID": "slides/lecture07-1.html#on-random-number-generators",
    "href": "slides/lecture07-1.html#on-random-number-generators",
    "title": "Monte Carlo Simulation",
    "section": "On Random Number Generators",
    "text": "On Random Number Generators\n\n\nRandom number generators are not really random, only pseudorandom.\nThis is why setting a seed is important. But even that can go wrong…\n\n\n\n\nXKCD Cartoon 221: Random Number\n\n\n\nSource: XKCD #221"
  },
  {
    "objectID": "slides/lecture07-1.html#monte-carlo-formal-approach",
    "href": "slides/lecture07-1.html#monte-carlo-formal-approach",
    "title": "Monte Carlo Simulation",
    "section": "Monte Carlo: Formal Approach",
    "text": "Monte Carlo: Formal Approach\nFormally: Monte Carlo estimation as the computation of the expected value of a random quantity \\(Y\\), \\(\\mu = \\mathbb{E}[Y]\\).\nTo do this, generate \\(n\\) independent and identically distributed values \\(Y_1, \\ldots, Y_n\\). Then the sample estimate is\n\\[\\tilde{\\mu}_n = \\frac{1}{n}\\sum_{i=1}^n Y_i\\]"
  },
  {
    "objectID": "slides/lecture07-1.html#the-law-of-large-numbers",
    "href": "slides/lecture07-1.html#the-law-of-large-numbers",
    "title": "Monte Carlo Simulation",
    "section": "The Law of Large Numbers",
    "text": "The Law of Large Numbers\nIf\n\n\\(Y\\) is a random variable and its expectation exists and\n\\(Y_1, \\ldots, Y_n\\) are independently and identically distributed\n\nThen by the weak law of large numbers:\n\\[\\lim_{n \\to \\infty} \\mathbb{P}\\left(\\left|\\tilde{\\mu}_n - \\mu\\right| \\leq \\varepsilon \\right) = 1\\]"
  },
  {
    "objectID": "slides/lecture07-1.html#the-law-of-large-numbers-1",
    "href": "slides/lecture07-1.html#the-law-of-large-numbers-1",
    "title": "Monte Carlo Simulation",
    "section": "The Law of Large Numbers",
    "text": "The Law of Large Numbers\nIn other words, eventually Monte Carlo estimates will get within an arbitrary error of the true expectation.\nBut how large is large enough?"
  },
  {
    "objectID": "slides/lecture07-1.html#monte-carlo-sample-mean",
    "href": "slides/lecture07-1.html#monte-carlo-sample-mean",
    "title": "Monte Carlo Simulation",
    "section": "Monte Carlo Sample Mean",
    "text": "Monte Carlo Sample Mean\nThe sample mean \\(\\tilde{\\mu}_n = \\frac{1}{n}\\sum_{i=1}^n Y_i\\) is itself a random variable.\n\nWith some assumptions (the mean of \\(Y\\) exists and \\(Y\\) has finite variance), the expected Monte Carlo sample mean \\(\\mathbb{E}[\\tilde{\\mu}_n]\\) is\n\\[\\frac{1}{n}\\sum_{i=1}^n \\mathbb{E}[Y_i] = \\frac{1}{n} n \\mu = \\mu\\]"
  },
  {
    "objectID": "slides/lecture07-1.html#monte-carlo-error",
    "href": "slides/lecture07-1.html#monte-carlo-error",
    "title": "Monte Carlo Simulation",
    "section": "Monte Carlo Error",
    "text": "Monte Carlo Error\nWe’d like to know more about the error of this estimate for a given sample size. The variance of this estimator is\n\\[\\tilde{\\sigma}_n^2 = \\text{Var}\\left(\\tilde{\\mu}_n\\right) = \\mathbb{E}\\left((\\tilde{\\mu}_n - \\mu)^2\\right) = \\frac{\\sigma_Y^2}{n}\\]\n\nSo as \\(n\\) increases, the standard error decreases:\n\\[\\tilde{\\sigma}_n = \\frac{\\sigma_Y}{\\sqrt{n}}\\]"
  },
  {
    "objectID": "slides/lecture07-1.html#monte-carlo-error-1",
    "href": "slides/lecture07-1.html#monte-carlo-error-1",
    "title": "Monte Carlo Simulation",
    "section": "Monte Carlo Error",
    "text": "Monte Carlo Error\nIn other words, if we want to decrease the Monte Carlo error by 10x, we need 100x additional samples. This is not an ideal method for high levels of accuracy.\n\n\n\nMonte Carlo is an extremely bad method. It should only be used when all alternative methods are worse.\n\n\n— Sokal, Monte Carlo Methods in Statistical Mechanics, 1996\n\n\n\n\nBut…often most alternatives are worse!"
  },
  {
    "objectID": "slides/lecture07-1.html#when-might-we-want-to-use-monte-carlo",
    "href": "slides/lecture07-1.html#when-might-we-want-to-use-monte-carlo",
    "title": "Monte Carlo Simulation",
    "section": "When Might We Want to Use Monte Carlo?",
    "text": "When Might We Want to Use Monte Carlo?\nIf you can compute your answers analytically or through quadrature, you probably should.\nBut for many “real” problems, this is either\n\nNot possible (or computationally intractable);\nRequires a lot of stylization and simplification."
  },
  {
    "objectID": "slides/lecture07-1.html#monte-carlo-confidence-intervals",
    "href": "slides/lecture07-1.html#monte-carlo-confidence-intervals",
    "title": "Monte Carlo Simulation",
    "section": "Monte Carlo Confidence Intervals",
    "text": "Monte Carlo Confidence Intervals\nBasic Idea: The Central Limit Theorem says that with enough samples, the errors are normally distributed:\n\\[\\left\\|\\tilde{\\mu}_n - \\mu\\right\\| \\to \\mathcal{N}\\left(0, \\frac{\\sigma_Y^2}{n}\\right)\\]"
  },
  {
    "objectID": "slides/lecture07-1.html#monte-carlo-confidence-intervals-1",
    "href": "slides/lecture07-1.html#monte-carlo-confidence-intervals-1",
    "title": "Monte Carlo Simulation",
    "section": "Monte Carlo Confidence Intervals",
    "text": "Monte Carlo Confidence Intervals\nThe \\(\\alpha\\)-confidence interval is: \\[\\tilde{\\mu}_n \\pm \\Phi^{-1}\\left(1 - \\frac{\\alpha}{2}\\right) \\frac{\\sigma_Y}{\\sqrt{n}}\\]\nFor example, the 95% confidence interval is \\[\\tilde{\\mu}_n \\pm 1.96 \\frac{\\sigma_Y}{\\sqrt{n}}.\\]"
  },
  {
    "objectID": "slides/lecture07-1.html#sidebar-estimating-sigma_y",
    "href": "slides/lecture07-1.html#sidebar-estimating-sigma_y",
    "title": "Monte Carlo Simulation",
    "section": "Sidebar: Estimating \\(\\sigma_Y\\)",
    "text": "Sidebar: Estimating \\(\\sigma_Y\\)\nWe don’t know the standard deviation \\(\\sigma_Y\\).\nBut we can estimate it using the simulation standard deviation:"
  },
  {
    "objectID": "slides/lecture07-1.html#implications-of-monte-carlo-error",
    "href": "slides/lecture07-1.html#implications-of-monte-carlo-error",
    "title": "Monte Carlo Simulation",
    "section": "Implications of Monte Carlo Error",
    "text": "Implications of Monte Carlo Error\nConverging at a rate of \\(1/\\sqrt{n}\\) is not great. But:\n\nAll models are wrong, and so there always exists some irreducible model error.\nWe often need a lot of simulations. Do we have enough computational power?"
  },
  {
    "objectID": "slides/lecture07-1.html#key-points",
    "href": "slides/lecture07-1.html#key-points",
    "title": "Monte Carlo Simulation",
    "section": "Key Points",
    "text": "Key Points\n\n\nMonte Carlo: estimate by simulating summary statistics\nInstead of computing integrals, approximate through sample summaries\nMC estimates are themselves random quantities\nConfidence intervals obtained through Central Limit Theorem\n\\(\\tilde{\\mu}_n \\to \\mu\\) at rate \\(1/\\sqrt{n}\\): not great!"
  },
  {
    "objectID": "slides/lecture07-1.html#next-classes",
    "href": "slides/lecture07-1.html#next-classes",
    "title": "Monte Carlo Simulation",
    "section": "Next Classes",
    "text": "Next Classes\nWednesday: Monte Carlo Examples: Flood Risk and Climate Change\nNext Week: The Bootstrap"
  },
  {
    "objectID": "slides/lecture07-1.html#assessments",
    "href": "slides/lecture07-1.html#assessments",
    "title": "Monte Carlo Simulation",
    "section": "Assessments",
    "text": "Assessments\nExercise 7: Assigned, due Friday\nReading: Kale et al. (2021)"
  },
  {
    "objectID": "slides/lecture07-1.html#references-1",
    "href": "slides/lecture07-1.html#references-1",
    "title": "Monte Carlo Simulation",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n\n\nKale, A., Kay, M., & Hullman, J. (2021). Visual reasoning strategies for effect size judgments and decisions. IEEE Trans. Vis. Comput. Graph., 27, 272–282. https://doi.org/10.1109/TVCG.2020.3030335"
  },
  {
    "objectID": "slides/lecture07-2.html#monte-carlo",
    "href": "slides/lecture07-2.html#monte-carlo",
    "title": "Monte Carlo Application and the Bootstrap",
    "section": "Monte Carlo",
    "text": "Monte Carlo\n\nStochastic Simulation\nEstimate \\[\\mu = \\mathbb{E}_p[f(x)] \\approx \\frac{1}{N} \\sum_{i=1}^n f(x_i) = \\hat{\\mu}_n\\]"
  },
  {
    "objectID": "slides/lecture07-2.html#monte-carlo-error-analysis",
    "href": "slides/lecture07-2.html#monte-carlo-error-analysis",
    "title": "Monte Carlo Application and the Bootstrap",
    "section": "Monte Carlo Error Analysis",
    "text": "Monte Carlo Error Analysis\n\nStandard error \\[\\hat{\\sigma}_n = \\frac{\\sigma_{f(x)}}{\\sqrt{n}} \\approx \\frac{\\hat{\\sigma}_{f(x_{1:n})}}{\\sqrt{n}}\\]\nConfidence interval given by CLT based on standard error."
  },
  {
    "objectID": "slides/lecture07-2.html#airshed-model",
    "href": "slides/lecture07-2.html#airshed-model",
    "title": "Monte Carlo Application and the Bootstrap",
    "section": "Airshed Model",
    "text": "Airshed Model\n\n\n\n\n\n\n\n\nFigure 1: Illustration of the airshed, including notation.\n\n\n\n\nGoal: Find the probability of exceeding the 1-hour SO2 average exposure concentration standard, which is 0.14 ppm.\n\n\nUncertainties: Hourly wind speed \\(u\\), the inflow concentration \\(C_\\text{in}\\), net rate of SO2 emission within the airshed \\(R=S-D\\)."
  },
  {
    "objectID": "slides/lecture07-2.html#airshed-model-1",
    "href": "slides/lecture07-2.html#airshed-model-1",
    "title": "Monte Carlo Application and the Bootstrap",
    "section": "Airshed Model",
    "text": "Airshed Model\n\n\nFigure 2: Illustration of the airshed, including notation.\n\\[\\frac{dC}{dt} = \\frac{u}{L} C_\\text{in} + \\frac{S-D}{WHL} - \\left(\\frac{u}{L} + k\\right)C\\]"
  },
  {
    "objectID": "slides/lecture07-2.html#forward-euler-discretization",
    "href": "slides/lecture07-2.html#forward-euler-discretization",
    "title": "Monte Carlo Application and the Bootstrap",
    "section": "Forward Euler Discretization",
    "text": "Forward Euler Discretization\n\\[\n\\frac{dC}{dt} = \\frac{u}{L} C_\\text{in}(t) + \\frac{S-D}{WHL} - \\left(\\frac{u}{L} + k\\right)C\\]\n\n\\[\\Rightarrow \\frac{C(t+1) - C(t)}{\\Delta t} = \\frac{u}{L} C_\\text{in}(t) + \\frac{R}{WHL} - \\left(\\frac{u}{L} + k\\right)C(t)\\]\n\n\n\\[\\bbox[yellow, 10px, border:5px solid red]{C(t+1) = \\left(1 - \\Delta t\\left(\\frac{u}{L} + k\\right)\\right)C(t) + \\Delta t \\left(\\frac{u}{L} C_\\text{in}(t) + \\frac{R}{WHL}\\right)}\n\\]"
  },
  {
    "objectID": "slides/lecture07-2.html#monte-carlo-samples",
    "href": "slides/lecture07-2.html#monte-carlo-samples",
    "title": "Monte Carlo Application and the Bootstrap",
    "section": "Monte Carlo Samples",
    "text": "Monte Carlo Samples\n\nCode\nnsamp = 1000\nu = rand(LogNormal(log(2), 1), nsamp)\nCin = rand(LogNormal(log(0.16), 0.12), nsamp)\nR = rand(Normal(0.5, 0.5), nsamp)\n\np1 = histogram(u, ylabel=\"count\", xlabel=L\"$u$ (m/s)\", label=false, tickfontsize=16, guidefontsize=18, size=(400, 450))\np2 = histogram(Cin, ylabel=\"count\", xlabel=L\"$C_{in}$ (ppm)\", label=false, tickfontsize=16, guidefontsize=18, size=(400, 450))\np3 = histogram(R, ylabel=\"count\", xlabel=L\"$R$ (ppm/hr)\", label=false, tickfontsize=16, guidefontsize=18, size=(400, 450))\ndisplay(p1)\ndisplay(p2)\ndisplay(p3)\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Monte Carlo samples for the airshed model.\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\nFigure 3"
  },
  {
    "objectID": "slides/lecture07-2.html#simulation-results",
    "href": "slides/lecture07-2.html#simulation-results",
    "title": "Monte Carlo Application and the Bootstrap",
    "section": "Simulation Results",
    "text": "Simulation Results\n\nCode\n# other parameters\nC₀ = 0.07\nT = 60\nk = 0.3\nW = 4\nH = 5\nL = 4\n# conduct simulation\nP = u / L .* Cin\nl = u / L .+ k\nC2 = zeros(T*100 + 1, nsamp)\nS = 0:0.01:T\nfor (i, t) in pairs(S)\n    if i == 1\n        C2[i, :] .= C₀\n    else\n        C2[i, :] = (1 .- 0.01*l) .* C2[i-1, :] .+ 0.01 * P .+ 0.01 * R / (H * W * L)\n    end\nend\nmean_SO2 = map(mean, eachcol(C2)) # calculate means\n# plot histogram\np1 = histogram(mean_SO2, xlabel=\"1-Hour Average Exposure (ppm)\", ylabel=\"Count\", legend=false, tickfontsize=16, guidefontsize=18)\nvline!(p1, [0.14], color=:red, linestyle=:dash, linewidth=3)\nxticks!(p1, 0:0.04:0.3)\nxaxis!(p1, xminorticks=2)\nplot!(p1, size=(600, 450))\n# plot cdf\np2 = plot(sort(mean_SO2), (1:nsamp) ./ nsamp, xlabel=\"1-Hour Average Exposure (ppm)\", ylabel=\"Cumulative Probability\", legend=false, tickfontsize=17, guidefontsize=18, linewidth=3)\nvline!(p2, [0.14], linestyle=:dash, color=:red, linewidth=3, minorgrid=true)\nxticks!(p2, 0:0.04:0.3)\nxaxis!(p2, xminorticks=2)\nyaxis!(p2, yminorticks=5)\nplot!(p2, size=(600, 450))\n\ndisplay(p1)\ndisplay(p2)\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Monte Carlo samples for the airshed model.\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure 4"
  },
  {
    "objectID": "slides/lecture07-2.html#monte-carlo-estimation",
    "href": "slides/lecture07-2.html#monte-carlo-estimation",
    "title": "Monte Carlo Application and the Bootstrap",
    "section": "Monte Carlo Estimation",
    "text": "Monte Carlo Estimation\n\n\n\\[\\hat{\\mu}_n = \\frac{1}{n}\\sum_{i=1}^n \\mathbb{I}[x_i &gt; 0.14]\\]\n\\[\\hat{\\sigma}_n = \\sqrt{\\frac{\\text{Var}(\\mathbb{I}[x_{1:n} &gt; 0.14])}{n}}\\]\n\n\n\nCode\n# show Monte Carlo estimate stabilization\navg_mc_out = zeros(nsamp)\navg_mc_out[1] = mean_SO2[1] &gt; 0.14\nstd_mc_out = zeros(nsamp)\nstd_mc_out[1] = 0\nfor i = 2:nsamp\n    avg_mc_out[i] = (avg_mc_out[i-1] * (i-1) + (mean_SO2[i] &gt; 0.14)) / i\n    std_mc_out[i] = 1/sqrt(i) * std(mean_SO2[1:i] .&gt; 0.14)\nend\np = plot(avg_mc_out, xlabel=\"Monte Carlo Iteration\", ylabel=\"Probability\", left_margin=3mm, legend=:false, ribbon=1.96*std_mc_out, fillalpha=0.3, linewidth=2, tickfontsize=16, guidefontsize=18, fillcolor=:red, right_margin=5mm, minorgrid=true)\nylims!(p, (0, 0.3))\nyaxis!(p, yminorticks=5)\nplot!(p, size=(600, 450))\ndisplay(p)\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Monte Carlo estimation for the airshed model."
  },
  {
    "objectID": "slides/lecture07-2.html#upshot-of-monte-carlo",
    "href": "slides/lecture07-2.html#upshot-of-monte-carlo",
    "title": "Monte Carlo Application and the Bootstrap",
    "section": "Upshot of Monte Carlo",
    "text": "Upshot of Monte Carlo\n\nEstimate summary statistics through simulation\nNeed input distributions. How do we get them?\nUncertainty Quantification: Inferring probabilistic representations of uncertainties (such as input distributions).\nSome UQ methods:\n\nThe Bootstrap\nMarkov chain Monte Carlo\nExpert elicitation"
  },
  {
    "objectID": "slides/lecture07-2.html#sampling-distributions",
    "href": "slides/lecture07-2.html#sampling-distributions",
    "title": "Monte Carlo Application and the Bootstrap",
    "section": "Sampling Distributions",
    "text": "Sampling Distributions\n\n\nThe sampling distribution of a statistic captures the uncertainty associated with random samples.\n\n\n\n\nSampling Distribution"
  },
  {
    "objectID": "slides/lecture07-2.html#the-bootstrap-principle",
    "href": "slides/lecture07-2.html#the-bootstrap-principle",
    "title": "Monte Carlo Application and the Bootstrap",
    "section": "The Bootstrap Principle",
    "text": "The Bootstrap Principle\n\n\nEfron (1979) suggested combining estimation with simulation: the bootstrap.\nKey idea: use the data to simulate a data-generating mechanism.\n\n\n\n\n\nBaron von Munchhausen Pulling Himself By His Hair\n\n\n\nSource: Wikipedia"
  },
  {
    "objectID": "slides/lecture07-2.html#monte-carlo-vs-bootstrapping",
    "href": "slides/lecture07-2.html#monte-carlo-vs-bootstrapping",
    "title": "Monte Carlo Application and the Bootstrap",
    "section": "Monte Carlo vs Bootstrapping",
    "text": "Monte Carlo vs Bootstrapping\nMonte Carlo: If we have a generative probability model (including input distributions), simulate new samples from the model and estimate the sampling distribution.\nBootstrap: assumes the existing data is representative of the “true” population, and can simulate based on properties of the data itself."
  },
  {
    "objectID": "slides/lecture07-2.html#why-does-the-bootstrap-work",
    "href": "slides/lecture07-2.html#why-does-the-bootstrap-work",
    "title": "Monte Carlo Application and the Bootstrap",
    "section": "Why Does The Bootstrap Work?",
    "text": "Why Does The Bootstrap Work?\nEfron’s key insight: due to the Central Limit Theorem, the differences between estimates drawn from the sampling distribution and the true value converge to a normal distribution.\n\nUse the bootstrap to approximate the sampling distribution through re-sampling and re-estimation.\nCan draw asymptotic quantities (bias estimates, confidence intervals, etc) from the differences between the sample estimate and the bootstrap estimates."
  },
  {
    "objectID": "slides/lecture07-2.html#what-can-we-do-with-the-bootstrap",
    "href": "slides/lecture07-2.html#what-can-we-do-with-the-bootstrap",
    "title": "Monte Carlo Application and the Bootstrap",
    "section": "What Can We Do With The Bootstrap?",
    "text": "What Can We Do With The Bootstrap?\nLet \\(t_0\\) the “true” value of a statistic, \\(\\hat{t}\\) the estimate of the statistic from the sample, and \\((\\tilde{t}_i)\\) the bootstrap estimates.\n\n\nEstimate Variance: \\(\\text{Var}[\\hat{t}] \\approx \\text{Var}[\\tilde{t}]\\)\nBias Correction: \\(\\mathbb{E}[\\hat{t}] - t_0 \\approx \\mathbb{E}[\\tilde{t}] - \\hat{t}\\)\nCompute basic \\(\\alpha\\)-confidence intervals: \\[\\left(\\hat{t} - (Q_{\\tilde{t}}(1-\\alpha/2) - \\hat{t}), \\hat{t} - (Q_{\\tilde{t}}(\\alpha/2) - \\hat{t})\\right)\\]"
  },
  {
    "objectID": "slides/lecture07-2.html#the-non-parametric-bootstrap-1",
    "href": "slides/lecture07-2.html#the-non-parametric-bootstrap-1",
    "title": "Monte Carlo Application and the Bootstrap",
    "section": "The Non-Parametric Bootstrap",
    "text": "The Non-Parametric Bootstrap\n\n\nThe non-parametric bootstrap is the most “naive” approach to the bootstrap: resample-then-estimate.\n\n\n\n\nNon-Parametric Bootstrap"
  },
  {
    "objectID": "slides/lecture07-2.html#simple-example-is-a-coin-fair",
    "href": "slides/lecture07-2.html#simple-example-is-a-coin-fair",
    "title": "Monte Carlo Application and the Bootstrap",
    "section": "Simple Example: Is A Coin Fair?",
    "text": "Simple Example: Is A Coin Fair?\nSuppose we have observed twenty flips with a coin, and want to know if it is weighted.\n\n\nCode\n# define coin-flip model\np_true = 0.6\nn_flips = 20\ncoin_dist = Bernoulli(p_true)\n# generate data set\ndat = rand(coin_dist, n_flips)\nfreq_dat = sum(dat) / length(dat)\ndat'\n\n\n1×20 adjoint(::Vector{Bool}) with eltype Bool:\n 0  1  0  1  0  0  1  0  1  0  1  0  1  1  1  0  0  1  1  0\n\n\nThe frequency of heads is 0.5."
  },
  {
    "objectID": "slides/lecture07-2.html#is-the-coin-fair",
    "href": "slides/lecture07-2.html#is-the-coin-fair",
    "title": "Monte Carlo Application and the Bootstrap",
    "section": "Is The Coin Fair?",
    "text": "Is The Coin Fair?\n\n\nCode\n# bootstrap: draw new samples\nfunction coin_boot_sample(dat)\n    boot_sample = sample(dat, length(dat); replace=true)\n    return boot_sample\nend\n\nfunction coin_boot_freq(dat, nsamp)\n    boot_freq = [sum(coin_boot_sample(dat)) for _ in 1:nsamp]\n    return boot_freq / length(dat)\nend\n\nboot_out = coin_boot_freq(dat, 1000)\nq_boot = 2 * freq_dat .- quantile(boot_out, [0.975, 0.025])\n\np = histogram(boot_out, xlabel=\"Heads Frequency\", ylabel=\"Count\", title=\"1000 Bootstrap Samples\", titlefontsize=20, guidefontsize=18, tickfontsize=16, legendfontsize=16, label=false, bottom_margin=7mm, left_margin=5mm, right_margin=5mm)\nvline!(p, [p_true], linewidth=3, color=:orange, linestyle=:dash, label=\"True Probability\")\nvline!(p, [mean(boot_out) ], linewidth=3, color=:red, linestyle=:dash, label=\"Bootstrap Mean\")\nvline!(p, [freq_dat], linewidth=3, color=:purple, linestyle=:dash, label=\"Observed Frequency\")\nvspan!(p, q_boot, linecolor=:grey, fillcolor=:grey, alpha=0.3, fillalpha=0.3, label=\"95% CI\")\nplot!(p, size=(1000, 450))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Bootstrap heads frequencies for 20 resamples."
  },
  {
    "objectID": "slides/lecture07-2.html#larger-sample-example",
    "href": "slides/lecture07-2.html#larger-sample-example",
    "title": "Monte Carlo Application and the Bootstrap",
    "section": "Larger Sample Example",
    "text": "Larger Sample Example\n\n\nCode\nn_flips = 50\ndat = rand(coin_dist, n_flips)\nfreq_dat = sum(dat) / length(dat)\n\nboot_out = coin_boot_freq(dat, 1000)\nq_boot = 2 * freq_dat .- quantile(boot_out, [0.975, 0.025])\n\np = histogram(boot_out, xlabel=\"Heads Frequency\", ylabel=\"Count\", title=\"1000 Bootstrap Samples\", titlefontsize=20, guidefontsize=18, tickfontsize=16, legendfontsize=16, label=false, bottom_margin=7mm, left_margin=5mm, right_margin=5mm)\nvline!(p, [p_true], linewidth=3, color=:orange, linestyle=:dash, label=\"True Probability\")\nvline!(p, [mean(boot_out) ], linewidth=3, color=:red, linestyle=:dash, label=\"Bootstrap Mean\")\nvline!(p, [freq_dat], linewidth=3, color=:purple, linestyle=:dash, label=\"Observed Frequency\")\nvspan!(p, q_boot, linecolor=:grey, fillcolor=:grey, alpha=0.3, fillalpha=0.3, label=\"95% CI\")\nplot!(p, size=(1000, 450))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: Bootstrap heads frequencies for 20 resamples."
  },
  {
    "objectID": "slides/lecture07-2.html#bootstrapping-with-structured-data",
    "href": "slides/lecture07-2.html#bootstrapping-with-structured-data",
    "title": "Monte Carlo Application and the Bootstrap",
    "section": "Bootstrapping with Structured Data",
    "text": "Bootstrapping with Structured Data\nThe naive non-parametric bootstrap that we just saw doesn’t work if data has structure, e.g. spatial or temporal dependence."
  },
  {
    "objectID": "slides/lecture07-2.html#bootstrapping-with-structured-data-1",
    "href": "slides/lecture07-2.html#bootstrapping-with-structured-data-1",
    "title": "Monte Carlo Application and the Bootstrap",
    "section": "Bootstrapping with Structured Data",
    "text": "Bootstrapping with Structured Data\n\nCode\nsl_dat = CSV.read(joinpath(@__DIR__, \"data\", \"sealevel\", \"CSIRO_Recons_gmsl_yr_2015.csv\"), DataFrame)\np1 = plot(sl_dat[:, 1] .- 0.5, sl_dat[:, 2], xlabel=\"Year\", ylabel=\"GMSL (mm)\", title=\"Sea-Level Rise Observations\", label=:false, linewidth=2, tickfontsize=16, guidefontsize=18, titlefontsize=20)\nplot!(p1, size=(600, 450))\n\nresample_index = sample(1:nrow(sl_dat), nrow(sl_dat); replace=true)\np2 = plot(sl_dat[:, 1] .- 0.5, sl_dat[resample_index, 2], xlabel=\"Year\", ylabel=\"GMSL (mm)\", title=\"Sea-Level Rise Resample\", label=:false, linewidth=2, tickfontsize=16, guidefontsize=18, titlefontsize=20)\nplot!(p2, size=(600, 450))\n\ndisplay(p1)\ndisplay(p2)\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Simple bootstrap with time series data.\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure 8"
  },
  {
    "objectID": "slides/lecture07-2.html#why-use-the-bootstrap",
    "href": "slides/lecture07-2.html#why-use-the-bootstrap",
    "title": "Monte Carlo Application and the Bootstrap",
    "section": "Why Use The Bootstrap?",
    "text": "Why Use The Bootstrap?\n\nDo not need to rely on variance asymptotics;\nCan obtain non-symmetric CIs."
  },
  {
    "objectID": "slides/lecture07-2.html#approaches-to-bootstrapping-structured-data",
    "href": "slides/lecture07-2.html#approaches-to-bootstrapping-structured-data",
    "title": "Monte Carlo Application and the Bootstrap",
    "section": "Approaches to Bootstrapping Structured Data",
    "text": "Approaches to Bootstrapping Structured Data\n\nCorrelations: Transform to uncorrelated data (principal components, etc.), sample, transform back.\nTime Series: Block bootstrap"
  },
  {
    "objectID": "slides/lecture07-2.html#block-bootstrap",
    "href": "slides/lecture07-2.html#block-bootstrap",
    "title": "Monte Carlo Application and the Bootstrap",
    "section": "Block Bootstrap",
    "text": "Block Bootstrap\nDivide the data \\(x_t\\) into blocks of length \\(b\\), \\(X_1, \\ldots, X_n\\).\nFor example: \\[X_1 = x_{1:b}, X_2 = x_{b+1:2b}, \\ldots, X_n = x_{(n-1)b+1:nb}\\]\nThen resample blocks and glue back: \\(X_{\\sigma(1)}, \\ldots, X_{\\sigma(n)}\\)"
  },
  {
    "objectID": "slides/lecture07-2.html#block-bootstrap-example",
    "href": "slides/lecture07-2.html#block-bootstrap-example",
    "title": "Monte Carlo Application and the Bootstrap",
    "section": "Block Bootstrap Example",
    "text": "Block Bootstrap Example\n\nCode\nblock_idx = collect(1:10:131)\nblocks_resample_idx = sample(1:length(block_idx)-1, length(block_idx)-1)\np1 = plot(sl_dat[1:130, 1] .- 0.5, sl_dat[1:130, 2], xlabel=\"Year\", ylabel=\"GMSL (mm)\", title=\"Sea-Level Rise Observations\", label=:false, linewidth=2, tickfontsize=16, guidefontsize=18, titlefontsize=20)\nvspan!(p1, sl_dat[block_idx, 1] .- 0.5, fillcolor=:gray, fillalpha=0.3, label=false)\nplot!(p1, size=(600, 450))\n\nblock_resample = zeros(130)\nfor i = 1:length(block_idx)-1\n    block_resample[(i-1)*10+1:i*10] = sl_dat[block_idx[blocks_resample_idx[i]]:block_idx[blocks_resample_idx[i]]+9, 2]\nend\np2 = plot(sl_dat[1:130, 1] .- 0.5, block_resample, xlabel=\"Year\", ylabel=\"GMSL (mm)\", title=\"Sea-Level Rise Resample\", label=:false, linewidth=2, tickfontsize=16, guidefontsize=18, titlefontsize=20)\nplot!(p2, size=(600, 450))\n\ndisplay(p1)\ndisplay(p2)\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Simple bootstrap with time series data.\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure 9"
  },
  {
    "objectID": "slides/lecture07-2.html#generalizing-the-block-bootstrap",
    "href": "slides/lecture07-2.html#generalizing-the-block-bootstrap",
    "title": "Monte Carlo Application and the Bootstrap",
    "section": "Generalizing the Block Bootstrap",
    "text": "Generalizing the Block Bootstrap\nThe rough transitions in the block bootstrap can really degrade estimator quality.\n\nImprove transitions between blocks\nMoving blocks (allow overlaps)"
  },
  {
    "objectID": "slides/lecture07-2.html#key-points",
    "href": "slides/lecture07-2.html#key-points",
    "title": "Monte Carlo Application and the Bootstrap",
    "section": "Key Points",
    "text": "Key Points\n\n\nBootstrap: Approximate sampling distribution by re-simulating data\nNon-Parametric Bootstrap: Treat data as representative of population and re-sample.\nMore complicated for structured data."
  },
  {
    "objectID": "slides/lecture07-2.html#sources-of-non-parametric-bootstrap-error",
    "href": "slides/lecture07-2.html#sources-of-non-parametric-bootstrap-error",
    "title": "Monte Carlo Application and the Bootstrap",
    "section": "Sources of Non-Parametric Bootstrap Error",
    "text": "Sources of Non-Parametric Bootstrap Error\n\nSampling error: error from using finitely many replications\nStatistical error: error in the bootstrap sampling distribution approximation"
  },
  {
    "objectID": "slides/lecture07-2.html#when-to-use-the-non-parametric-bootstrap",
    "href": "slides/lecture07-2.html#when-to-use-the-non-parametric-bootstrap",
    "title": "Monte Carlo Application and the Bootstrap",
    "section": "When To Use The Non-Parametric Bootstrap",
    "text": "When To Use The Non-Parametric Bootstrap\n\nSample is representative of the sampling distribution\nDoesn’t work well for extreme values!"
  },
  {
    "objectID": "slides/lecture07-2.html#next-classes",
    "href": "slides/lecture07-2.html#next-classes",
    "title": "Monte Carlo Application and the Bootstrap",
    "section": "Next Classes",
    "text": "Next Classes\nMonday: Parametric Bootstrap and Examples\nWednesday: What is a Markov Chain?"
  },
  {
    "objectID": "slides/lecture07-2.html#assessments",
    "href": "slides/lecture07-2.html#assessments",
    "title": "Monte Carlo Application and the Bootstrap",
    "section": "Assessments",
    "text": "Assessments\nExercise 7: Due Friday"
  },
  {
    "objectID": "slides/lecture07-2.html#references-1",
    "href": "slides/lecture07-2.html#references-1",
    "title": "Monte Carlo Application and the Bootstrap",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n\n\nEfron, B. (1979). Bootstrap methods: Another look at the jackknife. Ann. Stat., 7, 1–26. https://doi.org/10.1214/aos/1176344552"
  },
  {
    "objectID": "slides/lecture02-1.html#modes-of-data-analysis",
    "href": "slides/lecture02-1.html#modes-of-data-analysis",
    "title": "Probability Models and Model Residuals",
    "section": "Modes of Data Analysis",
    "text": "Modes of Data Analysis"
  },
  {
    "objectID": "slides/lecture02-1.html#probability-review",
    "href": "slides/lecture02-1.html#probability-review",
    "title": "Probability Models and Model Residuals",
    "section": "Probability Review",
    "text": "Probability Review\n\nDistributions\nCentral Limit Theorem\nConfidence Intervals"
  },
  {
    "objectID": "slides/lecture02-1.html#questions",
    "href": "slides/lecture02-1.html#questions",
    "title": "Probability Models and Model Residuals",
    "section": "Questions?",
    "text": "Questions?\n\n\n\n\n\n\n\n\nText: VSRIKRISH to 22333\nURL: https://pollev.com/vsrikrish  See Results"
  },
  {
    "objectID": "slides/lecture02-1.html#historical-temperature-anomalies",
    "href": "slides/lecture02-1.html#historical-temperature-anomalies",
    "title": "Probability Models and Model Residuals",
    "section": "Historical Temperature Anomalies",
    "text": "Historical Temperature Anomalies\n\n\nCode\ntemps = CSV.read(\"data/climate/HadCRUT.5.0.1.0.analysis.summary_series.global.annual.csv\", DataFrame, delim=\",\")\n\ntime_obs = temps[:, 1]\ntemp_obs = temps[:, 2]\ntemp_lo = temps[:, 3]\ntemp_hi = temps[:, 4]\n\ntemp_lo = temp_lo .- mean(temp_obs[1:20])\ntemp_hi = temp_hi .- mean(temp_obs[1:20])\ntemp_obs = temp_obs .- mean(temp_obs[1:20]) # compute anomalies relative to first 20 years of data\n\nplot(time_obs, temp_obs, ribbon=(temp_obs-temp_lo,temp_hi-temp_obs), color=\"blue\", linewidth=2, fillalpha=0.2, legend=false, xlabel=\"Year\", ylabel=\"Temperature anomaly (°C)\", labelfontsize=18, tickfontsize=16, bottom_margin=10mm, left_margin=10mm)\nplot!(size=(950, 450))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Global temperature anomalies\n\n\n\n\n\nData Source: HadCRUT 5.0.1.0"
  },
  {
    "objectID": "slides/lecture02-1.html#planetary-energy-balance",
    "href": "slides/lecture02-1.html#planetary-energy-balance",
    "title": "Probability Models and Model Residuals",
    "section": "Planetary Energy Balance",
    "text": "Planetary Energy Balance\n\nRepresentation of Planetary Energy Balance\nSource: Reprinted from A Climate Modeling Primer, A. Henderson-Sellers and K. McGuffie, Wiley, pg. 58, (1987) via https://www.e-education.psu.edu/meteo469/node/137."
  },
  {
    "objectID": "slides/lecture02-1.html#radiative-forcing",
    "href": "slides/lecture02-1.html#radiative-forcing",
    "title": "Probability Models and Model Residuals",
    "section": "Radiative Forcing",
    "text": "Radiative Forcing\nClimate changes result from changes to the energy balance of the planet (or radiative forcings), due to e.g.:\n\ngreenhouse gas emissions (which trap radiation, warming the planet);\naerosol emissions from air pollution or volcanic eruptions (which block incoming radiation, cooling the planet);\nchanges to the solar cycle (which can increase or decrease the incoming solar radiation)."
  },
  {
    "objectID": "slides/lecture02-1.html#historical-radiative-forcing",
    "href": "slides/lecture02-1.html#historical-radiative-forcing",
    "title": "Probability Models and Model Residuals",
    "section": "Historical Radiative Forcing",
    "text": "Historical Radiative Forcing\n\n\nCode\n# Dataset from https://zenodo.org/record/3973015\n# The CSV is read into a DataFrame object, and we specify that it is comma delimited\nforcings_all_85 = CSV.read(\"data/climate/ERF_ssp585_1750-2500.csv\", DataFrame, delim=\",\")\n\n# Separate out the individual components\nforcing_co2_85 = forcings_all_85[!,\"co2\"]\n# Get total aerosol forcings\nforcing_aerosol_rad_85 = forcings_all_85[!,\"aerosol-radiation_interactions\"]\nforcing_aerosol_cloud_85 = forcings_all_85[!,\"aerosol-cloud_interactions\"]\nforcing_aerosol_85 = forcing_aerosol_rad_85 + forcing_aerosol_cloud_85\nforcing_total_85 = forcings_all_85[!,\"total\"]\nforcing_non_aerosol_85 = forcing_total_85 - forcing_aerosol_85\nforcing_other_85 = forcing_total_85 - (forcing_co2_85 + forcing_aerosol_85)\n\nt = time_forcing = Int64.(forcings_all_85[!,\"year\"]) # Ensure that years are interpreted as integers\n\nplot(xlabel=\"Year\", ylabel=\"Radiative Forcing (W/m²)\", tickfontsize=16, guidefontsize=18, legendfontsize=16, leftmargin=10mm, bottommargin=5mm, right_margin=5mm)\nplot!(time_forcing, forcing_total_85, label=\"Total\", color=:black, linewidth=3)\nplot!(time_forcing, forcing_co2_85, label=\"CO₂\", color=:orange, linewidth=2)\nplot!(time_forcing, forcing_aerosol_85, label=\"Aerosol\", color=:blue, linewidth=2)\nplot!(time_forcing, forcing_other_85, label=\"Other\", color=:purple, linewidth=2)\nplot!(size=(800, 450))\nxlims!((1750, 2020))\nylims!(-4.5, 5)\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Historical and projected radiative forcings.\n\n\n\n\n\nData Source: https://zenodo.org/records/3973015"
  },
  {
    "objectID": "slides/lecture02-1.html#what-are-some-sources-of-relevant-uncertainty-in-understanding-past-and-future-climate-changes-and-impacts",
    "href": "slides/lecture02-1.html#what-are-some-sources-of-relevant-uncertainty-in-understanding-past-and-future-climate-changes-and-impacts",
    "title": "Probability Models and Model Residuals",
    "section": "What Are Some Sources of Relevant Uncertainty in Understanding Past and Future Climate Changes and Impacts?",
    "text": "What Are Some Sources of Relevant Uncertainty in Understanding Past and Future Climate Changes and Impacts?\n\nOne key question: what is the sensitivity of warming to continued CO2 emissions?"
  },
  {
    "objectID": "slides/lecture02-1.html#the-energy-balance-model-ebm",
    "href": "slides/lecture02-1.html#the-energy-balance-model-ebm",
    "title": "Probability Models and Model Residuals",
    "section": "The Energy Balance Model (EBM)",
    "text": "The Energy Balance Model (EBM)\n\\[\\begin{align*}\n\\overbrace{\\frac{dH}{dt}}^{\\text{change in heat}} &= \\overbrace{F}^{\\text{RF}} - \\overbrace{\\lambda T}^{\\substack{\\text{change in} \\\\ \\text{temperature}}} \\\\\n\\underbrace{C}_{\\substack{\\text{ocean heat} \\\\ \\text{capacity}}} \\frac{dT}{dt} &= F - \\lambda T \\\\\ncd \\frac{dT}{dt} &= F - \\lambda T,\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/lecture02-1.html#the-ebm-contd",
    "href": "slides/lecture02-1.html#the-ebm-contd",
    "title": "Probability Models and Model Residuals",
    "section": "The EBM (cont’d)",
    "text": "The EBM (cont’d)\n\n\\(c = 4.184\\times 10^6 \\\\ \\text{J/K/m}^2\\) is the specific heat of water per area.\nTotal RF: \\[F = F_\\text{non-aerosol} + \\alpha F_\\text{aerosol}.\\]\nThe climate feedback factor \\(\\lambda\\) controls how much the Earth warms in response to radiative forcing."
  },
  {
    "objectID": "slides/lecture02-1.html#ebm-solution",
    "href": "slides/lecture02-1.html#ebm-solution",
    "title": "Probability Models and Model Residuals",
    "section": "EBM Solution",
    "text": "EBM Solution\nUse Euler discretization:\n\\[\\begin{gather*}\nC \\frac{dT}{dt} = F - \\lambda T \\\\\\\\\n\\Rightarrow C \\frac{T_{i+1}-T_i}{\\Delta t} = F_i - \\lambda T_i \\\\\\\\\n\\Rightarrow \\bbox[yellow, 10px, border:5px solid red]{T_{i+1} = T_i + \\frac{F_i - \\lambda T_i}{C} \\Delta t}\n\\end{gather*}\\]"
  },
  {
    "objectID": "slides/lecture02-1.html#equilibrium-climate-sensitivity-ecs",
    "href": "slides/lecture02-1.html#equilibrium-climate-sensitivity-ecs",
    "title": "Probability Models and Model Residuals",
    "section": "Equilibrium Climate Sensitivity (ECS)",
    "text": "Equilibrium Climate Sensitivity (ECS)\nUnder steady-state conditions (constant \\(F\\) and \\(dT/dt = 0\\)), \\[T = \\frac{F}{\\lambda}.\\]\nWhen we double atmospheric CO2, we refer to the equilibrium temperature \\(S\\) as the equilibrium climate sensitivity:\n\\[S = \\underbrace{F_{2\\times \\text{CO}_2}}_{\\approx 4 \\text{W/m}^2}/\\lambda\\]"
  },
  {
    "objectID": "slides/lecture02-1.html#degree-of-freedom-free-parameters",
    "href": "slides/lecture02-1.html#degree-of-freedom-free-parameters",
    "title": "Probability Models and Model Residuals",
    "section": "Degree of Freedom / Free Parameters",
    "text": "Degree of Freedom / Free Parameters\nThere are a few uncertain parameters:\n\n\\(\\lambda\\) or \\(S\\)\n\\(d\\) (ocean mixing depth)\n\\(\\alpha\\) (aerosol scaling factor)"
  },
  {
    "objectID": "slides/lecture02-1.html#model-fitting-by-minimizing-loss",
    "href": "slides/lecture02-1.html#model-fitting-by-minimizing-loss",
    "title": "Probability Models and Model Residuals",
    "section": "Model Fitting By Minimizing Loss",
    "text": "Model Fitting By Minimizing Loss\nIdea: Find best estimates \\(\\theta^*\\) of model parameters \\(\\theta\\) by minimizing the mismatch between data and simulations (denote by \\(\\mathbf{y} = F(\\theta)\\)).\nKey choice: “loss function” \\(L(\\mathbf{y}, \\mathbf{x})\\), then: \\[\\theta^*  = \\underset{\\theta}{\\operatorname{argmin}} L(F(\\theta), \\mathbf{x}).\\]\n\nCan you think of some common loss functions? What do they imply about how to penalize model error?"
  },
  {
    "objectID": "slides/lecture02-1.html#programming-implementation",
    "href": "slides/lecture02-1.html#programming-implementation",
    "title": "Probability Models and Model Residuals",
    "section": "Programming Implementation",
    "text": "Programming Implementation\n\n# use default values of S=3.2°C, d=100m, α=1.3\nfunction ebm(rf_nonaerosol, rf_aerosol; p=(3.2, 100, 1.3))\n    # set up model parameters\n    S, d, α = p # this unpacks the parameter tuple into variables\n    F2xCO₂ = 4.0 # radiative forcing [W/m²] for a doubling of CO₂\n    λ = F2xCO₂ / S\n\n    c = 4.184e6 # heat capacity/area [J/K/m²]\n    C = c*d # heat capacity of mixed layer (per area)\n    F = rf_nonaerosol + α*rf_aerosol # radiative forcing\n    Δt = 31558152. # annual timestep [s]\n\n    T = zero(F)\n    for i in 1:length(F)-1\n        T[i+1] = T[i] + (F[i] - λ*T[i])/C * Δt\n    end\n    \n    # return after normalizing to reference period\n    return T .- mean(T[1:20])\nend"
  },
  {
    "objectID": "slides/lecture02-1.html#model-evaluation-default-parameters",
    "href": "slides/lecture02-1.html#model-evaluation-default-parameters",
    "title": "Probability Models and Model Residuals",
    "section": "Model Evaluation (Default Parameters)",
    "text": "Model Evaluation (Default Parameters)\n\n\nCode\n# generate simulations\nsim_years = 1850:2020 # model years to simulate\nidx = indexin(sim_years, t) # find indices in t vector of simulation years\n# since we specified default values for p, those are used for the parameters\ntemp_default = ebm(forcing_non_aerosol_85[idx], forcing_aerosol_85[idx]) \n\ntemp_obs = temp_obs[indexin(sim_years, time_obs)] # filter to simulated years for plotting\ntemp_obs = temp_obs .- mean(temp_obs[1:20]) # re-normalize to be consistent with the model\n# plot simulated output and data\nplot(sim_years, temp_default, xlabel=\"Year\", ylabel=\"Temperature Anomaly (°C)\", color=:blue, label=\"Simulation\", linewidth=3, tickfontsize=16, guidefontsize=18, legendfontsize=16, leftmargin=10mm, bottommargin=5mm, right_margin=5mm)\nscatter!(sim_years, temp_obs, color=:black, linewidth=2, fillalpha=0.2, label=\"Data\")\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Comparison of model simualtion with default parameters and data."
  },
  {
    "objectID": "slides/lecture02-1.html#fitting-the-ebm-by-minimizing-rmse",
    "href": "slides/lecture02-1.html#fitting-the-ebm-by-minimizing-rmse",
    "title": "Probability Models and Model Residuals",
    "section": "Fitting the EBM By Minimizing RMSE",
    "text": "Fitting the EBM By Minimizing RMSE\n\n# define RMSE function\nrmse(y, x) = sqrt(mean((y .- x).^2))\n\n# define wrapper function to map parameters to model evaluations\nebm_wrap(params) = ebm(forcing_non_aerosol_85[idx], forcing_aerosol_85[idx], p = params)\n# minimize RMSE within some range for each parameter\n# important to make everything a Float instead of an Int \nlower = [1.0, 50.0, 0.0]\nupper = [4.0, 150.0, 2.0]\np0 = [2.0, 100.0, 1.0]\nresult = Optim.optimize(params -&gt; rmse(ebm_wrap(params), temp_obs), lower, upper, p0)\nθ = result.minimizer\nθ"
  },
  {
    "objectID": "slides/lecture02-1.html#fitting-the-ebm-by-minimizing-rmse-output",
    "href": "slides/lecture02-1.html#fitting-the-ebm-by-minimizing-rmse-output",
    "title": "Probability Models and Model Residuals",
    "section": "Fitting the EBM By Minimizing RMSE",
    "text": "Fitting the EBM By Minimizing RMSE\n\n3-element Vector{Float64}:\n  1.943747735126569\n 86.39106697164416\n  0.7893331703704984"
  },
  {
    "objectID": "slides/lecture02-1.html#fitted-results",
    "href": "slides/lecture02-1.html#fitted-results",
    "title": "Probability Models and Model Residuals",
    "section": "Fitted Results",
    "text": "Fitted Results\n\n\nCode\ntemp_fitted = ebm(forcing_non_aerosol_85[idx], forcing_aerosol_85[idx]; p = θ) \n\n# plot simulated output and data\nplot(sim_years, temp_default, xlabel=\"Year\", ylabel=\"Temperature Anomaly (°C)\", color=:blue, label=\"Default Simulation\", linewidth=3, tickfontsize=16, guidefontsize=18, legendfontsize=16, leftmargin=10mm, bottommargin=5mm, right_margin=5mm)\nplot!(sim_years, temp_fitted, color=:red, label=\"Fitted Simulation\", linewidth=3)\nscatter!(sim_years, temp_obs, color=:black, linewidth=2, fillalpha=0.2, label=\"Data\")\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Comparison of model simulation with fitted parameters, default parameters, and data."
  },
  {
    "objectID": "slides/lecture02-1.html#what-the-ebm-neglects",
    "href": "slides/lecture02-1.html#what-the-ebm-neglects",
    "title": "Probability Models and Model Residuals",
    "section": "What the EBM Neglects",
    "text": "What the EBM Neglects\nWhat are some things the EBM neglects or simplifies?\n\n\n\n\n\n\n\nImportant\n\n\nSince models can be so stylized, optimal model parameters which ought to correspond to physical values may not match their true values."
  },
  {
    "objectID": "slides/lecture02-1.html#likelihood-of-parameters",
    "href": "slides/lecture02-1.html#likelihood-of-parameters",
    "title": "Probability Models and Model Residuals",
    "section": "Likelihood of Parameters",
    "text": "Likelihood of Parameters\nProbability of data given probability model: \\(p(\\mathbf{x} | \\theta)\\)\nLikelihood of parameters given probability model and data: \\(\\mathcal{L}(\\theta | \\mathbf{x}) = p(\\mathbf{x} | \\theta)\\)\nExample (normal distribution):\n\\[\\mathcal{L}(\\mu, \\sigma | \\mathbf{x}) = p(\\mathbf{x} | \\mu, \\sigma) = \\prod_{i=1}^n \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{1}{2}\\left(\\frac{x_i - \\mu}{\\sigma}^2\\right)\\right)\\]"
  },
  {
    "objectID": "slides/lecture02-1.html#probability-model-fitting",
    "href": "slides/lecture02-1.html#probability-model-fitting",
    "title": "Probability Models and Model Residuals",
    "section": "Probability Model Fitting",
    "text": "Probability Model Fitting\nFitting a probability model ⇔ Maximizing Likelihood\n\n\nWe often work with log-likelihoods since \\[\\operatorname{argmax} \\left[\\log f(x)\\right] = \\operatorname{argmax} f(x)\\]\nand sums and small numbers are more stable.\n\n\n\nCode\nx = 0:0.1:3\nf(x) = abs.(3 - (x-2)^2)\nplot(x, f.(x), color=:blue, label=L\"$f(x)$\", linewidth=3, legend=:bottomright, tickfontsize=16, legendfontsize=16, guidefontsize=18)\nplot!(x, log.(f.(x)), color=:red, label=L\"$\\log f(x))$\", linewidth=3)\nplot!(size=(500, 400))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Comparing logarithm of a function with the function."
  },
  {
    "objectID": "slides/lecture02-1.html#model-residuals",
    "href": "slides/lecture02-1.html#model-residuals",
    "title": "Probability Models and Model Residuals",
    "section": "Model Residuals",
    "text": "Model Residuals\nModel Residuals are the “error” between the model simulations and the data.\n\\[\\underbrace{\\mathbf{r}}_{\\text{residuals}} = F(\\mathbf{x}; \\theta) - \\underbrace{\\mathbf{y}}_{\\text{data}}\\]\n\nThe connection between statistical modeling and simulation modeling is developing a probability model for the residuals."
  },
  {
    "objectID": "slides/lecture02-1.html#statistical-interpretation-of-rmse-minimization",
    "href": "slides/lecture02-1.html#statistical-interpretation-of-rmse-minimization",
    "title": "Probability Models and Model Residuals",
    "section": "Statistical Interpretation of RMSE Minimization",
    "text": "Statistical Interpretation of RMSE Minimization\nClaim: Minimizing the (R)MSE is the same as maximizing likelihood assuming independent and identically-normally-distributed residuals (with known variance).\n\\[\n\\begin{gather*}\n\\mathbf{y} = F(\\mathbf{x}) - \\mathbf{r} \\\\\nr_i \\overset{\\text{i.i.d.}}{\\sim} \\mathcal{N}(0, \\sigma)\n\\end{gather*}\n\\]"
  },
  {
    "objectID": "slides/lecture02-1.html#likelihood-of-normal-residuals",
    "href": "slides/lecture02-1.html#likelihood-of-normal-residuals",
    "title": "Probability Models and Model Residuals",
    "section": "Likelihood of Normal Residuals",
    "text": "Likelihood of Normal Residuals\nData Probability Model:\n\\[y_i \\sim \\mathcal{N}(F(x_i), \\sigma)\\]\n\n\\[\\mathcal{L}(\\theta | \\mathbf{y}; F) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi}} \\exp(-\\frac{y_i - F(x_i)^2}{2\\sigma^2})\\]\n\n\n\\[\\log \\mathcal{L}(\\theta | \\mathbf{y}; F) = \\sum_{i=1}^n \\left[\\log \\frac{1}{\\sqrt{2\\pi}} - \\frac{1}{2\\sigma^2}(y_i - F(x_i))^2 \\right]\\]"
  },
  {
    "objectID": "slides/lecture02-1.html#simplifying-log-likelihood",
    "href": "slides/lecture02-1.html#simplifying-log-likelihood",
    "title": "Probability Models and Model Residuals",
    "section": "",
    "text": "\\[\n\\begin{align}\n\\log \\mathcal{L}(\\theta | \\mathbf{y}, F) &= \\sum_{i=1}^n \\left[\\log \\frac{1}{\\sqrt{2\\pi}} - \\frac{1}{2\\sigma^2}(y_i - F(x_i))  ^2 \\right] \\\\\n&= n \\log \\frac{1}{\\sqrt{2\\pi}} - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - F(x_i))^2\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/lecture02-1.html#simplifying-constants-ignore",
    "href": "slides/lecture02-1.html#simplifying-constants-ignore",
    "title": "Probability Models and Model Residuals",
    "section": "",
    "text": "Ignoring constants (including \\(\\sigma\\)):\n\\[\\log \\mathcal{L}(\\theta | \\mathbf{y}, F) \\propto -\\sum_{i=1}^n (y_i - F(x_i))^2.\\]\n\nMaximizing \\(f(x)\\) is equivalent to minimizing \\(-f(x)\\):\n\\[\n- \\log \\mathcal{L}(\\theta | \\mathbf{y}, F) \\propto \\sum_{i=1}^n (y_i - F(x_i))^2 = \\text{MSE}\n\\]"
  },
  {
    "objectID": "slides/lecture02-1.html#key-points-1",
    "href": "slides/lecture02-1.html#key-points-1",
    "title": "Probability Models and Model Residuals",
    "section": "Key Points",
    "text": "Key Points\n\nGoal of model-based data analysis: Gain insights into data or underlying system through model simulations.\nRequires developing probability model for data or residuals.\nFitting a model as maximum likelihood estimation.\nCan develop more complex models (autocorrelated residuals, non-normal errors) but these may not result in “standard” error metrics."
  },
  {
    "objectID": "slides/lecture02-1.html#next-classes",
    "href": "slides/lecture02-1.html#next-classes",
    "title": "Probability Models and Model Residuals",
    "section": "Next Class(es)",
    "text": "Next Class(es)\nWednesday: Bayesian statistics and probability models\nNext Week: Exploratory data analysis"
  },
  {
    "objectID": "slides/lecture02-1.html#assessments",
    "href": "slides/lecture02-1.html#assessments",
    "title": "Probability Models and Model Residuals",
    "section": "Assessments",
    "text": "Assessments\nFriday: HW1 and Exercise 1 due by 9pm."
  },
  {
    "objectID": "slides/lecture06-2.html#monday-schedule-change",
    "href": "slides/lecture06-2.html#monday-schedule-change",
    "title": "Figure Discussion",
    "section": "Monday Schedule Change",
    "text": "Monday Schedule Change\n\nNo office hours Monday, please email me if you need to meet!"
  },
  {
    "objectID": "slides/lecture06-2.html#literature-critique",
    "href": "slides/lecture06-2.html#literature-critique",
    "title": "Figure Discussion",
    "section": "Literature Critique",
    "text": "Literature Critique\n\n3/27: Literature Critique\nFind and read a peer-reviewed journal article involving an application of data analysis.\nWrite a short discussion paper (1-2 pages?) analyzing the hypotheses and statistical choices.\nPrepare presentation of the paper (10 minutes; students will submit peer evaluations).\n5850: Write a more formal referee report."
  },
  {
    "objectID": "slides/lecture06-2.html#next-classes",
    "href": "slides/lecture06-2.html#next-classes",
    "title": "Figure Discussion",
    "section": "Next Class(es)",
    "text": "Next Class(es)\nNext Week: Monte Carlo"
  },
  {
    "objectID": "slides/lecture06-2.html#assessments",
    "href": "slides/lecture06-2.html#assessments",
    "title": "Figure Discussion",
    "section": "Assessments",
    "text": "Assessments\nFriday: Project proposal due by 9pm.\nHW3 released this weekend, due 3/15."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About This Website",
    "section": "",
    "text": "This website contains course materials for the Spring 2024 edition of Environmental Data Analysis and Simulation, taught by Vivek Srikrishnan at Cornell University."
  },
  {
    "objectID": "about.html#acknowledgements",
    "href": "about.html#acknowledgements",
    "title": "About This Website",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nMuch of the material for this course has evolved from discussions and work with Klaus Keller, Tony Wong, Casey Helgeson, Ben Seiyon Lee and James Doss-Gollin. Many thanks also to Andrew Gelman and Christian Robert, whose work heavily inspired many aspects of this course and which I refer back to regularly. Kieran Healy also wrote an outstanding book on Data Visualization which I learned a great deal from and still reference.\nThe layout for this site was also inspired by and draws from STA 210 at Duke University and Andrew Heiss’s course materials at Georgia State."
  },
  {
    "objectID": "about.html#tools-and-generation-workflow",
    "href": "about.html#tools-and-generation-workflow",
    "title": "About This Website",
    "section": "Tools and Generation Workflow",
    "text": "Tools and Generation Workflow\nThis website was built with Quarto, which allows me to integrate Julia code and output with the web content, pdfs, and slides in an amazingly clean fashion, while simplifying the process of generation. All materials can be generated through a simple workflow from the [GitHub Repository]."
  },
  {
    "objectID": "project/simulation.html",
    "href": "project/simulation.html",
    "title": "Simulation Study Instructions",
    "section": "",
    "text": "The goal of the simulation study is to implement your model(s) and evaluate their simulations to assess the differences between them. Your goal is not to “test” relevant hypotheses yet, but to visualize and describe the differences in the model hindcasts and projections."
  },
  {
    "objectID": "project/simulation.html#simulation-study-guidelines",
    "href": "project/simulation.html#simulation-study-guidelines",
    "title": "Simulation Study Instructions",
    "section": "Simulation Study Guidelines",
    "text": "Simulation Study Guidelines\nThe proposal should have the following components with 1 inch margins (on all sides) and at least 11 point font.\n\nCalibration: Find and report appropriate parameter values for your model(s). This can be done through finding the maximum likelihood or maximum a posteriori estimates, with relevant uncertainty estimates obtained from the bootstrap or sampling from the posterior distribution using MCMC. Whatever approach you take, make sure to justify it and any associated assumptions (e.g. prior distributions).\nSimulation: Using the calibrated model(s), simulate synthetic datasets and compute useful summaries. Include useful graphical checks (including, if appropriate, hindcasts and projections) as needed.\nComparison: Discuss the differences in the calibrations and simulations and what they mean for your hypotheses(s). Were there any surprises?\nReferences: Make sure to include any references cited in your proposal."
  },
  {
    "objectID": "project/proposal.html",
    "href": "project/proposal.html",
    "title": "Project Proposal Instructions",
    "section": "",
    "text": "Your project should involve analyzing a data set of interest using model-based simulation. The proposal is an opportunity to think through and articulate the core science question, hypotheses, and modeling approaches, and get feedback about the plan."
  },
  {
    "objectID": "project/proposal.html#what-makes-a-good-project",
    "href": "project/proposal.html#what-makes-a-good-project",
    "title": "Project Proposal Instructions",
    "section": "What Makes A Good Project?",
    "text": "What Makes A Good Project?\nYour project topic should have the following features:\n\nFocus on a data set which can inform a relevant science question;\nInvolve one or more specific hypotheses which can be translated into a simulation model (either numerical or statistical is fine);\nIf only one hypothesis is considered, a meaningful “null” model should be included for comparison."
  },
  {
    "objectID": "project/proposal.html#proposal-guidelines",
    "href": "project/proposal.html#proposal-guidelines",
    "title": "Project Proposal Instructions",
    "section": "Proposal Guidelines",
    "text": "Proposal Guidelines\nThe proposal should have the following components, and be no more than 5 pages (not including references and plots) with 1 inch margins (on all sides) and at least 11 point font.\n\nBackground: What is the scientific context of your project? What question are you trying to address and why does it matter? What are the hypotheses you are focusing on?\nData Overview: Where was the data obtained from? In what context was it collected (e.g. is it observational data, collected in a lab, or reconstructed) and what implications that might have for its use in testing your hypotheses? Include some exploratory analysis and plots. Are there errors included in the data? Will you need any auxiliary data?\nModel Specification: What model(s) are you proposing to use to study the data in the context of your hypotheses? If you’re using a numerical simulation model, what parameters will you treat as uncertain and what is the probability model for the residuals? If you’re using a statistical model, what choices are you making about distributions? Include some initial exploratory analyses to analyze initial goodness-of-fit.\nReferences: Make sure to include any references cited in your proposal."
  },
  {
    "objectID": "exercises.html",
    "href": "exercises.html",
    "title": "Exercises",
    "section": "",
    "text": "This page contains information about and a schedule of the homework assignments for the semester.",
    "crumbs": [
      "Exercises"
    ]
  },
  {
    "objectID": "exercises.html#general-information",
    "href": "exercises.html#general-information",
    "title": "Exercises",
    "section": "General Information",
    "text": "General Information\n\nWhile the instructions for each assignment are available through the linked pages for quick and public access, if you are in the class you must use the link provided in Ed Discussion to accept the assignment. This will ensure that:\n\nYou have compatible versions of all relevant packages provided in the environment;\nYou have a GitHub repository that you can use to share your code.\n\nSubmit assignments by 9:00pm Eastern Time on the due date on Gradescope.\nSubmissions must be PDFs. Make sure that you tag the pages corresponding to each question; points will be deducted otherwise.",
    "crumbs": [
      "Exercises"
    ]
  },
  {
    "objectID": "exercises.html#rubric",
    "href": "exercises.html#rubric",
    "title": "Exercises",
    "section": "Rubric",
    "text": "Rubric\nThe standard rubric for homework problems is available here.",
    "crumbs": [
      "Exercises"
    ]
  },
  {
    "objectID": "exercises.html#schedule",
    "href": "exercises.html#schedule",
    "title": "Exercises",
    "section": "Schedule",
    "text": "Schedule\n\n\n\n\n\n\n\n\n\nWeek\nInstructions\nDue Date\n\n\n\n\n\nWeek 1\n\nFeb 02, 2024\n\n\n\nWeek 3\n\nFeb 09, 2024\n\n\n\nWeek 4\n\nFeb 16, 2024\n\n\n\nWeek 5\n\nFeb 23, 2024\n\n\n\nWeek 7\n\nMar 08, 2024\n\n\n\nWeek 8\n\nMar 15, 2024",
    "crumbs": [
      "Exercises"
    ]
  },
  {
    "objectID": "exercises/ex01/ex01.html",
    "href": "exercises/ex01/ex01.html",
    "title": "Exercise Set 01: Loading, Plotting, and Reasoning About Data",
    "section": "",
    "text": "Due Date\n\n\n\nFriday, 2/2/24, 9:00pm",
    "crumbs": [
      "Exercises",
      "Exercise 1"
    ]
  },
  {
    "objectID": "exercises/ex01/ex01.html#overview",
    "href": "exercises/ex01/ex01.html#overview",
    "title": "Exercise Set 01: Loading, Plotting, and Reasoning About Data",
    "section": "Overview",
    "text": "Overview\n\nInstructions\nThe goal of this exercise is for you to practice (or learn how to) load data from tabular data files, plot it, and do some basic reasoning about the data and relationships between variables.\n\n\nLoad Environment\nThe following code loads the environment and makes sure all needed packages are installed. This should be at the start of most Julia scripts.\n\n\nimport Pkg\nPkg.activate(@__DIR__)\nPkg.instantiate()\n\n\nThe following packages are included in the environment (to help you find other similar packages in other languages). The code below loads these packages for use in the subsequent notebook (the desired functionality for each package is commented next to the package).\n\n\nusing DataFrames # tabular data structure\nusing CSVFiles # reads/writes .csv files\nusing Plots # plotting library\nusing StatsBase # statistical quantities like mean, median, etc\nusing StatsPlots # some additional statistical plotting tools",
    "crumbs": [
      "Exercises",
      "Exercise 1"
    ]
  },
  {
    "objectID": "exercises/ex01/ex01.html#problems",
    "href": "exercises/ex01/ex01.html#problems",
    "title": "Exercise Set 01: Loading, Plotting, and Reasoning About Data",
    "section": "Problems",
    "text": "Problems\nThe goal of this exercise is for you to visualize and reason about the relationship between global mean surface temperature and global mean sea levels.\nProblems 1 and 2 are both marked out of 5 points.",
    "crumbs": [
      "Exercises",
      "Exercise 1"
    ]
  },
  {
    "objectID": "exercises/ex01/ex01.html#references",
    "href": "exercises/ex01/ex01.html#references",
    "title": "Exercise Set 01: Loading, Plotting, and Reasoning About Data",
    "section": "References",
    "text": "References",
    "crumbs": [
      "Exercises",
      "Exercise 1"
    ]
  },
  {
    "objectID": "exercises/ex04/ex04.html",
    "href": "exercises/ex04/ex04.html",
    "title": "Exercise Set 04: Distributions and Extreme Values",
    "section": "",
    "text": "Due Date\n\n\n\nFriday, 2/16/24, 9:00pm\nHow well do they fit the distribution of maxima (plot the fitted distributions over your histogram in one or more plots, use Q-Q plots, etc.)? What is the return level associated with an exceedance probability of 0.01, and how does this compare to the observed quantile? Which distributional fit do you prefer and why?",
    "crumbs": [
      "Exercises",
      "Exercise 4"
    ]
  },
  {
    "objectID": "exercises/ex04/ex04.html#overview",
    "href": "exercises/ex04/ex04.html#overview",
    "title": "Exercise Set 04: Distributions and Extreme Values",
    "section": "Overview",
    "text": "Overview\n\nInstructions\nThe goal of this exercise is for you to explore how distributional assumptions can influence estimates of extreme values and return levels.\n\n\nLoad Environment\nThe following code loads the environment and makes sure all needed packages are installed. This should be at the start of most Julia scripts.\n\n\nimport Pkg\nPkg.activate(@__DIR__)\nPkg.instantiate()\n\n\nThe following packages are included in the environment (to help you find other similar packages in other languages). The code below loads these packages for use in the subsequent notebook (the desired functionality for each package is commented next to the package).\n\n\nusing DataFrames # tabular data structure\nusing Distributions # API to work with statistical distributions\nusing Plots # plotting library\nusing StatsBase # statistical quantities like mean, median, etc\nusing StatsPlots # some additional statistical plotting tools\nusing Optim # optimization tools",
    "crumbs": [
      "Exercises",
      "Exercise 4"
    ]
  },
  {
    "objectID": "exercises/ex04/ex04.html#problems",
    "href": "exercises/ex04/ex04.html#problems",
    "title": "Exercise Set 04: Distributions and Extreme Values",
    "section": "Problems",
    "text": "Problems\n\nProblem 1\nWe represent an experimental data-generating process by finding the maximum value of 100 draws from \\(\\mathcal{N}(2.5, 10)\\). Repeat this experiment 1,000 times and plot the histogram of resulting maxima. What can you conclude about the distribution of the maxima?",
    "crumbs": [
      "Exercises",
      "Exercise 4"
    ]
  },
  {
    "objectID": "exercises/ex04/ex04.html#references",
    "href": "exercises/ex04/ex04.html#references",
    "title": "Exercise Set 04: Distributions and Extreme Values",
    "section": "References",
    "text": "References",
    "crumbs": [
      "Exercises",
      "Exercise 4"
    ]
  },
  {
    "objectID": "exercises/ex05/ex05.html",
    "href": "exercises/ex05/ex05.html",
    "title": "Exercise Set 05: Good and Bad Visualizations",
    "section": "",
    "text": "Due Date\n\n\n\nFriday, 2/23/24, 9:00pm",
    "crumbs": [
      "Exercises",
      "Exercise 5"
    ]
  },
  {
    "objectID": "exercises/ex05/ex05.html#overview",
    "href": "exercises/ex05/ex05.html#overview",
    "title": "Exercise Set 05: Good and Bad Visualizations",
    "section": "Overview",
    "text": "Overview\n\nInstructions\nThe goal of this exercise is for you to find and evaluate data visualizations which you think do a particularly good and bad job.\n\n\nLoad Environment\nThe following code loads the environment and makes sure all needed packages are installed. This should be at the start of most Julia scripts.\n\n\nimport Pkg\nPkg.activate(@__DIR__)\nPkg.instantiate()\n\n\nThe following packages are included in the environment (to help you find other similar packages in other languages). The code below loads these packages for use in the subsequent notebook (the desired functionality for each package is commented next to the package).\n\n\nusing DataFrames # tabular data structure\nusing CSVFiles # reads/writes .csv files\nusing Plots # plotting library\nusing StatsBase # statistical quantities like mean, median, etc\nusing StatsPlots # some additional statistical plotting tools",
    "crumbs": [
      "Exercises",
      "Exercise 5"
    ]
  },
  {
    "objectID": "exercises/ex05/ex05.html#problem",
    "href": "exercises/ex05/ex05.html#problem",
    "title": "Exercise Set 05: Good and Bad Visualizations",
    "section": "Problem",
    "text": "Problem\nFind an example of a data visualization (could be from any reasonable source: journalism, a scientific paper, generated from data) that you think does a particularly good job of communicating something about the underlying data, and one which does a particularly bad job. Write a brief summary (one paragraph) for each about what the visualization is trying to communicate and what makes it (in)effective. Make sure to include where I can find a raw version of the figure (or if you generated it yourself) which is higher-resolution for use in the class discussion next week.",
    "crumbs": [
      "Exercises",
      "Exercise 5"
    ]
  },
  {
    "objectID": "exercises/ex05/ex05.html#references",
    "href": "exercises/ex05/ex05.html#references",
    "title": "Exercise Set 05: Good and Bad Visualizations",
    "section": "References",
    "text": "References",
    "crumbs": [
      "Exercises",
      "Exercise 5"
    ]
  },
  {
    "objectID": "tutorials/turing-mcmc.html",
    "href": "tutorials/turing-mcmc.html",
    "title": "Markov Chain Monte Carlo With Turing",
    "section": "",
    "text": "This tutorial will give some examples of using Turing.jl and Markov Chain Monte Carlo to sample from posterior distributions.",
    "crumbs": [
      "Julia Tutorials",
      "MCMC with Turing"
    ]
  },
  {
    "objectID": "tutorials/turing-mcmc.html#overview",
    "href": "tutorials/turing-mcmc.html#overview",
    "title": "Markov Chain Monte Carlo With Turing",
    "section": "",
    "text": "This tutorial will give some examples of using Turing.jl and Markov Chain Monte Carlo to sample from posterior distributions.",
    "crumbs": [
      "Julia Tutorials",
      "MCMC with Turing"
    ]
  },
  {
    "objectID": "tutorials/turing-mcmc.html#setup",
    "href": "tutorials/turing-mcmc.html#setup",
    "title": "Markov Chain Monte Carlo With Turing",
    "section": "Setup",
    "text": "Setup\n\nusing Turing\nusing Distributions\nusing Plots\ndefault(fmt = :png) # the tide gauge data is long, this keeps images a manageable size\nusing LaTeXStrings\nusing StatsPlots\nusing Measures\nusing StatsBase\nusing Optim\nusing Random\nusing DataFrames\nusing DataFramesMeta\nusing Dates\nusing CSV\n\nAs this tutorial involves random number generation, we will set a random seed to ensure reproducibility.\n\nRandom.seed!(1);",
    "crumbs": [
      "Julia Tutorials",
      "MCMC with Turing"
    ]
  },
  {
    "objectID": "tutorials/turing-mcmc.html#fitting-a-linear-regression-model",
    "href": "tutorials/turing-mcmc.html#fitting-a-linear-regression-model",
    "title": "Markov Chain Monte Carlo With Turing",
    "section": "Fitting A Linear Regression Model",
    "text": "Fitting A Linear Regression Model\nLet’s start with a simple example: fitting a linear regression model to simulated data.\n\n\n\n\n\n\nPositive Control Tests\n\n\n\nSimulating data with a known data-generating process and then trying to obtain the parameters for that process is an important step in any workflow.\n\n\n\nSimulating Data\nThe data-generating process for this example will be: \\[\n\\begin{gather}\ny = 5 + 2x + \\varepsilon \\\\\n\\varepsilon \\sim \\text{Normal}(0, 3),\n\\end{gather}\n\\] where \\(\\varepsilon\\) is so-called “white noise”, which adds stochasticity to the data set. The generated dataset is shown in Figure 1.\n\n\n\n\n\n\n\n\nFigure 1: Scatterplot of our generated data.\n\n\n\n\n\nModel Specification\nThe statistical model for a standard linear regression problem is \\[\n\\begin{gather}\ny = a + bx + \\varepsilon \\\\\n\\varepsilon \\sim \\text{Normal}(0, \\sigma).\n\\end{gather}\n\\]\nRearranging, we can rewrite the likelihood function as: \\[y \\sim \\text{Normal}(\\mu, \\sigma),\\] where \\(\\mu = a + bx\\). This means that we have three parameters to fit: \\(a\\), \\(b\\), and \\(\\sigma^2\\).\nNext, we need to select priors on our parameters. We’ll use relatively generic distributions to avoid using the information we have (since we generated the data ourselves), but in practice, we’d want to use any relevant information that we had from our knowledge of the problem. Let’s use relatively diffuse normal distributions for the trend parameters \\(a\\) and \\(b\\) and a half-normal distribution (a normal distribution truncated at 0, to only allow positive values) for the variance \\(\\sigma^2\\), as recommended by Gelman (2006).\n\nGelman, A. (2006). Prior distributions for variance parameters in hierarchical models (comment on article by Browne and Draper). Bayesian Anal., 1(3), 515–533. https://doi.org/10.1214/06-BA117A\n\\[\n\\begin{gather}\na \\sim \\text{Normal(0, 10)} \\\\\nb \\sim \\text{Normal(0, 10)} \\\\\n\\sigma \\sim \\text{Half-Normal}(0, 25)\n\\end{gather}\n\\]\n\n\nUsing Turing\n\nCoding the Model\nTuring.jl uses the @model macro to specify the model function. We’ll follow the setup in the Turing documentation.\nTo specify distributions on parameters (and the data, which can be thought of as uncertain parameters in Bayesian statistics), use a tilde ~, and use equals = for transformations (which we don’t have in this case).\n\n@model function linear_regression(x, y)\n    # set priors\n1    σ ~ truncated(Normal(0, 25); lower=0)\n2    a ~ Normal(0, 10)\n    b ~ Normal(0, 10)\n\n    # compute the likelihood\n3    for i = 1:length(y)\n        # compute the mean value for the data point\n        μ = a + b * x[i]\n        y[i] ~ Normal(μ, σ)\n    end\nend\n\n\n1\n\nStandard deviations must be positive, so we use a normal distribution truncated at zero.\n\n2\n\nWe’ll keep these both relative uninformative to reflect a more “realistic” modeling scenario.\n\n3\n\nIn this case, we specify the likelihood with a loop. We could also rewrite this as a joint likelihood over all of the data using linear algebra, which might be more efficient for large and/or complex models or datasets, but the loop is more readable in this simple case.\n\n\n\n\nlinear_regression (generic function with 2 methods)\n\n\n\n\nFitting The Model\nNow we can call the sampler to draw from the posterior. We’ll use the No-U-Turn sampler (Hoffman & Gelman, 2014), which is a Hamiltonian Monte Carlo algorithm (a different category of MCMC sampler than the Metropolis-Hastings algorithm discussed in class). We’ll also use 4 chains so we can test that the chains are well-mixed, and each chain will be run for 5,000 iterations1\n\nHoffman, M. D., & Gelman, A. (2014). The No-U-Turn sampler: Adaptively setting path lengths in Hamiltonian Monte Carlo. J. Mach. Learn. Res., 15(47), 1593–1623.\n1 Hamiltonian Monte Carlo samplers often need to be run for fewer iterations than Metropolis-Hastings samplers, as the exploratory step uses information about the gradient of the statistical model, versus the random walk of Metropolis-Hastings. The disadvantage is that this gradient information must be available, which is not always the case for external simulation models. Simulation models coded in Julia can usually be automatically differentiated by Turing’s tools, however.\n# set up the sampler\n1model = linear_regression(x, y)\n2n_chains = 4\n3n_per_chain = 5000\n4chain = sample(model, NUTS(), MCMCThreads(), n_per_chain, n_chains, drop_warmup=true)\n5@show chain\n\n\n1\n\nInitialize the model with the data.\n\n2\n\nWe use multiple chains to help diagnose convergence.\n\n3\n\nThis sets the number of iterations for each chain.\n\n4\n\nSample from the posterior using NUTS and drop the iterations used to warmup the sampler. The MCMCThreads() call tells the sampler to use available processor threads for the multiple chains, but it will just sample them in serial if only one thread exists.\n\n5\n\nThe @show macro makes the display of the output a bit cleaner.\n\n\n\n\n┌ Warning: Only a single thread available: MCMC chains are not sampled in parallel\n└ @ AbstractMCMC ~/.julia/packages/AbstractMCMC/Es490/src/sample.jl:307\n┌ Info: Found initial step size\n└   ϵ = 0.003125\n┌ Info: Found initial step size\n└   ϵ = 0.003125\n┌ Info: Found initial step size\n└   ϵ = 0.00625\n┌ Info: Found initial step size\n└   ϵ = 0.000390625\nSampling (1 threads):  50%|██████████████▌              |  ETA: 0:00:00Sampling (1 threads): 100%|█████████████████████████████| Time: 0:00:01\n\n\nchain = MCMC chain (5000×15×4 Array{Float64, 3})\n\n\n\nChains MCMC chain (5000×15×4 Array{Float64, 3}):\nIterations        = 1001:1:6000\nNumber of chains  = 4\nSamples per chain = 5000\nWall duration     = 8.57 seconds\nCompute duration  = 7.08 seconds\nparameters        = σ, a, b\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\nSummary Statistics\n  parameters      mean       std      mcse    ess_bulk    ess_tail      rhat   ⋯\n      Symbol   Float64   Float64   Float64     Float64     Float64   Float64   ⋯\n           σ    5.3995    0.9907    0.0106   8907.8053   8357.6551    1.0008   ⋯\n           a    7.3413    2.1497    0.0228   8980.2158   9558.6500    1.0006   ⋯\n           b    1.7991    0.1916    0.0020   8980.6008   9477.6142    1.0006   ⋯\n                                                                1 column omitted\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n           σ    3.8939    4.6953    5.2650    5.9393    7.7752\n           a    2.9639    5.9700    7.3826    8.7456   11.5116\n           b    1.4242    1.6757    1.7948    1.9202    2.1879\n\n\n\n\nHow can we interpret the output? The first parts of the summary statistics are straightforward: we get the mean, standard deviation, and Monte Carlo standard error (mcse) of each parameter. We also get information about the effective sample size (ESS)2 and \\(\\hat{R}\\), which measures the ratio of within-chain variance and across-chain variance as a check for convergence3.\n2 The ESS reflects the efficiency of the sampler: this is an estimate of the equivalent number of independent samples; the more correlated the samples, the lower the ESS.3 The closer \\(\\hat{R}\\) is to 1, the better.In this case, we can see that we were generally able to recover the “true” data-generating values of \\(\\sigma = 4\\) and \\(b = 2\\), but \\(a\\) is slightly off (the mean is 3, rather than the data-generating value of 5). In fact, there is substantial uncertainty about \\(a\\), with a 95% credible interval of \\((3.1, 11.4)\\) (compared to \\((1.4, 2.2)\\) for \\(b\\)). This isn’t surprising: given the variance of the noise \\(\\sigma^2\\), there are many different intercepts which could fit within that spread.\nLet’s now plot the chains for visual inspection.\n\nplot(chain)\n\n\n\n\n\n\nFigure 2: Output from the MCMC sampler. Each row corresponds to a different parameter: \\(\\sigma\\), \\(a\\), and \\(b\\). Each chain is shown in a different color. The left column shows the sampler traceplots, and the right column the resulting posterior distributions.\n\n\n\n\nWe can see from Figure 2 that our chains mixed well and seem to have converged to similar distributions! The traceplots have a “hairy caterpiller” appearance, suggesting relatively little autocorrelation. We can also see how much more uncertainty there is with the intercept \\(a\\), while the slope \\(b\\) is much more constrained.\nAnother interesting comparison we can make is with the maximum-likelihood estimate (MLE), which we can obtain through optimization.\n\nmle_model = linear_regression(x, y)\n1mle = optimize(mle_model, MLE())\ncoef(mle)\n\n\n1\n\nThis is where we use the Optim.jl package in this tutorial.\n\n\n\n\n\n3-element Named Vector{Float64}\nA  │ \n───┼────────\nσ  │ 4.75545\na  │ 7.65636\nb  │ 1.77736\n\nWe could also get the maximum a posteriori (MAP) estimate, which includes the prior density, by replacing MLE() with MAP().\n\n\n\nModel Diagnostics and Posterior Predictive Checks\nOne advantage of the Bayesian modeling approach here is that we have access to a generative model, or a model which we can use to generate datasets. This means that we can now use Monte Carlo simulation, sampling from our posteriors, to look at how uncertainty in the parameter estimates propagates through the model. Let’s write a function which gets samples from the MCMC chains and generates datasets.\n\nfunction mc_predict_regression(x, chain)\n    # get the posterior samples\n1    a = Array(group(chain, :a))\n    b = Array(group(chain, :b))\n    σ = Array(group(chain, :σ))\n\n    # loop and generate alternative realizations\n    μ = a' .+ x * b'\n    y = zeros((length(x), length(a)))\n    for i = 1:length(a)\n        y[:, i] = rand.(Normal.(μ[:, i], σ[i]))\n    end\n    return y\nend\n\n\n1\n\nThe Array(group()) syntax is more general than we need, but is useful if we have multiple variables which were sampled as a group, for example multiple regression coefficients. Otherwise, we can just use e.g. Array(chain, :a).\n\n\n\n\nmc_predict_regression (generic function with 1 method)\n\n\nNow we can generate a predictive interval and median and compare to the data.\n\nx_pred = 0:20\ny_pred = mc_predict_regression(x_pred, chain)\n\n21×20000 Matrix{Float64}:\n -2.40865    1.9357    10.5589    15.7446   …   6.26661    0.222214  10.1702\n 10.7976    18.8545     0.296641   3.38924     10.5971    10.8189    15.9384\n -0.417529  -0.885769   5.56482    7.39414     -0.981201   7.27796    6.9013\n  4.33488   11.1663     9.51384    9.95352      5.88792   10.313     11.4574\n  5.26926    5.42713   20.2392    13.7574      17.4582    10.5193     8.7948\n 15.825     16.9226    19.3498    28.6916   …  13.6763    15.6275     6.24528\n 16.504     14.0514    13.6398    18.3671      14.349     18.3797    14.9837\n 23.6586    26.4983    29.1236    21.162       20.1668    20.2031    28.3504\n 16.5461    23.2524    20.7667    22.3589      12.5181     9.4516    12.7805\n 32.7533    13.6189    17.0692    26.5378      26.2872    16.5962    26.4759\n 17.9741    32.456     28.7925    25.5402   …  29.5071    30.4289    14.6678\n 27.0943    25.5964    20.0313    27.9132      33.8335    28.1729    26.8888\n 30.6918    18.5685    30.0918    34.6757      23.5247    27.7565    28.9372\n 27.8009    39.4466    34.1512    34.7717      30.2555    36.9157    26.2247\n 26.9089    34.0929    36.0757    39.6863      28.6581    33.8906    35.4109\n 44.642     31.9187    37.0507    25.4562   …  34.3312    27.4952    23.2894\n 47.9252    36.3149    34.642     37.9717      41.1715    34.784     36.6823\n 39.9288    27.0537    33.4583    42.8428      41.3582    32.8539    31.7867\n 44.4605    34.1545    46.2137    35.4133      42.2067    39.4086    36.1819\n 43.712     42.4212    41.7049    50.7652      51.3167    33.0422    49.135\n 45.9144    43.2963    52.2372    50.891    …  39.7291    52.5323    46.89\n\n\nNotice the dimension of y_pred: we have 20,000 columns, because we have 4 chains with 5,000 samples each. If we had wanted to subsample (which might be necessary if we had hundreds of thousands or millions of samples), we could have done that within mc_linear_regression before simulation.\n\n# get the boundaries for the 95% prediction interval and the median\ny_ci_low = quantile.(eachrow(y_pred), 0.025)\ny_ci_hi = quantile.(eachrow(y_pred), 0.975)\ny_med = quantile.(eachrow(y_pred), 0.5)\n\nNow, let’s plot the prediction interval and median, and compare to the original data.\n\n# plot prediction interval\n1plot(x_pred, y_ci_low, fillrange=y_ci_hi, xlabel=L\"$x$\", ylabel=L\"$y$\", fillalpha=0.3, fillcolor=:blue, label=\"95% Prediction Interval\", legend=:topleft, linealpha=0)\n2plot!(x_pred, y_med, color=:blue, label=\"Prediction Median\")\n3scatter!(x, y, color=:red, label=\"Data\")\n\n\n1\n\nPlot the 95% posterior prediction interval as a shaded blue ribbon.\n\n2\n\nPlot the posterior prediction median as a blue line.\n\n3\n\nPlot the data as discrete red points.\n\n\n\n\n\n\n\n\n\nFigure 3: Posterior 95% predictive interval and median for the linear regression model. The data is plotted in red for comparison.\n\n\n\n\nFrom Figure 3, it looks like our model might be slightly under-confident, as with 20 data points, we would expect 5% of them (or 1 data point) to be outside the 95% prediction interval. It’s hard to tell with only 20 data points, though! We could resolve this by tightening our priors, but this depends on how much information we used to specify them in the first place. The goal shouldn’t be to hit a specific level of uncertainty, but if there is a sound reason to tighten the priors, we could do so.\nNow let’s look at the residuals from the posterior median and the data. The partial autocorrelations plotted in Figure 4 are not fully convincing, as there are large autocorrelation coefficients with long lags, but the dataset is quite small, so it’s hard to draw strong conclusions. We won’t go further down this rabbit hole as we know our data-generating process involved independent noise, but for a real dataset, we might want to try a model specification with autocorrelated errors to compare.\n\n\n# calculate the median predictions and residuals\ny_pred_data = mc_predict_regression(x, chain)\ny_med_data = quantile.(eachrow(y_pred_data), 0.5)\nresiduals = y_med_data .- y\n\n# plot the residuals and a line to show the zero\nplot(pacf(residuals, 1:4), line=:stem, marker=:circle, legend=:false, grid=:false, linewidth=2, xlabel=\"Lag\", ylabel=\"Partial Autocorrelation\", markersize=8, tickfontsize=14, guidefontsize=16, legendfontsize=16)\nhline!([0], linestyle=:dot, color=:red)\n\n\n\n\n\n\nFigure 4: Partial autocorrelation function of model residuals, relative to the predictive median.",
    "crumbs": [
      "Julia Tutorials",
      "MCMC with Turing"
    ]
  },
  {
    "objectID": "tutorials/turing-mcmc.html#fitting-extreme-value-models-to-tide-gauge-data",
    "href": "tutorials/turing-mcmc.html#fitting-extreme-value-models-to-tide-gauge-data",
    "title": "Markov Chain Monte Carlo With Turing",
    "section": "Fitting Extreme Value Models to Tide Gauge Data",
    "text": "Fitting Extreme Value Models to Tide Gauge Data\nLet’s now look at an example of fitting an extreme value distribution (namely, a generalized extreme value distribution, or GEV) to tide gauge data. GEV distributions have three parameters:\n\n\\(\\mu\\), the location parameter, which reflects the positioning of the bulk of the GEV distribution;\n\\(\\sigma\\), the scale parameter, which reflects the width of the bulk;\n\\(\\xi\\), the shape parameter, which reflects the thickness and boundedness of the tail.\n\nThe shape parameter \\(\\xi\\) is often of interest, as there are three classes of GEV distributions corresponding to different signs:\n\n\\(\\xi &lt; 0\\) means that the distribution is bounded;\n\\(\\xi = 0\\) means that the distribution has a thinner tail, so the “extreme extremes” are less likely;\n\\(\\xi &gt; 0\\) means that the distribution has a thicker tail.\n\n\nLoad Data\nFirst, let’s load the data. We’ll use data from the University of Hawaii Sea Level Center (Caldwell et al., 2015) for San Francisco, from 1897-2013. If you don’t have this data and are working with the notebook, download it here. We’ll assume it’s in a data/ subdirectory, but change the path as needed.\n\nCaldwell, P. C., Merrifield, M. A., & Thompson, P. R. (2015). Sea level measured by tide gauges from global oceans — the joint archive for sea level holdings (NCEI accession 0019568). NOAA National Centers for Environmental Information (NCEI). https://doi.org/10.7289/V5V40S7W\nThe dataset consists of dates and hours and the tide-gauge measurement, in mm. We’ll load the dataset into a DataFrame.\n\nfunction load_data(fname)\n    date_format = DateFormat(\"yyyy-mm-dd HH:MM:SS\")\n1    df = @chain fname begin\n2        CSV.File(; delim=',', header=false)\n3        DataFrame\n4        rename(\"Column1\" =&gt; \"year\",\n                \"Column2\" =&gt; \"month\",\n                \"Column3\" =&gt; \"day\",\n                \"Column4\" =&gt; \"hour\",\n                \"Column5\" =&gt; \"gauge\")\n        # need to reformat the decimal date in the data file\n5        @transform :datetime = DateTime.(:year, :month, :day, :hour)\n        # replace -99999 with missing\n6        @transform :gauge = ifelse.(abs.(:gauge) .&gt;= 9999, missing, :gauge)\n7        select(:datetime, :gauge)\n    end\n    return df\nend\n\n\n1\n\nThis uses the DataFramesMeta.jl package, which makes it easy to string together commands to load and process data\n\n2\n\nLoad the file, assuming there is no header.\n\n3\n\nConvert to a DataFrame.\n\n4\n\nRename columns for ease of access.\n\n5\n\nReformat the decimal datetime provided in the file into a Julia DateTime.\n\n6\n\nReplace missing data with missing.\n\n7\n\nSelect only the :datetime and :gauge columns.\n\n\n\n\nload_data (generic function with 1 method)\n\n\n\ndat = load_data(\"data/h551a.csv\")\nfirst(dat, 6)\n\n\n\nTable 1: Processed hourly tide gauge data from San Francisco, from 8/1/1897-1/31/2023.\n\n\n\n\n\n6×2 DataFrame\n\n\n\nRow\ndatetime\ngauge\n\n\n\nDateTime\nInt64?\n\n\n\n\n1\n1897-08-01T08:00:00\n3292\n\n\n2\n1897-08-01T09:00:00\n3322\n\n\n3\n1897-08-01T10:00:00\n3139\n\n\n4\n1897-08-01T11:00:00\n2835\n\n\n5\n1897-08-01T12:00:00\n2377\n\n\n6\n1897-08-01T13:00:00\n2012\n\n\n\n\n\n\n\n\n\n\n1@df dat plot(:datetime, :gauge, label=\"Observations\", bottom_margin=9mm)\nxaxis!(\"Date\", xrot=30)\nyaxis!(\"Mean Water Level\")\n\n\n1\n\nThis uses the DataFrame plotting recipe with the @df macro from StatsPlots.jl. This is not needed (you could replace e.g. :datetime with dat.datetime), but it cleans things up slightly.\n\n\n\n\n\n\n\n\n\nFigure 5: Hourly mean water at the San Francisco tide gauge from 1897-2023.\n\n\n\n\nNext, we need to detrend the data to remove the impacts of sea-level rise. We do this by removing a one-year moving average, centered on the data point, per the recommendation of Arns et al. (2013).\n\n# calculate the moving average and subtract it off\nma_length = 366\nma_offset = Int(floor(ma_length/2))\nmoving_average(series,n) = [mean(@view series[i-n:i+n]) for i in n+1:length(series)-n]\ndat_ma = DataFrame(datetime=dat.datetime[ma_offset+1:end-ma_offset], residual=dat.gauge[ma_offset+1:end-ma_offset] .- moving_average(dat.gauge, ma_offset))\n\n# plot\n@df dat_ma plot(:datetime, :residual, label=\"Detrended Observations\", bottom_margin=9mm)\nxaxis!(\"Date\", xrot=30)\nyaxis!(\"Mean Water Level\")\n\n\n\n\n\n\nFigure 6: Mean water level from the San Francisco tide gauge, detrended using a 1-year moving average centered on the data point, per the recommendation of Arns et al. (2013).\n\n\nArns, A., Wahl, T., Haigh, I. D., Jensen, J., & Pattiaratchi, C. (2013). Estimating extreme water level probabilities: A comparison of the direct methods and recommendations for best practise. Coast. Eng., 81, 51–66. https://doi.org/10.1016/j.coastaleng.2013.07.003\n\n\n\nThe last step in preparing the data is to find the annual maxima. We can do this using the groupby, transform, and combine functions from DataFrames.jl, as below.\n\n# calculate the annual maxima\n1dat_ma = dropmissing(dat_ma)\n2dat_annmax = combine(dat_ma -&gt; dat_ma[argmax(dat_ma.residual), :],\n                groupby(DataFrames.transform(dat_ma, :datetime =&gt; x-&gt;year.(x)), :datetime_function))\n3delete!(dat_annmax, nrow(dat_annmax))\n\n# make a histogram of the maxima to see the distribution\nhistogram(dat_annmax.residual, label=false)\nylabel!(\"Count\")\nxlabel!(\"Mean Water Level (mm)\")\n\n\n1\n\nIf we don’t drop the values which are missing, they will affect the next call to argmax.\n\n2\n\nThis first groups the data based on the year (with groupby and using Dates.year() to get the year of each data point), then pulls the rows which correspond to the maxima for each year (using argmax).\n\n3\n\nThis will delete the last year, in this case 2023, because the dataset only goes until March 2023 and this data point is almost certainly an outlier due to the limited data from that year.\n\n\n\n\n\n\n\n\n\nFigure 7: Histogram of annual block maxima from 1898-2022 from the San Francisco tide gauge dataset.\n\n\n\n\n\n\nFit The Model\n\n@model function gev_annmax(y)               \n1    μ ~ Normal(1000, 100)\n2    σ ~ truncated(Normal(0, 100); lower=0)\n3    ξ ~ Normal(0, 0.5)\n\n4    y ~ GeneralizedExtremeValue(μ, σ, ξ)\nend\n\n5gev_model = gev_annmax(dat_annmax.residual)\n6n_chains = 4\n7n_per_chain = 5000\n8gev_chain = sample(gev_model, NUTS(), MCMCThreads(), n_per_chain, n_chains; drop_warmup=true)\n@show gev_chain\n\n\n1\n\nLocation parameter prior: We know that this is roughly on the 1000 mm order of magnitude, but want to keep this relatively broad.\n\n2\n\nScale parameter prior: This parameter must be positive, so we use a normal truncated at zero.\n\n3\n\nShape parameter prior: These are usually small and are hard to constrain, so we will use a more informative prior.\n\n4\n\nThe data is independently GEV-distributed as we’ve removed the long-term trend and are using long blocks.\n\n5\n\nInitialize the model.\n\n6\n\nWe use multiple chains to help diagnose convergence.\n\n7\n\nThis sets the number of iterations for each chain.\n\n8\n\nSample from the posterior using NUTS and drop the iterations used to warmup the sampler.\n\n\n\n\n┌ Warning: Only a single thread available: MCMC chains are not sampled in parallel\n└ @ AbstractMCMC ~/.julia/packages/AbstractMCMC/Es490/src/sample.jl:307\n┌ Info: Found initial step size\n└   ϵ = 0.05\n┌ Info: Found initial step size\n└   ϵ = 0.0015625\n┌ Info: Found initial step size\n└   ϵ = 0.000390625\n┌ Info: Found initial step size\n└   ϵ = 0.00625\nSampling (1 threads):  50%|██████████████▌              |  ETA: 0:00:01Sampling (1 threads): 100%|█████████████████████████████| Time: 0:00:02\n\n\ngev_chain = MCMC chain (5000×15×4 Array{Float64, 3})\n\n\n\nChains MCMC chain (5000×15×4 Array{Float64, 3}):\nIterations        = 1001:1:6000\nNumber of chains  = 4\nSamples per chain = 5000\nWall duration     = 5.28 seconds\nCompute duration  = 4.85 seconds\nparameters        = μ, σ, ξ\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\nSummary Statistics\n  parameters        mean       std      mcse     ess_bulk     ess_tail      rh ⋯\n      Symbol     Float64   Float64   Float64      Float64      Float64   Float ⋯\n           μ   1257.8434    5.6421    0.0489   13375.4301   11521.3394    1.00 ⋯\n           σ     57.2113    4.2214    0.0363   13619.4235   13686.7664    1.00 ⋯\n           ξ      0.0295    0.0625    0.0005   14332.1783   11774.3672    1.00 ⋯\n                                                               2 columns omitted\nQuantiles\n  parameters        2.5%       25.0%       50.0%       75.0%       97.5% \n      Symbol     Float64     Float64     Float64     Float64     Float64 \n           μ   1246.9789   1254.0379   1257.7738   1261.5762   1269.0232\n           σ     49.5961     54.2510     57.0043     59.8866     66.0566\n           ξ     -0.0814     -0.0150      0.0258      0.0693      0.1629\n\n\n\n\n\nplot(gev_chain)\n\n\n\n\n\n\nFigure 8: Traceplots (left) and marginal distributions (right) from the MCMC sampler for the GEV model.\n\n\n\n\nFrom Figure 8, it looks like all of the chains have converged to the same distribution; the Gelman-Rubin diagnostic is also close to 1 for all parameters. Next, we can look at a corner plot to see how the parameters are correlated.\n\ncorner(gev_chain)\n\n\n\n\n\n\nFigure 9: Corner plot for the GEV model.\n\n\n\n\nFigure 9 suggests that the location and scale parameters \\(\\mu\\) and \\(\\sigma\\) are positively correlated. This makes some intuitive sense, as increasing the location parameter shifts the bulk of the distribution in a positive direction, and the increasing scale parameter then increases the likelihood of lower values. However, if these parameters are increased, the shape parameter \\(\\xi\\) decreases, as the tail of the GEV does not need to be as thick due to the increased proximity of outliers to the bulk.",
    "crumbs": [
      "Julia Tutorials",
      "MCMC with Turing"
    ]
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "This is a 3 credit course offered as an elective.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#learning-outcomes",
    "href": "syllabus.html#learning-outcomes",
    "title": "Syllabus",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nAfter completing this class, students will be able to:\n\ncreate, interpret, and critique data visualizations;\ncalibrate environmental models to observations, possibly including censored and missing data;\nsimulate alternative datasets from models using statistical methods such as the bootstrap and Monte Carlo;\nassess model adequacy and performance using predictive simulations;\napply and contextualize model selection criteria;\nevaluate evidence for and against hypotheses about environmental systems using model simulations;\nemulate computationally-complex models with simpler representations.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#prerequisites-preparation",
    "href": "syllabus.html#prerequisites-preparation",
    "title": "Syllabus",
    "section": "Prerequisites & Preparation",
    "text": "Prerequisites & Preparation\nThe following courses/material would be ideal preparation:\n\nOne course in programming (e.g. CS 1110, 1112 or ENGRD/CEE 3200)\nOne course in probability or statistics (ENGRD 2700, CEE 3040, or equivalent)\n\nIn the absence of one or more these prerequisites, you can seek the permission of instructor.\n\n\n\n\n\n\nWhat If My Programming or Stats Skills Are Rusty?\n\n\n\nIf your programming or statistics skills are a little rusty, don’t worry! We will review concepts and build skills as needed.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#typical-topics",
    "href": "syllabus.html#typical-topics",
    "title": "Syllabus",
    "section": "Typical Topics",
    "text": "Typical Topics\n\nIntroduction to exploratory data analysis;\nReview of probability and statistics;\nBayesian decision theory;\nPrinciples of data visualization;\nModel residuals and discrepancies;\nCensored, truncated, and missing data;\nStatistical methods for calibration;\nPredictive model assessment;\nEmulation with surrogate models",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-meetings",
    "href": "syllabus.html#course-meetings",
    "title": "Syllabus",
    "section": "Course Meetings",
    "text": "Course Meetings\nThis course meets MWF from 11:40–12:55 in 160 Riley-Robb Hall. In addition to the course meetings (a total of 42 lectures, 50 minutes each), the final project will be due during the university finals period. In addition to the work during the semester, students can expect to devote, on average, 6 hours of effort during the exam period.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-philosophy-and-expectations",
    "href": "syllabus.html#course-philosophy-and-expectations",
    "title": "Syllabus",
    "section": "Course Philosophy and Expectations",
    "text": "Course Philosophy and Expectations\nThe goal of our course is to help you gain competancy and knowledge in the area of data analysis. This involves a dual responsibility on the part of the instructor and the student. As the instructor, my responsibility is to provide you with a structure and opportunity to learn. To this end, I will commit to:\n\nprovide organized and focused lectures, in-class activities, and assignments;\nencourage students to regularly evaluate and provide feedback on the course;\nmanage the classroom atmosphere to promote learning;\nschedule sufficient out-of-class contact opportunities, such as office hours;\nallow adequate time for assignment completion;\nmake lecture materials, class policies, activities, and assignments accessible to students.\n\nI encourage you to discuss any concerns with me during office hours or through a course communications channel! Please let me know if you do not feel that I am holding up my end of the bargain.\nStudents can optimize their performance in the course by:\n\nattending all lectures;\ndoing any required preparatory work before class;\nactively participating in online and in-class discussions;\nbeginning assignments and other work early;\nand attending office hours as needed.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#community",
    "href": "syllabus.html#community",
    "title": "Syllabus",
    "section": "Community",
    "text": "Community\n\nDiversity and Inclusion\nOur goal in this class is to foster an inclusive learning environment and make everyone feel comfortable in the classroom, regardless of social identity, background, and specific learning needs. As engineers, our work touches on many critical aspects of society, and questions of inclusion and social justice cannot be separated from considerations of how data are generated, collected, and analyzed.\nIn all communications and interactions with each other, members of this class community (students and instructors) are expected to be respectful and inclusive. In this spirit, we ask all participants to:\n\nshare their experiences, values, and beliefs;\nbe open to and respectful of the views of others; and\nvalue each other’s opinions and communicate in a respectful manner.\n\nPlease let me know if you feel any aspect(s) of class could be made more inclusive. Please also share any preferred name(s) and/or your pronouns with me if you wish: I use he/him/his, and you can refer to me either as Vivek or Prof. Srikrishnan.\n\n\n\n\n\n\nPlease, Be Excellent To Teach Other\n\n\n\nWe all make mistakes in our communications with one another, both when speaking and listening. Be mindful of how spoken or written language might be misunderstood, and be aware that, for a variety of reasons, how others perceive your words and actions may not be exactly how you intended them. At the same time, it is also essential that we be respectful and interpret each other’s comments and actions in good faith.\n\n\n\n\nStudent Accomodations\nLet me know if you have any access barriers in this course, whether they relate to course materials, assignments, or communications. If any special accomodations would help you navigate any barriers and improve your chances of success, please exercise your right to those accomodations and reach out to me as early as possible with your Student Disability Services (SDS) accomodation letter. This will ensure that we have enough time to make appropriate arrangements.\n\n\n\n\n\n\nIf you need more immediate accomodations, but do not yet have a letter, please let me know and then follow up with SDS.\n\n\n\n\n\nCourse Communications\nMost course communications will occur via Ed Discussion. Public Ed posts are generally preferred to private posts or emails, as other students can benefit from the discussions. If you would like to discuss something privately, please do reach out through email or a private Ed post (which will only be viewable by you and the course staff).\nAnnouncements will be made on the course website and in Ed.\n\n\n\n\n\n\nEd Tips\n\n\n\n\nIf you wait until the day an assignment is due (or even late the previous night) to ask a question on Ed, there is a strong chance that I will not see your post prior to the deadline.\nBut if you see unanswered questions and you have some insight, please answer! This class will work best when we all work together as a community.\n\n\n\n\n\nMental Health Resources\nWe all have to take care of our mental health, just as we would our physical health. As a student, you may experience a range of issues which can negatively impact your mental health. Please do not ignore any of these stressors, or feel like you have to navigate these challenges alone! You are part of a community of students, faculty, and staff, who have a responsibility to look for one another’s well-being. If you are struggling with managing your mental health, or if you believe a classmate may be struggling, please reach out to the course instructor, the TA, or, for broader support, please take advantage of Cornell’s mental health resources.\n\n\n\n\n\n\nMental Health And This Class\n\n\n\nI am not a trained counselor, but I am here to support you in whatever capacity we can. You should never feel that you need to push yourself past your limits to complete any assignment for this class or any other. If we need to make modifications to the course or assignment schedule, you can certainly reach out to me, and all relevant discussions will be kept strictly confidential.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-policies",
    "href": "syllabus.html#course-policies",
    "title": "Syllabus",
    "section": "Course Policies",
    "text": "Course Policies\n\nAttendance\nAttendance is not required, but in general, students who attend class regularly will do better and get more out of the class than students who do not. Your class participation grade will reflect both the quantity and quality of your participation, only some of which can occur asynchronously. I will put as many course materials, such as lecture notes and announcements, as possible online, but viewing materials online is not the same as active participation and engagement. Life happens, of course, and this may lead you to miss class. Let me know if you need any appropriate arrangements ahead of time.\n\n\n\n\n\n\nWhat If I’m Sick?\n\n\n\nPlease stay home if you’re feeling sick! This is beneficial for both for your own recovery and the health and safety of your classmates. We will also make any necessary arrangements for you to stay on top of the class material and if whatever is going on will negatively impact your grade, for example by causing you to be unable to submit an assignment on time.\n\n\n\n\nOffice Hours\nOffice hours will be held in 318 Riley-Robb before class on MW, from 10-11AM, and after class on M, from 1-2PM, in 318 Riley-Robb Hall. Some time will be available after class for brief questions, but this is limited. If these times do not work for you, or you need some additional time outside of office hours, please reach out to Prof. Srikrishnan about scheduling a meeting. Depending on schedules, these requests may not be accepted on short notice (e.g. homework is due on Friday and you reach out late on Thursday), but with several days notice we should be able to find a time that will work.\nOffice hours are intended to help all students who attend. This time is limited, and is best spent on issues that are relevant to as many students as possible. While we will do our best to answer individual questions, students asking us to verify or debug homework solutions will have the lowest priority (but please do ask about how to verify or debug your own solutions!). However, we are happy to discuss conceptual approaches to solving homework problems, which may help to reveal bugs.\nSpace at office hours can be limited (we may shift to the conference room in 316 Riley-Robb if offices are full and it is available). If the room is crowded and you can find an alternative source of assistance, or if your question is low priority (e.g. debugging) please be kind and make room for others.\n\n\nMask Policies\nMasks are encouraged but not required in the classroom, per university policy. However, the University strongly encourages compliance with requests to mask from students, faculty, and staff who are concerned about the risk of infection. Please be respectful of these concerns and requests if you cannot wear a mask.\n\n\nAcademic Integrity\n\n\n\n\n\n\nTL;DR: Don’t cheat, copy, or plagiarize!\n\n\n\nThis class is designed to encourage collaboration, and students are encouraged to discuss their work with other students. However, I expect students to abide by the Cornell University Code of Academic Integrity in all aspects of this class. All work submitted must represent the students’ own work and understanding, whether individually or as a group (depending on the particulars of the assignment). This includes analyses, code, software runs, and reports. Engineering as a profession relies upon the honesty and integrity of its practitioners (see e.g. the American Society for Civil Engineers’ Code of Ethics).\n\n\nExternal Resources\nThe collaborative environment in this class should not be viewed as an invitation for plagiarism. Plagiarism occurs when a writer intentionally misrepresents another’s words or ideas (including code!) as their own without acknowledging the source. All external resources which are consulted while working on an assignment should be referenced, including other students and faculty with whom the assignment is discussed. You will never be penalized for consulting an external source for help and referencing it, but plagiarism will result in a zero for that assignment as well as the potential for your case to be passed on for additional disciplinary action.\n\n\nAI/ML Resource Policy\nAs noted, all work submitted for a grade in this course must reflect your own understanding. The use and consulation of AI/ML tools, such as ChatGPT or similar, must be pre-approved and clearly referenced. If approved, you must:\n\nreference the URL of the service you are using, including the specific date you accessed it;\nprovide the exact query or queries used to interact with the tool; and\nreport the exact response received.\n\nFailure to attain prior approval or fully reference the interaction, as described above, will be treated as plagiarism and referred to the University accordingly.\n\n\nLate Work Policy\nIn general, late work can be submitted up to 24 hours after the due date at a 50% penalty. However, sometimes things come up in life. Please reach out ahead of time if you have extenuating circumstances (including University-approved absences or illnesses) which would make it difficult for you to submit your work on time. Note that e.g. job interviews or a busy schedule outside of this course are not valid reasons for extensions. If an extension is granted, any late penalties will be waived up to the extension date. In extreme circumstances, assignments can be forgiven, and your grade will be computed as though those did not occur, giving your other assignments more weight.\n\n\nRegrade Requests\nRegrade requests can be submitted up to one week after the graded work is released on Gradescope.\nAll regrade requests must include a brief justification for the request or they will not be considered. Good justifications include (but are not limited to): - My answer agrees with the posted solution, but I still lost points. - I lost 4 points for something, but the rubric says it should only be worth 2 points. - You took points off for something, but it’s right here. - My answer is correct, even though it does not match the posted solution; here is an explanation. - There is no explanation for my grade. - I got a perfect score, but my solution has a mistake (you will receive extra credit for this! see below!) - There is a major error in the posted solution; here is an explanation (full credit for everyone, but Prof. Srikrishnan will decide what constitutes a “major error”! see below!).\n\n\n\n\n\n\nWe Can Only Grade What You Submitted\n\n\n\nAll regrades will be assessed based only on the submitted work. You cannot get a higher grade by explanation what you meant (either in person or online) or by adding information or reasoning to what is submitted after the fact. The goal of the regrade is to draw attention to a potential grading problem, not to supplement the submission.\n\n\nOnce Prof. Srikrishnan issues a final response to a regrade request, further requests for that submission will be ignored.\n\n\n\n\n\n\nRegrade Requests Can Be A Gamble!\n\n\n\nWhile you should submit regrade requests for legitimate errors, using them for fishing expeditions can also result in lost points if Prof. Srikrishnan decide that your initial grade was too lenient or if additional errors are identified.\n\n\n\n\n\n\n\n\nWhat If I Find A Different Type of Mistake?\n\n\n\n\nIf you submit a regrade request correctly reporting that a problem was graded too leniently — that is, that your score was higher than it should be based on the rubric — your score will be increased by the difference. For example, if your original score on a problem was 8/10 and you successfully argue that your score should have been 3/10, your new score will be 13/10.\nIf a significant error is discovered in a posted homework solution or in the exam solutions, everyone will in the class will receive full credit for the (sub)problem. Prof. Srikrishnan will decide what is “significant”.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#assessments",
    "href": "syllabus.html#assessments",
    "title": "Syllabus",
    "section": "Assessments",
    "text": "Assessments\n\nTechnologies\nWe will use Canvas as a gradebook, and to distribute PDFs of readings (which also be made available through the website, via the Cornell library). Ed Discussion will be used for course communications. Assignments will be submitted and graded in Gradescope.\nStudents can use any programming language they like to solve problems, though we will make notebooks and package environments available for Julia (which may help structure your assignments if you use a different language) via GitHub. If students use a language other than Julia, we may limited in the programming assistance we can provide (though we’re happy to try to help!).\nWe recommend students create a GitHub account and use GitHub to version control and share their code throughout the semester.\n\n\nGrading\nFinal grades will be computed based on the following assessment weights:\n\n\n\nAssessment\nWeight\n\n\n\n\nExercises\n10%\n\n\nReadings\n10%\n\n\nLiterature Critique\n15%\n\n\nHomework Assignments\n30%\n\n\nTerm Project\n35%\n\n\n\nThe following grading scale will be used to convert the numerical weighted average to letter grades:\n\n\n\nGrade\nRange\n\n\n\n\nA\n94–100\n\n\nA-\n90–94\n\n\nB+\n87–90\n\n\nB\n84–87\n\n\nB-\n80–84\n\n\nC+\n77–80\n\n\nC\n74–77\n\n\nC-\n70–74\n\n\nD+\n67–70\n\n\nD\n64–67\n\n\nD-\n61–64\n\n\nF\n&lt; 61\n\n\n\n\n\nExercises\nMost weeks, students will be given a set of exercises (typically involving analyzing a dataset, a model, or a figure) to complete. These will involve a small amount of programming, a minor calculation, and/or visual assessment of data or a figure. Exercises will be provided the previous Monday and are intended to align with the content for the week, and solutions should be submitted by 9:00pm on the Friday at the end of the given week. These exercises will be given as quizzes on Gradescope and will be auto-graded. The lowest exercise score will be dropped automatically.\n\n\nReadings\nReadings will be assigned for discussion throughout the semester. Students are expected to summarize their takeaways and thoughts on the reading and respond to others’ comments on the Ed forum during the week after the reading is assigned (typically Monday – Monday). Collaborative annotation assignments will be set up in Canvas to facilitate reading ahead of the discussion on the Ed forum. For key readings, a student in BEE 5850 will be assigned to lead a 35 minute in-class discussion on the reading.\n\n\nLiterature Critique\nStudents will select a peer-reviewed journal article related to an application of data analysis and will write a short discussion paper analyzing the hypotheses and statistical choices. Students enrolled in BEE 5850 will also write a referee report, as if they were a peer reviewer. Students will give an in-class presentation of their paper during one class period. Other students will be asked to submit evaluations of the presentations.\n\n\nHomework Assignments\nApproximately 6 homework assignments will be assigned throughout the semester (roughly one per course module). You will typically have 2 weeks to work on each assignment, though this depends on the module length. Students are encouraged to collaborate and learn from each other on homework assignments, but each student must submit their own solutions reflecting their understanding of the material. Consulting and referencing external resources and your peers is encouraged (engineering is a collaborative discipline!), but plagiarism is a violation of academic integrity.\nSome notes on assignment and grading logistics:\n\nHomeworks are due by 9:00pm Eastern Time on the designed due date. Your assignment notebook (which include your writeup and codes) should be submitted to Gradescope as a PDF with the answers to each question tagged (a failure to do this will result in deductions).\nA standard rubric is available.\nStudents in 5850 will be asked to complete additional homework problems which go more deeply into the underlying concepts or apply more advanced techniques.\nRegrade requests for specific problems must be made within a week of the grading of that assignment.\n\n\n\nTerm Project\nThroughout the semester, students will apply the concepts and methods from class to a data set of their choosing. If a student does not have a data set in mind, we will find one which aligns with their interests.\nThe term project can be completed individually or in groups of 2. Students will provide updates throughout the semester corresponding to the various tasks discussed in each module and will submit a final report at the end of the semester. The deliverables are:\n\nA proposal describing the research question and hypotheses, the data set, and the numerical or statistical models the student would like to use to test the hypotheses;\nA simulation study applying simulation methods to the models;\nA final presentation and report.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "Term Project",
    "section": "",
    "text": "The term project gives you an opportunity to use and extend the methods we have learned in class to an environmental data set and/or model of your choosing. More details will be provided over the semester. We will discuss each component in class as well as providing relevant information on this page.",
    "crumbs": [
      "Term Project"
    ]
  },
  {
    "objectID": "project.html#overview",
    "href": "project.html#overview",
    "title": "Term Project",
    "section": "",
    "text": "The term project gives you an opportunity to use and extend the methods we have learned in class to an environmental data set and/or model of your choosing. More details will be provided over the semester. We will discuss each component in class as well as providing relevant information on this page.",
    "crumbs": [
      "Term Project"
    ]
  },
  {
    "objectID": "project.html#overall-instructions",
    "href": "project.html#overall-instructions",
    "title": "Term Project",
    "section": "Overall Instructions",
    "text": "Overall Instructions\n\nStudents can work individually or in groups of 2.",
    "crumbs": [
      "Term Project"
    ]
  },
  {
    "objectID": "project.html#schedule",
    "href": "project.html#schedule",
    "title": "Term Project",
    "section": "Schedule",
    "text": "Schedule\n\n\n\n\n\n\n\n\nMilestone\nInstructions\nDue Date\n\n\n\n\nProposal\n\nFri, Mar 1\n\n\nSimulation Study\n\nFri, Apr 12\n\n\nPresentations\n\nMon, May 06\n\n\nFinal Report\n\nFri, May 17",
    "crumbs": [
      "Term Project"
    ]
  }
]