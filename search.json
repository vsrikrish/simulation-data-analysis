[
  {
    "objectID": "slides/lecture01-2.html#modes-of-data-analysis",
    "href": "slides/lecture01-2.html#modes-of-data-analysis",
    "title": "Uncertainty and Probability Review",
    "section": "Modes of Data Analysis",
    "text": "Modes of Data Analysis"
  },
  {
    "objectID": "slides/lecture01-2.html#workflowcourse-organization",
    "href": "slides/lecture01-2.html#workflowcourse-organization",
    "title": "Uncertainty and Probability Review",
    "section": "Workflow/Course Organization",
    "text": "Workflow/Course Organization"
  },
  {
    "objectID": "slides/lecture01-2.html#questions",
    "href": "slides/lecture01-2.html#questions",
    "title": "Uncertainty and Probability Review",
    "section": "Questions?",
    "text": "Questions?\n\n\n\n\n\n\n\n\n\n\nText: VSRIKRISH to 22333\nURL: https://pollev.com/vsrikrish  See Results"
  },
  {
    "objectID": "slides/lecture01-2.html#what-is-uncertainty",
    "href": "slides/lecture01-2.html#what-is-uncertainty",
    "title": "Uncertainty and Probability Review",
    "section": "What Is Uncertainty?",
    "text": "What Is Uncertainty?\n\n\n\n\n\n\n\n\n\n\nText: VSRIKRISH to 22333\nURL: https://pollev.com/vsrikrish  See Results"
  },
  {
    "objectID": "slides/lecture01-2.html#what-is-uncertainty-1",
    "href": "slides/lecture01-2.html#what-is-uncertainty-1",
    "title": "Uncertainty and Probability Review",
    "section": "What Is Uncertainty?",
    "text": "What Is Uncertainty?\n\n\n\n…A departure from the (unachievable) ideal of complete determinism…\n\n\n— Walker et al. (2003)"
  },
  {
    "objectID": "slides/lecture01-2.html#types-of-uncertainty",
    "href": "slides/lecture01-2.html#types-of-uncertainty",
    "title": "Uncertainty and Probability Review",
    "section": "Types of Uncertainty",
    "text": "Types of Uncertainty\n\nAleatoric uncertainty: Uncertainties due to randomness/stochasticity;\nEpistemic uncertainty: Uncertainties due to lack of knowledge."
  },
  {
    "objectID": "slides/lecture01-2.html#data-relevant-uncertainty-taxonomy",
    "href": "slides/lecture01-2.html#data-relevant-uncertainty-taxonomy",
    "title": "Uncertainty and Probability Review",
    "section": "Data-Relevant Uncertainty Taxonomy",
    "text": "Data-Relevant Uncertainty Taxonomy\n\n\n\n\n\n\n\n\nUncertainty\nAssociated Uncertainties\nExamples\n\n\n\n\nStructural\nIncluded physical processes, mathematical form\nModel inadequacy, (epistemic) residual uncertainty\n\n\nParametric\nParameter uncertainty\nChoice of parameters, strength of coupling between models\n\n\nSampling\nNatural variability, (aleatoric) residual uncertainty\nInternal variability, uncertain boundary conditions"
  },
  {
    "objectID": "slides/lecture01-2.html#probability-distributions-1",
    "href": "slides/lecture01-2.html#probability-distributions-1",
    "title": "Uncertainty and Probability Review",
    "section": "Probability Distributions",
    "text": "Probability Distributions\nProbability distributions are often used to quantify uncertainty.\n\\[x \\to \\mathbb{P}_{\\color{green}\\nu}[x] = p_{\\color{green}\\nu}\\left(x | {\\color{purple}\\theta}\\right)\\]\n\n\\({\\color{green}\\nu}\\): probability distribution (often implicit);\n\\({\\color{purple}\\theta}\\): distribution parameters"
  },
  {
    "objectID": "slides/lecture01-2.html#sampling-notation",
    "href": "slides/lecture01-2.html#sampling-notation",
    "title": "Uncertainty and Probability Review",
    "section": "Sampling Notation",
    "text": "Sampling Notation\nTo write \\(x\\) is sampled from \\(p(x|\\theta)\\): \\[x \\sim f(\\theta)\\]\nFor example, for a normal distribution: \\[x \\sim \\mathcal{N}(\\mu, \\sigma)\\]"
  },
  {
    "objectID": "slides/lecture01-2.html#probability-density-function",
    "href": "slides/lecture01-2.html#probability-density-function",
    "title": "Uncertainty and Probability Review",
    "section": "Probability Density Function",
    "text": "Probability Density Function\nA continuous distribution \\(\\mathcal{D}\\) has a probability density function (PDF) \\(f_\\mathcal{D}(x) = p(x | \\theta)\\).\nThe probability of \\(x\\) occurring in an interval \\((a, b)\\) is \\[\\mathbb{P}[a \\leq x \\leq b] = \\int_a^b f_\\mathcal{D}(x)dx.\\]\n\n\n\n\n\n\nImportant\n\n\nThe probability that \\(x\\) has a specific value \\(x^*\\), \\(\\mathbb{P}(x = x^*)\\), is zero!"
  },
  {
    "objectID": "slides/lecture01-2.html#cumulative-density-functions",
    "href": "slides/lecture01-2.html#cumulative-density-functions",
    "title": "Uncertainty and Probability Review",
    "section": "Cumulative Density Functions",
    "text": "Cumulative Density Functions\nIf \\(\\mathcal{D}\\) is a distribution with PDF \\(f_\\mathcal{D}(x)\\), the cumulative density function (CDF) of \\(\\mathcal{D}\\) \\(F_\\mathcal{D}(x)\\):\n\\[F_\\mathcal{D}(x) = \\int_{-\\infty}^x f_\\mathcal{D}(u)du.\\]\nIf \\(f_\\mathcal{D}\\) is continuous at \\(x\\): \\[f_\\mathcal{D}(x) = \\frac{d}{dx}F_\\mathcal{D}(x).\\]"
  },
  {
    "objectID": "slides/lecture01-2.html#probability-mass-functions",
    "href": "slides/lecture01-2.html#probability-mass-functions",
    "title": "Uncertainty and Probability Review",
    "section": "Probability Mass Functions",
    "text": "Probability Mass Functions\nDiscrete distributions have probability mass functions (PMFs) which are defined at point values, e.g. \\(p(x = x^*) \\neq 0\\)."
  },
  {
    "objectID": "slides/lecture01-2.html#example-normal-distribution",
    "href": "slides/lecture01-2.html#example-normal-distribution",
    "title": "Uncertainty and Probability Review",
    "section": "Example: Normal Distribution",
    "text": "Example: Normal Distribution\n\\[f_\\mathcal{D}(x) = p(x | \\mu, \\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{1}{2}\\left(\\frac{x - \\mu}{\\sigma}^2\\right)\\right)\\]"
  },
  {
    "objectID": "slides/lecture01-2.html#why-are-normal-distributions-so-commonly-used",
    "href": "slides/lecture01-2.html#why-are-normal-distributions-so-commonly-used",
    "title": "Uncertainty and Probability Review",
    "section": "Why Are Normal Distributions So Commonly Used?",
    "text": "Why Are Normal Distributions So Commonly Used?\n\nSymmetry/Unimodality\nLinearity\nCentral Limit Theorem"
  },
  {
    "objectID": "slides/lecture01-2.html#linearity",
    "href": "slides/lecture01-2.html#linearity",
    "title": "Uncertainty and Probability Review",
    "section": "Linearity",
    "text": "Linearity\n\nIf \\(X \\sim \\mathcal{N}(\\mu, \\sigma)\\): \\[\\bbox[yellow, 10px, border:5px solid red] {aX + b \\sim \\mathcal{N}\\left(a\\mu + b, |a|\\sigma\\right)}\\]\nIf \\(X_1 \\sim \\mathcal{N}(\\mu_1, \\sigma_1)\\), \\(X_2 \\sim \\mathcal{N}(\\mu_2, \\sigma_2)\\): \\[\\bbox[yellow, 5px, border:5px solid red] {X_1 + X_2 \\sim \\mathcal{N}\\left(\\mu_1 + \\mu_2, \\sqrt{\\sigma_1^2 + \\sigma_2^2}\\right)}\\]"
  },
  {
    "objectID": "slides/lecture01-2.html#central-limit-theorem-sampling-distributions",
    "href": "slides/lecture01-2.html#central-limit-theorem-sampling-distributions",
    "title": "Uncertainty and Probability Review",
    "section": "Central Limit Theorem: Sampling Distributions",
    "text": "Central Limit Theorem: Sampling Distributions\nThe sum or mean of a random sample is itself a random variable:\n\\[\\bar{X}_n = \\frac{1}{n}\\sum_{i=1}^n X_i \\sim \\mathcal{D}_n\\]\n\n\\(\\mathcal{D}_n\\): The sampling distribution of the mean (or sum, or other estimate of interest)."
  },
  {
    "objectID": "slides/lecture01-2.html#central-limit-theorem",
    "href": "slides/lecture01-2.html#central-limit-theorem",
    "title": "Uncertainty and Probability Review",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\nIf\n\n\\(\\mathbb{E}[X_i] = \\mu\\)\nand \\(\\text{Var}(X_i) = \\sigma^2 &lt; \\infty\\),\n\n\\[\\begin{align*}\n&\\bbox[yellow, 10px, border:5px solid red]\n{\\lim_{n \\to \\infty} \\sqrt{n}(\\bar{X}_n - \\mu ) = \\mathcal{N}(0, \\sigma^2)} \\\\\n\\Rightarrow &\\bbox[yellow, 10px, border:5px solid red] {\\bar{X}_n \\overset{\\text{approx}}{\\sim} \\mathcal{N}(\\mu, \\sigma^2/n)}\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/lecture01-2.html#central-limit-theorem-more-intuitive",
    "href": "slides/lecture01-2.html#central-limit-theorem-more-intuitive",
    "title": "Uncertainty and Probability Review",
    "section": "Central Limit Theorem (More Intuitive)",
    "text": "Central Limit Theorem (More Intuitive)\nFor a large enough set of samples, the sampling distribution of a sum or mean of random variables is approximately a normal distribution, even if the random variables themselves are not."
  },
  {
    "objectID": "slides/lecture01-2.html#why-are-normal-distributions-so-commonly-used-1",
    "href": "slides/lecture01-2.html#why-are-normal-distributions-so-commonly-used-1",
    "title": "Uncertainty and Probability Review",
    "section": "Why Are Normal Distributions So Commonly Used?",
    "text": "Why Are Normal Distributions So Commonly Used?\n\nCentral Limit Theorem: For a large enough dataset, can assume statistical quantities have an approximately normal distribution.\nLinearity/Other Mathematical Properties: Easy to work with/do calculations\n\n\nCan we think about when this might break down?"
  },
  {
    "objectID": "slides/lecture01-2.html#other-useful-distributions",
    "href": "slides/lecture01-2.html#other-useful-distributions",
    "title": "Uncertainty and Probability Review",
    "section": "Other Useful Distributions",
    "text": "Other Useful Distributions\n\nUniform: \\(\\text{Unif}(a, b)\\) (equal probability);\nPoisson: \\(\\text{Poisson}(\\lambda)\\) (count data);\nBernoulli: \\(\\text{Bernoulli}(p)\\) (coin flips);\nBinomial: \\(\\text{Binomial}(n, p)\\) (number of successes);\nCauchy: \\(\\text{Cauchy}(\\gamma)\\) (fat tails);\nGeneralized Extreme Value: \\(\\text{GEV}(\\mu, \\sigma, \\xi)\\) (maxima/minima)"
  },
  {
    "objectID": "slides/lecture01-2.html#what-is-probability",
    "href": "slides/lecture01-2.html#what-is-probability",
    "title": "Uncertainty and Probability Review",
    "section": "What Is Probability?",
    "text": "What Is Probability?\n\nHow we communicate/capture uncertainty depends on how we interpret probability:\n\nFrequentist: \\(\\mathbb{P}[A]\\) is the long-run frequency of event A occurring.\nBayesian: \\(\\mathbb{P}[A]\\) is the degree of belief (betting odds) of event A occurring."
  },
  {
    "objectID": "slides/lecture01-2.html#frequentist-probability",
    "href": "slides/lecture01-2.html#frequentist-probability",
    "title": "Uncertainty and Probability Review",
    "section": "Frequentist Probability",
    "text": "Frequentist Probability\n\n\nFrequentist:\n\nData are random, but there is a “true” parameter set for a given model.\nHow consistent are estimates for different data?\n\n\n\nBayesian:\n\nData and parameters are random;\nProbability of parameters and unobserved data as consistency with observations."
  },
  {
    "objectID": "slides/lecture01-2.html#confidence-intervals",
    "href": "slides/lecture01-2.html#confidence-intervals",
    "title": "Uncertainty and Probability Review",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\n\n\nFrequentist estimates have confidence intervals, which will contain the “true” parameter value for \\(\\alpha\\)% of data samples.\nNo guarantee that an individual CI contains the true value (with any “probability”)!\n\n\n\n\n\n\n\n\n\n\n\nSource: https://www.wikihow.com/Throw-a-Horseshoe"
  },
  {
    "objectID": "slides/lecture01-2.html#example-95-cis-for-n0.4-2",
    "href": "slides/lecture01-2.html#example-95-cis-for-n0.4-2",
    "title": "Uncertainty and Probability Review",
    "section": "Example: 95% CIs for N(0.4, 2)",
    "text": "Example: 95% CIs for N(0.4, 2)\n\nCode\n# set up distribution\nmean_true = 0.4\nn_cis = 100 # number of CIs to compute\ndist = Normal(mean_true, 2)\n\n# use sample size of 100\nsamples = rand(dist, (100, n_cis))\n# mapslices broadcasts over a matrix dimension, could also use a loop\nsample_means = mapslices(mean, samples; dims=1)\nsample_sd = mapslices(std, samples; dims=1) \nmc_sd = 1.96 * sample_sd / sqrt(100)\nmc_ci = zeros(n_cis, 2) # preallocate\nfor i = 1:n_cis\n    mc_ci[i, 1] = sample_means[i] - mc_sd[i]\n    mc_ci[i, 2] = sample_means[i] + mc_sd[i]\nend\n# find which CIs contain the true value\nci_true = (mc_ci[:, 1] .&lt; mean_true) .&& (mc_ci[:, 2] .&gt; mean_true)\n# compute percentage of CIs which contain the true value\nci_frac1 = 100 * sum(ci_true) ./ n_cis\n\n# plot CIs\np1 = plot([mc_ci[1, :]], [1, 1], linewidth=3, color=:blue, label=\"95% Confidence Interval\", title=\"Sample Size 100\", yticks=:false, tickfontsize=14, titlefontsize=20, legend=:false, guidefontsize=16)\nfor i = 2:n_cis\n    if ci_true[i]\n        plot!(p1, [mc_ci[i, :]], [i, i], linewidth=2, color=:blue, label=:false)\n    else\n        plot!(p1, [mc_ci[i, :]], [i, i], linewidth=2, color=:red, label=:false)\n    end\nend\nvline!(p1, [mean_true], color=:black, linewidth=2, linestyle=:dash, label=\"True Value\") # plot true value as a vertical line\nxaxis!(p1, \"Estimate\")\nplot!(p1, size=(500, 400)) # resize to fit slide\n\n# use sample size of 1000\nsamples = rand(dist, (1000, n_cis))\n# mapslices broadcasts over a matrix dimension, could also use a loop\nsample_means = mapslices(mean, samples; dims=1)\nsample_sd = mapslices(std, samples; dims=1) \nmc_sd = 1.96 * sample_sd / sqrt(1000)\nmc_ci = zeros(n_cis, 2) # preallocate\nfor i = 1:n_cis\n    mc_ci[i, 1] = sample_means[i] - mc_sd[i]\n    mc_ci[i, 2] = sample_means[i] + mc_sd[i]\nend\n# find which CIs contain the true value\nci_true = (mc_ci[:, 1] .&lt; mean_true) .&& (mc_ci[:, 2] .&gt; mean_true)\n# compute percentage of CIs which contain the true value\nci_frac2 = 100 * sum(ci_true) ./ n_cis\n\n# plot CIs\np2 = plot([mc_ci[1, :]], [1, 1], linewidth=3, color=:blue, label=\"95% Confidence Interval\", title=\"Sample Size 1,000\", yticks=:false, tickfontsize=14, titlefontsize=20, legend=:false, guidefontsize=16)\nfor i = 2:n_cis\n    if ci_true[i]\n        plot!(p1, [mc_ci[i, :]], [i, i], linewidth=2, color=:blue, label=:false)\n    else\n        plot!(p1, [mc_ci[i, :]], [i, i], linewidth=2, color=:red, label=:false)\n    end\nend\nvline!(p2, [mean_true], color=:black, linewidth=2, linestyle=:dash, label=\"True Value\") # plot true value as a vertical line\nxaxis!(p2, \"Estimate\")\nplot!(p2, size=(500, 400)) # resize to fit slide\n\ndisplay(p1)\ndisplay(p2)\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Sample Size 100\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Sample Size 1,000\n\n\n\n\n\n\n\nFigure 1: Display of 95% confidence intervals\n\n\n\n90% of the CIs contain the true value (left) vs. 94% (right)"
  },
  {
    "objectID": "slides/lecture01-2.html#key-points",
    "href": "slides/lecture01-2.html#key-points",
    "title": "Uncertainty and Probability Review",
    "section": "Key Points",
    "text": "Key Points\n\nDifferent model-relevant uncertainties (will matter later!)\nReviewed probability distribution basics\n\nMany different distributions, suitable for different purposes\nProbability density functions vs. cumulative density functions"
  },
  {
    "objectID": "slides/lecture01-2.html#key-points-1",
    "href": "slides/lecture01-2.html#key-points-1",
    "title": "Uncertainty and Probability Review",
    "section": "Key Points",
    "text": "Key Points\n\nFrequentist vs. Bayesian probability (matters a bit later)\n\nFrequentist: parameters as fixed (trying to recover with enough experiments)\nBayesian: parameters as random (probability reflects degree of consistency with observations)\nIn both cases data are random!"
  },
  {
    "objectID": "slides/lecture01-2.html#key-points-2",
    "href": "slides/lecture01-2.html#key-points-2",
    "title": "Uncertainty and Probability Review",
    "section": "Key Points",
    "text": "Key Points\n\nConfidence Intervals:\n\n\\(\\alpha\\)% of \\(\\alpha\\)-CIs generated from different samples will contain the “true” parameter value\nDo not say anything about probability of including true value!"
  },
  {
    "objectID": "slides/lecture01-2.html#references",
    "href": "slides/lecture01-2.html#references",
    "title": "Uncertainty and Probability Review",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nWalker, W. E., Harremoës, P., Rotmans, J., Sluijs, J. P. van der, Asselt, M. B. A. van, Janssen, P., & Krayer von Krauss, M. P. (2003). Defining uncertainty: A conceptual basis for uncertainty management in model-based decision support. Integrated Assessment, 4, 5–17. https://doi.org/10.1076/iaij.4.1.5.16466"
  }
]